{
    "https://medium.datadriveninvestor.com/4-simple-steps-in-building-ocr-1f41c66099c1": {
        "author": "Naga Kiran",
        "length": "4 min read",
        "title": "4 Simple steps in building OCR",
        "tags": [
            "machine-learning",
            "ocr",
            "python",
            "nlp",
            "image"
        ],
        "content": [
            "OCR (optical character recognition) is the recognition of printed or written text characters by a computer. This involves photoscanning of the text character-by-character, analysis of the scanned-in image, and then translation of the character image into character codes, such as ASCII, commonly used in data processing.",
            "In OCR processing, the scanned-in image or bitmap is analyzed for light and dark areas in order to identify each alphabetic letter or numeric digit. When a character is recognized, it is converted into an ASCII code. Special circuit boards and computer chips designed expressly for OCR are used to speed up the recognition process.",
            "www.datadriveninvestor.com",
            "Steps in Optical Character Recognition :-",
            "1) Extraction of Character boundaries from Image,",
            "2) Building a Convolutional Neural Network(ConvNet) in remembering the Character images,",
            "3) Loading trained Convolutional Neural Network(ConvNet) Model,",
            "4) Consolidating ConvNet predictions of characters",
            "github.com",
            "The Algorithm is built in a way to segment each individual character in a Image as individual images :-) , followed by recognition and consolidation to text in an Image.",
            "to download the Pretrained Models . to download sample labelled character Images train data.",
            "1) Optical Scanning \u2702\ufe0f from Image :",
            "Select any document or letter of having text information",
            "Extract Character boundaries: Contours can be explained simply as a curve joining all the continuous points (along the boundary). The contours are a useful tool for shape analysis and object detection and recognition. Here Contours explained in differentiating each individual character in an image with using contour dilation technique. Create a boundary to each character in an image with using OpenCV Contours method. Character recognition with the use ofOpenCV contours method. OpenCV code implementation in differentiating the words with the use of contours",
            "ret,thresh1 = cv2.threshold(im1,180,255,cv2.THRESH_BINARY_INV) kernel = np.ones((5,5),np.uint8) dilated = cv2.dilate(thresh1,kernel,iterations = 2) _,contours, hierarchy = cv2.findContours(dilated,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) cordinates = [] for cnt in contours: x,y,w,h = cv2.boundingRect(cnt) cordinates.append((x,y,w,h)) #bound the images cv2.rectangle(im,(x,y),(x+w,y+h),(0,255,0),1) cv2.namedWindow('BindingBox', cv2.WINDOW_NORMAL) cv2.imwrite('data/BindingBox4.jpg',im)",
            "Naming Convention followed(Labelling) : The extracted text characters should be labelled with the Original character name associated with it.",
            "Naming convention followed here is, last letter of file name should be the name associated with the character for pre-processing the images data.",
            "Pre-processing",
            "The raw data depending on the data acquisition type is subjected to a number of preliminary processing steps to make it usable in the descriptive stages of character analysis. The image resulting from scanning process may contain certain amount of noise Smoothing implies both filling and thinning. Filling eliminates small breaks, gaps and holes in digitized characters while thinning reduces width of line.",
            "(a) noise reduction (b) normalization of the data and (c) compression in the amount of information to be retained.",
            "2) Build a ConvNet Model \u2702\ufe0f(Character Recognition Model):",
            "Convolution Network of 8 layers with 2*4 layers residual feedbacks used in remembering the Patterns \u2702\ufe0f of the Individual Character Images.",
            "1st Model will train on the Individual Character Images with direct Classification to predict the Images with softmax Classification of Character Categories. 2nd Model is same model with last before layer as predictor which will Calculate a Embedding of specified Flatten Neurons ( The Predicted flatten Values will have Feature Information of Receipt Images ).",
            "3) Load Trained ConvNet OCR model:",
            "Optical Character recognition last step involves preprocessing of image into specific word related contours and letter contours, followed by prediction and consolidating according to letter and word related contours in an image.",
            "once after training the model, we can save and load the pre-trained Optical character recognition model.",
            "4) Test and Consolidate Predictions of OCR :",
            "Consolidate predictions involves, assigning specific ID to each word related contour with the line associated with the word in image, Consolidating all predictions in a sorted series of specific word related contour and letters associated word."
        ]
    },
    "https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9": {
        "author": "Anders Christiansen",
        "length": "6 min read",
        "title": "Anchor Boxes \u2014 The key to quality object detection",
        "tags": [
            "machine-learning",
            "object-detection",
            "computer-vision"
        ],
        "content": [
            "One of the hardest concepts to grasp when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes. It is also one of the most important parameters you can tune for improved performance on your dataset. In fact, if anchor boxes are not tuned correctly, your neural network will never even know that certain small, large or irregular objects exist and will never have a chance to detect them. Luckily, there are some simple steps you can take to make sure you do not fall into this trap.",
            "When you use a neural network like YOLO or SDD to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object. The multiple predictions are output with the following format:",
            "Prediction 1: (X, Y, Height, Width), Class\u2026.Prediction ~80,000: (X, Y, Height, Width), Class",
            "Where the(X, Y, Height, Width) is called the \u201cbounding box\u201d, or box surrounding the objects. This box and the object class are labelled manually by human annotators.",
            "In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:",
            "We need to tell our network if each of its predictions is correct or not in order for it to be able to learn. But what do we tell the neural network it prediction should be? Should the predicted class be:",
            "Prediction 1: PearPrediction 2: Apple",
            "Or should it be:",
            "Prediction 1: ApplePrediction 2: Pear",
            "What if the network predicts:",
            "Prediction 1: ApplePrediction 2: Apple",
            "We need our network\u2019s two predictors to be able to tell whether it is their job to predict the pear or the apple. To do this there are a several tools. Predictors can specialize in certain size objects, objects with a certain aspect ratio (tall vs. wide), or objects in different parts of the image. Most networks use all three criteria. In our example of the pear/apple image, we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image. Then we would have our answer for what the network should be predicting:",
            "Prediction 1: PearPrediction 2: Apple",
            "State of the art object detection systems currently do the following:",
            "1. Create thousands of \u201canchor boxes\u201d or \u201cprior boxes\u201d for each predictor that represent the ideal location, shape and size of the object it specializes in predicting.",
            "2. For each anchor box, calculate which object\u2019s bounding box has the highest overlap divided by non-overlap. This is called Intersection Over Union or IOU.",
            "3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU.",
            "4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example.",
            "5. If the highest IOU is less than 40%, then the anchor box should predict that there is no object.",
            "This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object appears in an image. Taking a look at an open source implementation of RetinaNet, a state-of-the-art object detector, we can visualize the anchor boxes. There are too many to visualize all at once, however here are just 1% of them:",
            "Using the default anchor box configuration can create predictors that are too specialized and objects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes. In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak our anchor boxes to be much smaller, such as this 1% sample:",
            "In the RetinaNet configuration, the smallest anchor box size is 32x32. This means that many objects smaller than this will go undetected. Here is an example from the WiderFace dataset (Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou) where we match bounding boxes to their respective anchor boxes, but some fall through the cracks:",
            "In this case, only four of the ground truth bounding boxes overlap with any of the anchor boxes. The neural network will never learn to predict the other faces. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the faces line up with at least one of our anchor boxes and our neural network can learn to detect them!",
            "As a general rule, you should ask yourself the following questions about your dataset before diving into training your model:",
            "What is the smallest size box I want to be able to detect? What is the largest size box I want to be able to detect? What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side.",
            "You can get a rough estimate of these by actually calculating the most extreme sizes and aspect ratios in the dataset. YOLO v3, another object detector, uses K-means to estimate the ideal bounding boxes. Another option is to learn the anchor box configuration.",
            "Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes and then decoding them as though they were predictions from your model. You should be able to recover the ground truth bounding boxes.",
            "Also, remember that if the center of the bounding box and anchor box differ, this will reduce the IOU. Even if you have small anchor boxes, you may miss some ground truth boxes if the stride between anchor boxes is wide. One way to ameliorate this is to lower the IOU threshold from 50% to 40%.",
            "A recent article by David Pacassi Torrico comparing current API implementations of face detection highlights the importance of correctly specifying anchor boxes. You can see that the algorithms do well except for small faces. Below are some pictures where an API failed to detect any faces at all, but many were detected with our new model:",
            "If you enjoy this article, you might like reading about object detection without anchor boxes.",
            "For a more in-depth explanation of anchor boxes you can refer to Andrew Ng\u2019s Deep Learning Specialization or Jeremy Howards\u2019s fast.ai"
        ]
    },
    "https://medium.com/@ghimire.aiesecer/data-augmentation-techniques-for-image-classification-in-tensorflow-5dad8db481de": {
        "author": "Suman Ghimire",
        "length": "3 min read",
        "title": "Data Augmentation Techniques for Image Classification in Tensorflow",
        "tags": [
            "machine-learning",
            "augmentation",
            "deep-learning",
            "tensorflow",
            "image-classification"
        ],
        "content": [
            "In computer vision, data augmentation is the technique to improve the performance of computer vision systems. This makes our classification algorithm more robust to changes such as sunlight, illumination etc, across our training dataset. Tensorflow API provides wide range of data augmentation methods to improve the classification performance in DNN. Using these augmentation steps to our dataset could increase the networks ability to generalize as more training data is generated, with variation from our original data. This technique is handy when dealing with small amount of dataset for the purpose of training DNN. Data augmentation, is also another way, we can reduce overfitting on models, where we increase the amount of training data using information only in our training data.",
            "Arguments can be provided as key-value pairs. The augmentation can be initialized as below, where you can specify your own desired values, otherwise, preprocessor.proto will just supply the default value:",
            "data_augmentation_options { random_image_scale { min_scale_ratio: 0.3 max_scale_ratio: 1.5 } }",
            "where, user-specified value will overwrite the defaults,",
            "def random_image_scale(image, masks=None, min_scale_ratio=0.5, max_scale_ratio=2.0, seed=None, preprocess_vars_cache=None): \"\"\"Scales the image size. Args: image: rank 3 float32 tensor contains 1 image -> [height, width, channels]. masks: (optional) rank 3 float32 tensor containing masks with size [height, width, num_masks]. The value is set to None if there are no masks. min_scale_ratio: minimum scaling ratio. max_scale_ratio: maximum scaling ratio. seed: random seed. preprocess_vars_cache: PreprocessorCache object that records       previously performed augmentations. Updated in-place. If this                            function is called multiple times with the same non-null cache, it will perform deterministically. Returns: image: image which is the same rank as input image. masks: If masks is not none, resized masks which are the same rank as input masks will be returned. with tf.name_scope('RandomImageScale', values=[image]): result = [] image_shape = tf.shape(image) image_height = image_shape[0] image_width = image_shape[1] generator_func = functools.partial( tf.random_uniform, [], minval=min_scale_ratio, maxval=max_scale_ratio, dtype=tf.float32, seed=seed) size_coef = _get_or_create_preprocess_rand_vars( generator_func, preprocessor_cache.PreprocessorCache.IMAGE_SCALE, preprocess_vars_cache) image_newysize = tf.to_int32( tf.multiply(tf.to_float(image_height), size_coef)) image_newxsize = tf.to_int32( tf.multiply(tf.to_float(image_width), size_coef)) image = tf.image.resize_images( image, [image_newysize, image_newxsize], align_corners=True) result.append(image) if masks: masks = tf.image.resize_nearest_neighbor( masks, [image_newysize, image_newxsize], align_corners=True) result.append(masks) return tuple(result)",
            "For randomly adjusting saturation,",
            "data_augmentation_options { random_adjust_saturation { } }",
            "where, the default values will be initialized unless specified,",
            "def random_adjust_saturation(image, min_delta=0.8, max_delta=1.25, seed=None, preprocess_vars_cache=None): \"\"\"Randomly adjusts saturation. Makes sure the output image is still between 0 and 255. Args: image: rank 3 float32 tensor contains 1 image -> [height, width,   channels] with pixel values varying between [0, 255]. min_delta: see max_delta. max_delta: how much to change the saturation. Saturation will change with a value between min_delta and max_delta. This value will be multiplied to the current saturation of the image. seed: random seed. preprocess_vars_cache: PreprocessorCache object that records previously performed augmentations. Updated in-place. If this function is called multiple times with the same non-null cache, it will perform deterministically. Returns: image: image which is the same shape as input image. \"\"\" with tf.name_scope('RandomAdjustSaturation', values=[image]): generator_func = functools.partial(tf.random_uniform, [], min_delta, max_delta, seed=seed) saturation_factor = _get_or_create_preprocess_rand_vars( generator_func, preprocessor_cache.PreprocessorCache.ADJUST_SATURATION, preprocess_vars_cache) image = tf.image.adjust_saturation(image/255, saturation_factor)*255 image=tf.clip_by_value(image,clip_value_min=0.0,clip_value_max=255.0) return image",
            "Here are the lists of options provided in Preprocessor.proto. Choose augmentation technique from the list of options that are suited for your purpose.",
            "NormalizeImage normalize_image = 1; RandomHorizontalFlip random_horizontal_flip = 2;      RandomVerticalFlip random_vertical_flip = 3 RandomPixelValueScale random_pixel_value_scale = 4; RandomImageScale random_image_scale = 5; RandomRGBtoGray random_rgb_to_gray = 6; RandomAdjustBrightness random_adjust_brightness = 7; RandomAdjustContrast random_adjust_contrast = 8; RandomAdjustHue random_adjust_hue = 9; RandomAdjustSaturation random_adjust_saturation = 10; RandomDistortColor random_distort_color = 11; RandomJitterBoxes random_jitter_boxes = 12; RandomCropImage random_crop_image = 13; RandomPadImage random_pad_image = 14; RandomCropPadImage random_crop_pad_image = 15; RandomCropToAspectRatio random_crop_to_aspect_ratio = 16; RandomBlackPatches random_black_patches = 17; RandomResizeMethod random_resize_method = 18; ScaleBoxesToPixelCoordinates scale_boxes_to_pixel_coordinates = 19; ResizeImage resize_image = 20; SubtractChannelMean subtract_channel_mean = 21; SSDRandomCrop ssd_random_crop = 22; SSDRandomCropPad ssd_random_crop_pad = 23; SSDRandomCropFixedAspectRatio ssd_random_crop_fixed_aspect_ratio = 24;",
            "Cheers!!! Happy Deep-Learning",
            "Suman Ghimire",
            "Co-Founder at Pixel Labs ( https://www.pixelnetworks.net/)",
            "Msc. Engineering & Technology ( Sustainable Agriculture)",
            "References",
            "github.com",
            "stackoverflow.com"
        ]
    },
    "https://towardsdatascience.com/deepmind-releases-a-new-state-of-the-art-image-classification-model-nfnets-75c0b3f37312": {
        "author": "Mostafa Ibrahim",
        "length": "6 min read",
        "title": "Deepmind releases a new State-Of-The-Art Image Classification model \u2014 NFNets",
        "tags": [
            "data-science",
            "machine-learning",
            "artificial-intelligence",
            "ai",
            "deep-learning"
        ],
        "content": [
            "Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7\u00d7 faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%.",
            "Source: arxiv",
            "One of the most annoying things about training a model is the time it takes to train it and the amount of memory needed to fit in the data and the models. Since image classification is one of the most common machine learning tasks, Deepmind released a new model that matches the state-of-art (SOTA) performance with significantly less size, higher training speed, and fewer optimization techniques for simplicity.",
            "In their work, they examine the current SOTA models such as EfficientNets and ResNets. In their analysis they pindown some of the optimization techniques that utilize a lot of memory without producing a significant value for performance. They prove that these networks can achieve the same performance without those optimization techniques.",
            "Although the proposed model might be the most interesting bit, I still find the analysis of previous work to be very interesting. Simply because this is where most of the learning happens, we start understanding what could have been done better and why the newly proposed method/technique is an improvement over the old one.",
            "The paper starts off with an analysis of batch normalisation. Why? because although it has shown great results and has been used heavily in tons of SOTA models, it has several disadvantages outlined by the paper [1], such as:",
            "Very expensive computational costs Introduces a lot of extra hyper-parameters that need further fine-tuning Causes a lot of implementation errors in distributed training Performs poorly on small batch sizes, which are used often in training larger models",
            "But first, before removing batch normalization, we have to understand what benefits it brought to the models. Because we want to find a smarter way to still have those benefits, but with fewer cons. Those benefits are [1]:",
            "It downscales residual branches in deep ResNets. ResNets are one of the most widely used image classification networks. They usually extend to thousands of layers, and batch normalization reduces the scale of \u201chidden activations\u201d that often cause gradients to behave in a funny way ( gradient exploding problem ) Eliminates mean-shift for popular activation functions such as ReLU and GeLU. In large networks, the output of those activation functions typically shifts towards very large values on average. This causes the network to predict the same label for all samples in certain situations (such as initialization) decreasing its performance. Batch normalization solves this mean-shift problem.",
            "There are some other benefits, but I think you got the gist that its all mainly about regularisation and smoothing the training process.",
            "Although there have been previous attempts to remove batch normalization (BN) in various papers, the results didn\u2019t match the SOTA performance or training latency and seemed to fail on large batch sizes, and this is the main selling point of this paper. They succeed in removing (BN) without affecting performance, and with improving the training latency by a large margin.",
            "To do that, they propose a gradient clipping technique called Adaptive Gradient Clipping (AGC) [1]. Essentially, gradient clipping is used to stabilize model training [1] by not allowing the gradient to go beyond a certain threshold. This allows using larger training rates and thus faster convergence without the exploding gradient problem.",
            "However, the main issue is setting the threshold hyper-parameter, which is quite a difficult and manual task. The main benefit of AGC is to remove this hyperparameter. To do this we have to examine the gradient norms and the parameter norms.",
            "Although I am quite interested in the mathematics behind every machine learning model, I understand that a lot of ML enthusiasts don\u2019t enjoy reading a bunch of long differential equations, that\u2019s why I will explain AGC from a theoretical/intuitive perspective rather than a mathematically rigorous one.",
            "A norm is simply a measure of the magnitude of a vector. AGC is built on the premise that:",
            "the unit-wise ratio of the norm of the gradients to the norm of the weights of a layer provides a simple measure of how much a single gradient descent step will change the original weights.",
            "Source: arxiv",
            "But why is that premise valid? Let\u2019s back up a little. A very high gradient will make our learning unstable, and if that's the case then the ratio of the gradient of the weight matrix to the weight matrix will be very high.",
            "That weight ratio is equivalent to:",
            "learning rate x the ratio between the gradient and the weight matrix (which is our premise).",
            "So essentially, the ratio proposed by that premise is a valid indicator as to whether we should clip the gradient or not. There is also another minor tweak. They have found that through multiple experiments, it's much better to use a unit-wise ratio of gradient norms instead of a layer-wise ratio (because each layer can have more than one gradient).",
            "In addition to AGC, they also used dropout to substitute the regularisation effect that Batch normalization was offering.",
            "They also used an optimization technique called Sharpness-Aware Minimization (SAM) [1].",
            "Motivated by the connection between the geometry of the loss landscape and generalization \u2014 including a generalization bound that we prove here \u2014 we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several.",
            "Source: SAM arxiv paper",
            "The idea of loss sharpness seems quite interesting and I might be exploring it in another article for the sake of brevity here. One final point to note here though is that they make a small modification to SAM [1] to reduce its computational cost by 20\u201340%! and they only employ it on their 2 largest model variants. It\u2019s always great to see additions being made to such techniques instead of just using them out of the box. I think this shows that they have analyzed it greatly before using it (and thus were able to optimize it a bit).",
            "Final thoughts and take away",
            "Who would have thought that replacing a minor optimization technique such as batch normalization would result in a 9x improvement in training latency. I think this sends a message of being a bit more skeptical about popular optimization techniques that are used everywhere. In all fairness, I have been a victim of this crime before, I used to just put every popular optimization technique into my machine learning projects without fully examining its pros and cons. I guess this is one of the main benefits of reading ML papers, the analysis of previous SOTAs!",
            "If you want to receive regular paper reviews about the latest papers in AI & Machine learning, add your email here & Subscribe!",
            "https://artisanal-motivator-8249.ck.page/5524b8f934",
            "References:",
            "[1] High-Performance Large-Scale Image Recognition Without Normalization. Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan. 2021",
            "If you are interested in reading more about other novel papers, check out my articles here:",
            "towardsdatascience.com",
            "towardsdatascience.com",
            "towardsdatascience.com",
            "towardsdatascience.com",
            "towardsdatascience.com"
        ]
    },
    "https://towardsdatascience.com/facebook-deit-a-promising-new-technique-for-image-classification-6516f039b4bb": {
        "author": "Mostafa Ibrahim",
        "length": "4 min read",
        "title": "Facebook DeIt: State-of-art image classification with 0 convolutions and less than 1% of the state-of-art dataset",
        "tags": [
            "machine-learning",
            "data-science",
            "artificial-intelligence",
            "deep-learning",
            "data"
        ],
        "content": [
            "A few weeks ago, Facebook released a new ML model (Data-Efficient Image Transformer, DeIt) that achieves state-of-art image classification performance using only the ImageNet dataset (1.2 million images). State of the art visual transformers can only reach this performance using hundreds of millions of images [1]. And how Facebook achieved this is the most interesting bit since they didn\u2019t use any convolutions or a large dataset.",
            "There are a lot of great machine learning papers that are being released almost daily. The reason that I chose this one to review is that it's using some interesting techniques.",
            "One of those techniques is attention and transformers which I don\u2019t want to cover thoroughly since there are tons of other articles about them. I am however going to give a quick overview just so we can explore DeIt properly.",
            "Transformers and attention have been dominating the machine learning space for the last few years. They started in NLP and now they are moving to images.",
            "Visual transformers use Multi-head Self Attention layers. Those layers are based on the attention mechanism that utilizes queries, keys, and vectors to \u201cpay attention\u201d to information from different representations at different positions.",
            "A classic transformer block for images starts with a normal Feed Forward Network followed by a Multi-head Self Attention layer. One interesting bit is that the feed-forward network used an activation function called Gaussian Error Linear Unit which aims to regularize the model by randomly multiplying a few activations by 0.",
            "The visual transformer has some issues that were solved by this paper, such as:",
            "It was trained on 300 million images (JFT-300M [1]) Those 300 million images are a private dataset It couldn\u2019t generalize well.",
            "Okay now that we have covered the basics, let\u2019s start taking a look at what is special about this paper.",
            "Knowledge distillation refers to the idea of model compression by teaching a smaller network, step by step, exactly what to do using a bigger already trained network. The \u2018soft labels\u2019 refer to the output feature maps by the bigger network after every convolution layer. The smaller network is then trained to learn the exact behavior of the bigger network by trying to replicate it\u2019s outputs at every level (not just the final loss).",
            "Source: Prakhar Ganesh",
            "This is quite fascinating, just like in the real world we have teachers, in ML we have bigger smaller networks mimicking larger networks to learn from them.",
            "Typical visual transformers use the concept of a trainable vector called the class token. This token attempts to replace conventional pooling layers that can be found in Convolutional Neural Networks. It boosts the model's performance and spreads out the information from image patches.",
            "Facebook adds a distillation token that interacts with this class token and other initial embeddings at the start to boost the self-attention mechanism of the model. This token is a trainable vector that is being learned during training.",
            "Its objective is to minimize the Kullback-Leibler (KL) divergence between the softmax of the teacher and the softmax of the student model, (this is called soft distillation). All you need to know about the KL divergence is that it measures the difference between 2 distributions.",
            "So essentially, this distillation token tries to minimize the difference in the information of the student network and the teacher network. This is quite an impressive and novel strategy!",
            "They have also verified [1] that the usefulness of this new token by attempting to add a class token (instead of the distillation token). The result was worse performance.",
            "Note that the teacher network here is a Convolutional Neural Network.",
            "One of the best things about this paper is that Facebook has released the full code, dataset, paper, and pretty much everything. They released 3 different models of different sizes. And as you can see from the graph, they all perform quite well even compared to one of the best and most recent networks, EfficientNet.",
            "In summary, I think these are the 3 main tricks to Facebook\u2019s achievement:",
            "The power of visual transformers and attention Replacing word embeddings with patch embeddings through a distillation token Not relying on convolutions",
            "Final thoughts:",
            "There is no such thing as a perfect model, I am sure this model has a few flaws. However, it\u2019s quite interesting to see what the top AI researchers are doing. I hope you got the intuition behind the distillation token trick so that you can be inventing your own tricks in your ML projects!",
            "I didn\u2019t want to dive into mathematics (although I love math) so that the article would suit a larger audience. If you are interested in that and in checking out more of their results, I suggest taking a look at the paper.",
            "If you want to receive regular paper reviews about the latest papers in AI & Machine learning, add your email here & Subscribe!",
            "https://artisanal-motivator-8249.ck.page/5524b8f934",
            "References:",
            "[1] Training data-efficient image transformers & distillation through attention. Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou. 2021 In arxiv"
        ]
    },
    "https://medium.com/deep-learning-journals/fast-scnn-explained-and-implemented-using-tensorflow-2-0-6bd17c17a49e": {
        "author": "Kshitiz Rimal",
        "length": "8 min read",
        "title": "Fast-SCNN explained and implemented using Tensorflow 2.0",
        "tags": [
            "machine-learning",
            "tensorflow",
            "deep-learning",
            "semantic-segmentation",
            "fast-scnn"
        ],
        "content": [
            "Fast Segmentation Convolutional Neural Network (Fast-SCNN) is an above real-time semantic segmentation model on high resolution image data suited to efficient computation on embedded devices with low memory. The authors of the original paper are: Rudra PK Poudel, Stephan Liwicki and Roberto Cipolla. The code used in this article is not the official implementation from the authors but an attempt by me to re-construct the model as described on the paper.",
            "Since the rise of autonomous vehicles, it is highly desirable that there exist a model that can process input in real-time. There already exist some state-of-the-art offline semantic segmentation models, but these models are large in size and memory requirement and requires expensive computation, Fast-SCNN can provide solution to all these problems.",
            "Some key aspects of Fast-SCNN are:",
            "Above real-time segmentation on high resolution images (1024 x 2048px) Yield Accuracy of 68% mean intersection over union Process Input at 123.5 frames per second on Cityscapes dataset No large pre-training required Combines spatial details at High resolution with deep features extracted at lower resolution",
            "Moreover, Fast-SCNN uses popular techniques found in state-of-the-art models to ensure the performance as listed above, techniques like Pyramid Pooling Module or PPM as used in PSPNet, Inverted Residual Bottleneck layers as used in MobileNet V2 and Feature Fusion Module used in model such as ContextNet, which uses both deep features extracted from the lower resolution data and spatial details from higher resolution data to ensure better and fast segmentation.",
            "Let us now begin with the exploration and the implementation of the Fast-SCNN. Fast-SCNN is constructed using 4 major building blocks. They are:",
            "Learning to Down-sample Global Feature Extractor Feature Fusion Classifier",
            "So far we know that the first few layers of Deep Convolutional neural network extract the low level features such as, edges and corners from the image. So to exploit this feature and make it re-usable for further layers, Learning to Down Sample is used. Its a coarse global feature extractor that can be re-used and shared by other modules in the network.",
            "Learning to Down-sample module uses 3 layers to extract out these global features. These are: Conv2D layer followed by 2 Depthwise Separable Convolutional Layers. During the implementation, after each Conv2D and Depthwise Separable Conv layers, a Batchnorm layer followed by Relu activation is used, as usually its a standard practice to introduce BatchNorm and Activations after such layers. Here, all 3 layers uses stride of 2 and kernel size of 3x3.",
            "Now, let us begin by first implementing this module. To begin, lets first install Tensorflow 2.0. It\u2019s easier than ever to do this now. We can simply use Google Colab and begin our implementation. As of now, there is only alpha version of Tensorflow 2.0 available, which you can simply install using following command:",
            "!pip install tensorflow-gpu==2.0.0-alpha0",
            "Here, \u2018-gpu\u2019 implies that my Google Colab notebook uses GPU, and in case of yours, if you prefer to not use it, you can simply remove the \u2018-gpu\u2019 and then the Tensorflow installation will utilize the cpu of the system.",
            "After that, let\u2019s import Tensorflow:",
            "import tensorflow as tf",
            "Now, let\u2019s first create the Input layer for our Model. In Tensorflow 2.0 using TF.Keras high level api, we can do so by:",
            "This Input layer is our entry point to the model that we are going to build. Here we are utilizing Tf.Keras Functional api. The reason to use Functional api instead of Sequential is because, it provides flexibility that we require to construct this particular model.",
            "Moving on, let us now define the layers for the Learning to Down-sample module. For that, to make the process easy and re-usable, I have created a custom function that will check if the layer I want to add is a Conv2D layer or Depthwise Separable layer and later checks if I want to add relu at the end of the layer or not. Using this code block made the implementation easy to understand and re-usable through out the implementation.",
            "In TF.Keras, Convolutional layer is defined as tf.keras.layers.Conv2D and Depthwise Separable Layer as tf.keras.layers.SeparableConv2D",
            "Now, let\u2019s add the layers for the module by simply calling our custom function with proper parameters:",
            "This module aimed to capture the global context for the segmentation. It directly takes the output from the Learning to Down-sample module. In this section different bottleneck residual blocks are introduced and a special module called Pyramid Pooling Module or PPM is also introduced to aggregate different region-based context information.",
            "Let\u2019s begin with Bottleneck residual block.",
            "Above is the description for the bottleneck residual block from the paper. Similar to above, let us now implement it using tf.keras high level api.",
            "We begin by first describing some custom functions as per the table above. We begin with residual block which will call our custom conv_block function to add Conv2D and then adds DepthWise Conv2D layer followed by point-wise convolution layer, as described on table above. Then the final output from point-wise convolution is added with the original input to make it residual.",
            "This bottleneck residual block is added multiple times in the architecture, the number of times they are added is denoted by the \u2019n\u2019 parameter in the Table 1 above. So, as per the architecture described in the paper, in order to added it \u2019n\u2019 times, we introduce another custom function that will just do that.",
            "Now let\u2019s add these bottleneck blocks to our model.",
            "Here, you will notice that the first input to these bottleneck block is from the output of the Learning to down-sample module.",
            "The final block to this Global Feature Extractor section is the Pyramid Pooling Module or PPM in short.",
            "PPM takes the feature maps from the last convolutional layer and then applies multiple sub region average pooling and upscaling functions to harvest different sub-region representations, and then they are concatenated together which carries both local and global context information from the image making the segmentation process more accurate.",
            "To implement that using TF.Keras, I have used another custom function",
            "Lets add this PPM module that will take input from the last bottleneck block",
            "The second argument here are the number of bin sizes to be provided to the PPM module, the bin sizes here used are as per described in the paper. These bin sizes are used to make AveragePooling at different sub regions as described in custom function above.",
            "In this module, two inputs are added together to better represent the segmentation. The first one is Higher level feature extracted from the Learning to Down-sample module, the output from this Learning to Down-sample module is point-wise convoluted first, before adding to the second input. Here no activation is added at the end of the point-wise convolution.",
            "The Second input is the output from the Global Feature Extractor. But before adding the second input, they first Upsampled by the factor of (4,4) and then DepthWise Convoluted and finally followed by another point-wise convolution. No activation is added to the point wise convolution output, the activations are introduced only after adding these two inputs.",
            "Here is the lower resolution operations implemented using TF.Keras",
            "Now, let us add these two inputs together for the feature fusion module.",
            "In classifier section, 2 DepthWise Separable Convolutional Layers are introduced followed by 1 Point-wise Convolutional layer. After each of these layers, BatchNorm layers followed by ReLU activations are also introduced.",
            "One thing to note here is that, in the original paper\u2019s table (Table 1 above), there is no mention of Upscaling and Dropout layers after the point-wise convolutional layer, but in later part of the paper it is described that these layers are added after the point-wise convolutional layer. Therefore, during the implementation, I have also introduced these two layers as per written in the paper.",
            "After Upscaling as per desired from the final output, the SoftMax activation is introduced as the final layer.",
            "Now that we have added all the layers, let\u2019s create our final model and compile it. To create the model, as already mentioned above, we are using Functional api from TF.Keras. Here the input to the model is the initial Input Layer described at the Learning to Down-sample module and the output is the final classifier output.",
            "Now, let\u2019s compile it with optimizers and loss functions. In the original paper, the authors have used SGD optimizer with momentum value of 0.9 with batch-size of 12 during the training process. They have also used poly-learning rate for the learning rate scheduling with base value of 0.045 and power as 0.9. For the sake of simplicity, I have not used any learning rate scheduling here, but if required you can add it yourself for your particular training process. Further, it is always good idea to start with ADAM optimizer while compiling the model, but in this particular case with CityScapes dataset, authors have used only SGD. But in general case, it\u2019s always good to start with ADAM optimizer and then move towards other different variations if required. For the loss function, authors have used Cross Entropy loss and so have used here during the implementation.",
            "In the paper, authors have used 19 categories from CityScapes Dataset for the training and evaluation. With this implementation you can tweak your model as per desired with any number of output you might require for your particular project.",
            "Here are some of the validation results from Fast-SCNN, compared with input image and ground truth.",
            "I hope you enjoyed this implementation article and if you think I have made any mistake during the implementation or explanation process, feel free to correct me or suggest me the changes.",
            "In this way, we can easily implement Fast-SCNN using Tensorflow 2.0 and its high level api TF.Keras. Below are the references I have used while implementing the model.",
            "For full code, you can visit my GitHub Repo:",
            "github.com",
            "Link to the Original paper: https://arxiv.org/abs/1902.04502 Link to PSPNet Original paper: https://arxiv.org/pdf/1612.01105.pdf Link to ContextNet original paper: https://arxiv.org/abs/1805.04554 CityScapes Dataset official website: https://www.cityscapes-dataset.com/ Official Guide to Tensorflow 2.0: https://www.tensorflow.org/alpha Full code for the implementation: https://github.com/kshitizrimal/Fast-SCNN/blob/master/tf_2_0_fast_scnn.py Official Implementation of ContextNet: https://www.toshiba.eu/eu/Cambridge-Research-Laboratory/Computer-Vision/Resources/ContextNet/?fbclid=IwAR1T-eLK_xLq1Hu7Xz161YCaKzoZBtQMyvUFTySxbEqM6NNHY7xWV7nq9rA Pyramid Pooling Module code, inspired from the PSPNet Implementation: https://github.com/dhkim0225/keras-image-segmentation/blob/master/model/pspnet.py Bottleneck Residual block code inspired from MobileNet V2 Implementation: https://github.com/xiaochus/MobileNetV2/blob/master/mobilenet_v2.py"
        ]
    },
    "https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a": {
        "author": "Ayush Thakur",
        "length": "12 min read",
        "title": "Interpretability in Deep Learning with W&B \u2014 CAM and GradCAM",
        "tags": [
            "model-interpretability",
            "wandb",
            "convolutional-network",
            "deep-learning",
            "production-ml"
        ],
        "content": [
            "Training a classification model is interesting, but have you ever wondered how your model is making its predictions? Is your model actually looking at the dog in the image before classifying it as a dog with 98% accuracy? Interesting, isn\u2019t it. In today\u2019s report, we will explore why deep learning models need to be interpretable, and some interesting methods to peek under the hood of a deep learning model. Deep learning interpretability is a very exciting area of research and much progress is being made in this direction already.",
            "So why should you care about interpretability? After all, the success of your business or your project is judged primarily by how good the accuracy of your model is. But in order to deploy our models in the real world, we need to consider other factors too. For instance, is racially biased? Or, what if it\u2019s classifying humans with 97% accuracy, but while it classifies men with 99% accuracy, it only achieves 95% accuracy on women?",
            "Understanding how a model makes its predictions can also help us debug your network. [Check out this blog post on \u2018Debugging Neural Networks with PyTorch and W&B Using Gradients and Visualizations\u2019 for some other techniques that can help].",
            "At this point, we are all familiar with the concept that deep learning models make predictions based on the learned representation expressed in terms of other simpler representations. That is, deep learning allows us to build complex concepts out of simpler concepts. Here\u2019s an amazing Distill Pub post to help you understand this concept better. We also know that these representations are learned while we train the model with our input data and the label, in case of some supervised learning task like image classification. One of the criticisms of this approach is that the learned features in a neural network are not interpretable.",
            "Today we\u2019ll look at 2techniques that address this criticism and shed light into neural networks\u2019 \u201cblack-box\u201d nature of learning.",
            "Class Activation Map(CAM) Gradient CAM",
            "It has been observed that convolution units of various layers of a convolutional neural network act as an object detector even though no such prior about the location of the object is provided while training the network for a classification task. Even though convolution has this remarkable property, it is lost when we use a fully connected layer for the classification task. To avoid the use of a fully connected network some architectures like Network in Network(NiN) and GoogLeNet are fully convolutional neural networks.",
            "Global Average Pooling(GAP) is a very commonly used layer in such architectures. It is mainly used as a regularizer to prevent overfitting while training. The authors of Learning Deep Features for Discriminative Localization found out that by tweaking such an architecture, they can extend the advantages of GAP and can retain its localization ability until the last layer. Let\u2019s try to quickly understand the procedure of generating CAM using GAP.",
            "The class activation map simply indicates the discriminative region in the image which the CNN uses to classify that image in a particular category. For this technique, the network consists of ConvNet and just before the Softmax layer(for multi-class classification), global average pooling is performed on the convolutional feature maps. The output of this layer is used as features for a fully-connected layer that produces the desired classification output. Given this simple connectivity structure, we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps.",
            "Let\u2019s try to implement this. \ud83d\ude04",
            "Suppose you have built your deep classifier with Conv blocks and a few fully connected layers. We will have to modify this architecture such that there aren\u2019t any fully connected layers. We will use the GlobalAveragePooling2D layer between the output layer (softmax/sigmoid) and the last convolutional block.",
            "The CAMmodel provides a required modification to our cat and dog classifier. Here I am using pre-trained VGG16 model to simulate my already trained cat-dog classifier.",
            "def CAMmodel(): ## Simulating my pretrained dog and cat classifier. vgg = VGG16(include_top=False, weights='imagenet') vgg.trainable = False ## Flatten the layer so that it's not nested in the sequential model. vgg_flat = flatten_model(vgg) ## Insert GAP vgg_flat.append(keras.layers.GlobalAveragePooling2D()) vgg_flat.append(keras.layers.Dense(1, activation='sigmoid')) model = keras.models.Sequential(vgg_flat) return model",
            "A simple utility flatten_model returns the list of layers in my pre-trained model. This is done so that the layers are not nested when modified using Sequential model and the last convolutional layer can be accessed and used as an output. I appended GlobalAveragePooling2D and Dense in the returned array from flatten_model. Finally, the Sequential model is returned.",
            "def flatten_model(model_nested): ''' Utility to flatten pretrained model ''' layers_flat = [] for layer in model_nested.layers: try: layers_flat.extend(layer.layers) except AttributeError: layers_flat.append(layer) return layers_flat",
            "Next we call model.build() with the appropriate model input shape.",
            "keras.backend.clear_session() model = CAMmodel() model.build((None, None, None, 3)) # Note model.summary()",
            "Since a new layer was introduced, we have to retrain the model. But we don\u2019t need to retrain the entire model. We can freeze the convolutional blocks by using vgg.trainable=False.",
            "Observations:",
            "There is a decline in the model performance in terms of both training and validation accuracy. The optimal train and validation accuracy that I achieved was 99.01% and 95.67% respectively. Thus for the implementation of CAM, we have to modify our architecture and thus a decline in model performance.",
            "In the __init__for the CAM class, we initialize cammodel. Notice there are two outputs from this cammodel:",
            "Output from the last convolutional layer ( block5_conv3 here) The model prediction (softmax/sigmoid).",
            "class CAM: def __init__(self, model, layerName): self.model = model self.layerName = layerName ## Prepare cammodel last_conv_layer = self.model.get_layer(self.layerName).output self.cammodel = keras.models.Model(inputs=self.model.input, outputs=[last_conv_layer, self.model.output]) def compute_heatmap(self, image, classIdx): ## Get the output of last conv layer and model prediction [conv_outputs, predictions] = self.cammodel.predict(image) conv_outputs = conv_outputs[0, :, :, :] conv_outputs = np.rollaxis(conv_outputs, 2) ## Get class weights between class_weights = self.model.layers[-1].get_weights()[0] ## Create the class activation map. caml = np.zeros(shape = conv_outputs.shape[1:3], dtype=np.float32) for i, w in enumerate(class_weights[:]): caml += w * conv_outputs[i, :, :] caml /= np.max(caml) caml = cv2.resize(caml, (image.shape[1], image.shape[2])) ## Prepare heat map heatmap = cv2.applyColorMap(np.uint8(255*caml), cv2.COLORMAP_JET) heatmap[np.where(caml < 0.2)] = 0 return heatmap def overlay_heatmap(self, heatmap, image): img = heatmap*0.5 + image img = img*255 img = img.astype('uint8') return (heatmap, img)",
            "The compute_heatmap method is responsible for generating the heatmap which is the discriminative region used by CNN to identify the category (class of image).",
            "cammodel.predict() on the input image will give the feature map of the last convolutional layer of shape (1,7,7,512) . We also extract the weights of the output layer of shape (512,1) . Finally, the dot product of the extracted weights from the final layer and the feature map is calculated to produce the class activation map.",
            "Now we wrap everything in a callback. The CamLogger callback integrates wandb.log() method to log the generated activation maps onto the W&B run page. The heatmap returned from the CAM is finally overlayed on the original image by calling overlay_heatmap() method.",
            "We can draw lot of conclusions from the the plots as shown below. \ud83d\udc47 Note the examples chart contains validation images along with their prediction scores. If the prediction score is greater than 0.5, the network classifies the image as a dog, otherwise as a cat. While CAM charts have their corresponding class activation maps. Let's go through some observations:",
            "The model is classifying the images as dogs by looking at the facial region in the image. For some images it\u2019s able to look at the entire body, except the paws. The model is classifying the images as cats by looking at the ears, paws and whiskers.",
            "For a misclassified image the model is not looking at where it should be looking. Thus by using CAM we are able to interpret the reason behind this misclassification, which is really cool.",
            "Why is that? Even though the ears, paws and whiskers are present in the image why did it look at something else. One reason I can think of is that since we haven\u2019t fine tuned our pretrained VGG16 on our cat-dog dataset, the CNN as feature extractor is not entirely familiar with the patterns (distributions) appearing in our dataset.",
            "When multiple instances of the same class are present in the image, the model looks only at one of them. But that is okay, given that we are not concerned about object detection. Note that the confidence is low because of this.",
            "Other use cases:",
            "CAM can be used for a weakly supervised object localization task. The authors of the linked paper tested the ability of the CAM for a localization task on the ILSVRC 2014 benchmark dataset. The technique was able to achieve 37.1% top-5 error for object localization on this dataset, which is close to the 34.2% top-5 error achieved by a fully supervised CNN approach.",
            "Even though CAM was amazing it had some limitations:",
            "The model needs to be modified in order to use CAM. The modified model needs to be retrained, which is computationally expensive. Since fully connected Dense layers are removed. the model performance will surely suffer. This means the prediction score doesn\u2019t give the actual picture of the model\u2019s ability. The use case was bound by architectural constraints, i.e., architectures performing GAP over convolutional maps immediately before output layer.",
            "What makes a good visual explanation?:",
            "Certainly the technique should localize the class in the image. We saw this in CAM and it was worked remarkable good. Finer details should be captured, i.e., the activation map should be high resolution.",
            "Thus the authors of Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, a really amazing paper, came up with modifications to CAM and previous approaches. Their approach uses the gradients of any target prediction flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the class of the image.",
            "Thus Grad-CAM is a strict generalization over CAM. Beside overcoming the limitations of CAM it\u2019s applicable to different deep learning tasks involving CNNs. It is applicable to:",
            "CNNs with fully-connected layers (e.g. VGG) without any modification to the network. CNNs used for structured outputs like image captioning. CNNs used in tasks with multi-modal inputs like visual Q&A or reinforcement learning, without architectural changes or re-training.",
            "Let\u2019s implement this \ud83d\ude04",
            "We will focus on the image classification task. Unlike CAM we don\u2019t have to modify our model for this task and retrain it.",
            "I have used a VGG16 model pretrained on ImageNet as my base model and I'm simulating Transfer Learning with this.",
            "The layers of the baseline model are turned to non-trainable by using vgg.trainable = False. Note how I have used fully connected layers in the model.",
            "def catdogmodel(): inp = keras.layers.Input(shape=(224,224,3)) vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp, input_shape=(224,224,3)) vgg.trainable = False x = vgg.get_layer('block5_pool').output x = tf.keras.layers.GlobalAveragePooling2D()(x) x = keras.layers.Dense(64, activation='relu')(x) output = keras.layers.Dense(1, activation='sigmoid')(x) model = tf.keras.models.Model(inputs = inp, outputs=output) return model",
            "You will find the class GradCAM in the linked notebook. This is a modified implementation from Grad-CAM: Visualize class activation maps with Keras, TensorFlow, and Deep Learning, an amazing blog post, by Adrian Rosebrook of PyImageSearch.com. I would highly suggest checking out the step by step implementation of the GradCAM class in that blog post.",
            "I made two modifications to it:",
            "While doing transfer learning, that is, if your target (last) convolutional layer is non trainable, tape.gradient(loss, convOutputs) will return None . This is because tape.gradient() by default does not trace non-trainable variables/layers. Thus to use that layer for computing your gradients you need to allow GradientTape to watch it by calling tape.watch() on the target layer output (tensor). Hence the change,",
            "with tf.GradientTape() as tape: tape.watch(self.gradModel.get_layer(self.layerName).output) inputs = tf.cast(image, tf.float32) (convOutputs, predictions) = self.gradModel(inputs)",
            "The original implementation didn\u2019t account for binary classification. The original authors also talked about softmax-ing the output. So in order to train a simple cat and dog classifier, I made a small modification. Hence the change,",
            "if len(predictions)==1: # Binary Classification loss = predictions[0] else: loss = predictions[:, classIdx]",
            "The GRADCAM class can be used after the model is trained or as a callback. Here's a small excerpt from his blog post.",
            "The third point motivated me to work on this project. I built a custom callback around this GRADCAM implementation and used wandb.log() to log the activation maps. Thus by using this callback you can use GradCAM while training.",
            "Given we\u2019re working with a simple dataset I have only trained for few epochs and the model seems to work well.",
            "Here\u2019s the GradCAM custom callback.",
            "class GRADCamLogger(tf.keras.callbacks.Callback): def __init__(self, validation_data, layer_name): super(GRADCamLogger, self).__init__() self.validation_data = validation_data self.layer_name = layer_name def on_epoch_end(self, logs, epoch): images = [] grad_cam = [] ## Initialize GRADCam Class cam = GradCAM(model, self.layer_name) for image in self.validation_data: image = np.expand_dims(image, 0) pred = model.predict(image) classIDx = np.argmax(pred[0]) ## Compute Heatmap heatmap = cam.compute_heatmap(image, classIDx) image = image.reshape(image.shape[1:]) image = image*255 image = image.astype(np.uint8) ## Overlay heatmap on original image heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1])) (heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5) images.append(image) grad_cam.append(output) wandb.log({\"images\": [wandb.Image(image) for image in images]}) wandb.log({\"gradcam\": [wandb.Image(cam) for cam in grad_cam]})",
            "GradCAM being a strict generalization over CAM, should be preferred over CAM. To understand the theoretical underpinnings of this technique I recommend reading Demystifying Convolutional Neural Networks using GradCam by Divyanshu Mishra or simply reading the linked paper. A couple interesting conclusions we can draw include:",
            "The model looks at the face of the dogs to classify them correctly, while I am unsure about the cat.",
            "The model is able to localize multiple instances of the class in an image, i.e. the prediction score is accounting for multiple dogs and cats in the image.",
            "Class Activation Maps and Grad-CAMs are a few approaches that introduce some explainability/interpretability into deep learning models, and are quite widely used. What\u2019s most fascinating about these techniques is the ability to perform the object localization task, even without training the model with a location prior. GradCAM, when used for image captioning, can help us understand what region in the image is used to generate a certain word. When used for a Visual Q&A task, it can help us understand why the model came to a particular answer. Even though Grad-CAM is class-discriminative and localizes the relevant image regions, it lacks the ability to highlight fine-grained details the way pixel-space gradient visualization methods like Guided backpropagation, and Deconvolution do. Thus the authors combined Grad-CAM with Guided backpropagation.",
            "Thanks for reading this report until the end. I hope you find the callbacks introduced here helpful for your deep learning wizardry. Please feel free to reach out to me on Twitter(@ayushthakur0) for any feedback on this report. Thank you."
        ]
    },
    "https://towardsdatascience.com/custom-object-detection-using-tensorflow-from-scratch-e61da2e10087": {
        "author": "Khush Patel",
        "length": "8 min read",
        "title": "Custom Object Detection using TensorFlow from Scratch",
        "tags": [
            "tensorflow",
            "deep-learning",
            "artificial-intelligence",
            "object-detection",
            "computer-vision"
        ],
        "content": [
            "In this tutorial, we\u2019re going to get our hands dirty and train our own dog (corgi) detector using a pre-trained SSD MobileNet V2 model.",
            "Instead of training your own model from scratch, you can build on existing models and fine-tune them for your own purpose without requiring as much computing power.",
            "Install Tensorflow using the following command:",
            "$ pip install tensorflow",
            "If you have a GPU that you can use with Tensorflow:",
            "$ pip install tensorflow-gpu",
            "$ pip install pillow Cython lxml jupyter matplotlib",
            "Install protobuf using Homebrew (you can learn more about Homebrew here)",
            "$ brew install protobuf",
            "For protobuf installation on other OS, follow the instructions here.",
            "In this tutorial, we\u2019re going to use resources in the Tensorflow models repository. Since it does not come with the Tensorflow installation, we need to clone it from their Github repo:",
            "First change into the Tensorflow directory:",
            "# For example: ~/anaconda/envs/<your_env_name>/lib/python3.6/site-packages/tensorflow $ cd <path_to_your_tensorflow_installation>",
            "Clone the Tensorflow models repository:",
            "$ git clone https://github.com/tensorflow/models.git",
            "From this point on, this directory will be referred to as the modelsdirectory",
            "Every time you start a new terminal window to work with the pre-trained models, it is important to compile Protobuf and change your PYTHONPATH.",
            "Run the following from your terminal:",
            "$ cd <path_to_your_tensorflow_installation>/models/research/ $ protoc object_detection/protos/*.proto --python_out=. $ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim",
            "Run a quick test to confirm that the Object Detection API is working properly:",
            "$ python object_detection/builders/model_builder_test.py",
            "If the result looks like the following, you\u2019re ready to proceed to the next steps!",
            "............... ---------------------------------------------------------------------- Ran 15 tests in 0.123s OK",
            "To make this tutorial easier to follow along, create the following folder structure within the models directory you just cloned:",
            "models \u251c\u2500\u2500 annotations |   \u2514\u2500\u2500 xmls \u251c\u2500\u2500 images \u251c\u2500\u2500 checkpoints \u251c\u2500\u2500 tf_record \u251c\u2500\u2500 research ...",
            "These folders will be used to store the required components for our model as we proceed.",
            "You can collect data in either images or video format. Here I mentioned both ways to collect data.",
            "Data preparation is the most important part of training your own model. Since we\u2019re going to train a corgi detector, we must collect pictures of corgis! About 200 of them would be sufficient.",
            "I recommend using google-images-download to download images. It searches Google Images and then downloads images based on the inputs you provided. In the inputs, you can specify search parameters such as keywords, number of images, image format, image size, and usage rights.",
            "Since we\u2019re downloading more than 100 images at a time, we need a chromedriver in the models directory (download here). Once you have the chromedriver ready, you could use this sample command to download images. Make sure all your images are in the jpg format:",
            "# From the models directory $ googleimagesdownload --keywords 'welsh corgi dog' \\ --limit 200 \\ --size medium \\ --chromedriver ./chromedriver \\ --format jpg",
            "After downloading, save all images to models/images/. To make subsequent processes easier, let's rename the images as numbers (e.g. 1.jpg, 2.jpg) by running the following script:",
            "import os path = 'models/images/' counter = 1 for f in os.listdir(path): suffix = f.split('.')[-1] if suffix == 'jpg' or suffix == 'png': new = '{}.{}'.format(str(counter), suffix) os.rename(path + f, path + new) counter = int(counter) + 1",
            "medium.com",
            "Once you\u2019ve collected all the images you need, you need to label them manually. There are many packages that serve this purpose. labelImg is a popular choice.",
            "labelImg provides a user-friendly GUI. Plus, it saves label files (.xml) in the popular Pascal VOC format. If you want these images for training YOLO(You only Look Once) then use YOLO. Just set the current directory and Save directory as per our structure.",
            "Here's what a labelled image looks like in labelImg:",
            "Double check that every image has a corresponding .xml file and save them in models/annotations/xmls/.",
            "For a large number of annotations, you can use different shortcuts mentioned below:",
            "Ctrl + u - Load all of the images from a directory Ctrl + r - Change the default annotation target dir Ctrl + s - Save w - Create a rect box d - Next image a - Previous image del - Delete the selected rect box Ctrl++ - Zoom in Ctrl-- - Zoom out Ctrl + d - Copy the current label and rect box Space - Flag the current image as verified \u2191\u2192\u2193\u2190Keyboard arrows to move selected rect box",
            "Classes need to be listed in the label map. Since we\u2019re only detecting corgis, the label map should contain only one item like the following:",
            "item { id: 1 name: 'corgi' }",
            "Note that id must start from 1, because 0 is a reserved id.",
            "Save this file as label_map.pbtxt in models/annotations/",
            "trainval.txt is a list of image names without file extensions. Since we have sequential numbers for image names, the list should look like this:",
            "1 2 3 ... 198 199 200",
            "Save this file as trainval.txt in models/annotations/",
            "You can use this link to create XML files to CSV. We have all images and their bounding boxes are in XML format. Also all image has separate XML file so using this code we are creating a CSV file which contains all the XML files and their bounding box co-ordinates to single CSV file which is input for creating TFrecords.",
            "TFRecord is an important data format designed for Tensorflow. (Read more about it here). Before you can train your custom object detector, you must convert your data into the TFRecord format.",
            "Since we need to train as well as validate our model, the data set will be split into training (train.record) and validation sets (val.record). The purpose of training set is straight forward - it is the set of examples the model learns from. The validation set is a set of examples used DURING TRAINING to iteratively assess model accuracy.",
            "We\u2019re going to use create_tf_record.py to convert our data set into train.record and val.record. Download here and save it to models/research/object_detection/dataset_tools/.",
            "Just change the label name in if row_label == \u2018Label1\u2019: as per your classifications.",
            "This script is preconfigured to do 70\u201330 train-val split. Execute it by running:",
            "# From the models directory $ python research/object_detection/dataset_tools/create_tf_record.py",
            "If the script is executed successfully, train.record and val.record should appear in your models/research/ directory. Move them into the models/tf_record/ directory.",
            "There are many pre-trained object detection models available in the model zoo. In order to train them using our custom data set, the models need to be restored in Tensorflow using their checkpoints (.ckpt files), which are records of previous model states.",
            "For this tutorial, we\u2019re going to download ssd_mobilenet_v2_coco here and save its model checkpoint files (model.ckpt.meta, model.ckpt.index, model.ckpt.data-00000-of-00001) to our models/checkpoints/ directory.",
            "Each of the pretrained models has a config file that contains details about the model. To detect our custom class, the config file needs to be modified accordingly.",
            "The config files are included in the models directory you cloned in the very beginning. You can find them in:",
            "models/research/object_detection/samples/configs",
            "In our case, we\u2019ll modify the config file for ssd_mobilenet_v2_coco. Make a copy of it first and save it in the models/ directory.",
            "Here are the items we need to change:",
            "Since we\u2019re only trying to detect corgis, change num_classes to 1 fine_tune_checkpoint tells the model which checkpoint file to use. Set this to checkpoints/model.ckpt The model also needs to know where the TFRecord files and label maps are for both training and validation sets. Since our train.record and val.record are saved in tf_record folder, our config should reflect that:",
            "train_input_reader: { tf_record_input_reader { input_path: \"tf_record/train.record\" } label_map_path: \"annotations/label_map.pbtxt\" } eval_input_reader: { tf_record_input_reader { input_path: \"tf_record/val.record\" } label_map_path: \"annotations/label_map.pbtxt\" shuffle: false num_readers: 1 }",
            "At this point, your models directory should look like this:",
            "models \u251c\u2500\u2500 annotations |   \u251c\u2500\u2500 label_map.pbtxt |   \u251c\u2500\u2500 trainval.txt |   \u2514\u2500\u2500 xmls |       \u251c\u2500\u2500 1.xml |       \u251c\u2500\u2500 2.xml |       \u251c\u2500\u2500 ... | \u251c\u2500\u2500 images |   \u251c\u2500\u2500 1.jpg |   \u251c\u2500\u2500 2.jpg |   \u251c\u2500\u2500 ... | \u251c\u2500\u2500 checkpoints |   \u251c\u2500\u2500 model.ckpt.data-00000-of-00001 |   \u251c\u2500\u2500 model.ckpt.index |   \u2514\u2500\u2500 model.ckpt.meta | \u251c\u2500\u2500 tf_record |   \u251c\u2500\u2500 train.record |   \u2514\u2500\u2500 val.record | \u251c\u2500\u2500 research |   \u251c\u2500\u2500 ... ...",
            "If you have successfully completed all previous steps, you\u2019re ready to start training!",
            "Follow the steps below:",
            "# Change into the models directory $ cd tensorflow/models # Make directory for storing training progress $ mkdir train # Make directory for storing validation results $ mkdir eval # Begin training $ python research/object_detection/train.py \\ --logtostderr \\ --train_dir=train \\ --pipeline_config_path=ssd_mobilenet_v2_coco.config",
            "Training time varies depending on the computing power of your machine.",
            "Evaluation can be run in parallel with training. The eval.py script checks the train directory for progress and evaluate the model based on the most recent checkpoint.",
            "# From the models directory $ python research/object_detection/eval.py \\ --logtostderr \\ --pipeline_config_path=ssd_mobilenet_v2_coco.config \\ --checkpoint_dir=train \\ --eval_dir=eval",
            "You can visualize model training progress using Tensorboard:",
            "# From the models directory $ tensorboard --logdir=./",
            "Based on the graphs output by Tensorboard, you may decide when you want to stop training. Usually, you may stop the process when the loss function is tapering off and no longer decreasing by a significant amount. In my case, I stopped at step 3258.",
            "Once you finish training your model, you can export your model to be used for inference. If you\u2019ve been following the folder structure, use the following command:",
            "# From the models directory $ mkdir fine_tuned_model $ python research/object_detection/export_inference_graph.py \\ --input_type image_tensor \\ --pipeline_config_path ssd_mobilenet_v2_coco.config \\ --trained_checkpoint_prefix  train/model.ckpt-<the_highest_checkpoint_number> \\ --output_directory fine_tuned_model",
            "Now that you have a model, you can use it to detect corgis in pictures and videos! For the purpose of demonstration, we\u2019re going to detect corgis in an image. Before you proceed, pick an image you want to test the model with.",
            "The models directory came with a notebook file (.ipynb) that we can use to get inference with a few tweaks. It is located at models/research/object_detection/object_detection_tutorial.ipynb. Follow the steps below to tweak the notebook:",
            "MODEL_NAME = 'ssd_mobilenet_v2_coco_2018_03_29' PATH_TO_CKPT = 'path/to/your/frozen_inference_graph.pb' PATH_TO_LABELS = 'models/annotations/label_map.pbtxt' NUM_CLASSES = 1 Comment out cell #5 completely (just below Download Model ) Since we\u2019re only testing on one image, comment out PATH_TO_TEST_IMAGES_DIR and TEST_IMAGE_PATHS in cell #9 (just below Detection ) In cell #11 (the last cell), remove the for-loop, unindent its content, and add path to your test image:",
            "imagepath = 'path/to/image_you_want_to_test.jpg",
            "After following through the steps, run the notebook and you should see the corgi in your test image highlighted by a bounding box!",
            "There you have your custom corgi detector!",
            "Tensorflow Object Detection Model Documentation",
            "Do visit my Website: http://www.khushpatel.com"
        ]
    },
    "https://medium.com/mindboard/image-classification-with-variable-input-resolution-in-keras-cbfbe576126f": {
        "author": "Eric Muccino",
        "length": "4 min read",
        "title": "Image Classification with Variable Input Resolution in Keras",
        "tags": [
            "convolutional-network",
            "machine-learning",
            "deep-learning",
            "neural-networks",
            "masala"
        ],
        "content": [
            "Convolutional neural networks (CNN) are a type of neural network designed for image classification. For an introduction to CNNs, check out this post by Matthew Stewart.",
            "Many CNN architectures require that all input images have the same resolution (height and width). This happens when a convolutional layer is flattened and fully connected to a dense layer. Since the network has to initialize the dense layer weights, the convolutional layer resolution must be known, requiring the input resolution to be predetermined and constant. The fallback of this is that all images, both during training and inference, must be resized to fit the resolution dimensions, creating an extra step in image preprocessing and causing a loss of resolution on large images needing to be down-sampled. This could have negative impacts on model performance, depending on the image data used and what the model is trying predict.",
            "There is a way to avoid specifying input dimensions when setting up a CNN, allowing for variable image resolutions during training and inference. This is done by using global pooling layers after the final convolutional layer and before any dense layers within the CNN. Global pooling reduces each channel of a convolutional layer to a single value in a way that is not dependent on the resolution of the channel. This is usually done by taking either the average or the maximum of all of the values in the channel, leaving you with a single layer of neurons, one for each channel. The number of channels in a convolutional layer is defined in the model architecture and is independent of the channel resolutions.",
            "Now let\u2019s take a look at how to train a CNN with variable image resolution using Keras. We\u2019ll use the Imagenette dataset which can be found here. The following code was written with TensorFlow 2.1.",
            "First, we build our neural network. We will use one of the supplied CNN models in the applications module of Keras. Don\u2019t forget to include a global pooling layer.",
            "Next, we need to build a batch generator class that will load and preprocess our data during training. While we do this, we will have to keep in mind some limitations to the extent of our image resolution variability. If an image is too large, we may run out of memory. To avoid this, we set a threshold for the maximum height and width we will allow our images to be. Anything above this threshold will be down-sampled. Also, TensorFlow requires that each input tensor has uniform shape. This means we will have to pad the images of each batch to match the height and width of the largest image in the batch. Batches containing single images do not need to be padded, so we can run inference on images without any padding if we run them one at a time.",
            "All that is left is to specify our training hyperparameters, initialize train and test generators, and train our model.",
            "In this post, we\u2019ve explored how to use global pooling layers to create and train a CNN that is capable of handling variable image resolution. While we focused on the benefit of resolution variability, global pooling layers provide other benefits that we did not explore, such as nonlinearity and regularization. Consider using global pooling for reasons beyond input shapes.",
            "Keep in mind:While our model is capable of receiving any image resolution that fits in memory, it is generally a good idea to train on images that are roughly the same size. This is so that the model will encounter artifacts within images that are of similar scale, allowing it to generalize more easily. Similarly, for best results, we want to run inference on images that are close in size to those that were used in training. However, this will depend on the specifics of the use case and the images being used.",
            "Some improvements that could be made to the training data generator used in this tutorial: 1) Random batch image augmentations.2) Random batch padding styles.3) Parallel data processing.",
            "The Mindboard Data Science Team explores cutting-edge technologies in innovative ways to provide original solutions, including the Masala.AI product line. Masala provides media content rating services such as vRate, a browser extension that detects and blocks mature content with custom sensitivity settings. The vRate browser extension is available for download via the Chrome Web Store. Check out www.masala.ai for more info."
        ]
    },
    "https://medium.com/analytics-vidhya/image-super-resolution-using-gans-a10254f58892": {
        "author": "gautam iruvanti",
        "length": "4 min read",
        "title": "Image Super Resolution Using GANs",
        "tags": [
            "adversarial-network",
            "machine-learning",
            "data-science",
            "image-processing",
            "deep-learning"
        ],
        "content": [
            "Image super resolution is a technique of reconstructing a high resolution image from the observed low resolution image.Most of the approaches for Image Super Resolution till now used the MSE (mean squared error )as a loss function , the problem with MSE as a loss function is that the high texture details of the image are averaged to create a smooth reconstruction .",
            "GANs solve this problem by using the perceptual loss which drives the image reconstruction towards the natural image manifold producing perceptually more realistic and convincing solutions .",
            "What are GANs ?",
            "Generative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other (thus the \u201cadversarial\u201d) in order to generate new, synthetic instances of data that can pass for real data.",
            "The two neural networks are the generator and the discriminator . The generator tries to produce a new data instance and the discriminator tries to distinguish whether the data belongs to the training data set or not .",
            "links to learn more about GANs :",
            "machinelearningmastery.com",
            "pathmind.com",
            "Super Resolution GANs :",
            "Super-resolution GANs apply a deep network in combination with an adversarial network to produce higher resolution images. As mentioned above, SR GANs tend to produce images which are more appealing to humans with more details compared to an architecture built without GANs.",
            "1)Introduction",
            "SR-GANs consist of two networks a generator and a discriminator.",
            "The discriminator(critic) is trained to differentiate between the real HR image and the generated image and is implemented using CNNs .",
            "The generator is used to take input LR images and generate HR output images which is sent to the critic for evaluation and is implemented using ResNet blocks.",
            "The loss function is a multi component loss function consisting of content loss and adversarial loss.",
            "2)SR GAN Architecture",
            "3)Loss Function",
            "Perceptual loss function",
            "Previous approaches to image super resolution based their approach on MSE , but we improve upon this approach by using the perceptual loss function . The perceptual loss function generates images with respect to perceptually relevant characteristics.",
            "Perceptual Loss is the weighted sum of content loss and adversarial loss",
            "Dateset used",
            "The SRGAN was trained on the DIV2K data set , the data set consists of 800 HR images .",
            "The images are down sampled and then used for training . The data set has a large variety of images .",
            "Training",
            "The network was trained on google colab on the DIV2K data set for 350 epochs .",
            "Results",
            "Link to the code",
            "github.com",
            "References and further reading",
            "arxiv.org",
            "machinelearningmastery.com",
            "pathmind.com"
        ]
    },
    "https://towardsdatascience.com/learn-to-pay-attention-trainable-visual-attention-in-cnns-87e2869f89f1": {
        "author": "Rachel Lea Ballantyne Draelos",
        "length": "11 min read",
        "title": "Learn to Pay Attention! Trainable Visual Attention in CNNs",
        "tags": [
            "machine-learning",
            "attention",
            "cnn",
            "neural-networks",
            "artificial-intelligence"
        ],
        "content": [
            "When training an image model, we want the model to be able to focus on important parts of the image. One way of accomplishing this is through trainable attention mechanisms. In this post we will first discuss the difference between post-hoc vs. trainable attention and soft vs. hard attention. We\u2019ll then dive into the details of a 2018 ICLR paper, \u201cLearn to Pay Attention\u201d which describes one approach to trainable soft attention for image classification.",
            "Here is the key paper we will study in this post: Jetley S, Lord NA, Lee N, Torr PH. Learn to pay attention. ICLR 2018.",
            "There is also a nice Pytorch implementation of the paper available here: SaoYan/LearnToPayAttention",
            "Note: attention is also used extensively in natural language processing. The focus of this post is attention in computer vision tasks.",
            "Let\u2019s start with the English definition of the word \u201cattention\u201d:",
            "attention: notice taken of someone or something; the regarding of someone or something as interesting or important",
            "Similarly, in machine learning, \u201cattention\u201d refers to:",
            "definition (1): trainable attention: a group of techniques that help a \u201cmodel-in-training\u201d notice important things more effectively",
            "and",
            "definition (2): post-hoc attention: a group of techniques that help humans visualize what an already-trained model thinks is important",
            "When people think of attention, they usually think of definition (1), for trainable attention. A trainable attention mechanism is trained while the network is trained, and is supposed to help the network to focus on key elements of the image.",
            "Confusingly, post-hoc heatmap visualization techniques are also sometimes referred to as \u201cattention\u201d which is why I\u2019ve included definition (2). These post-hoc attention mechanisms create a heat map from an already-trained network, and include:",
            "I emphasize that these post-hoc techniques are not intended to change the way the model learns, or to change what the model learns. They are applied to an already-trained model with fixed weights, and are intended solely to provide insight into the model\u2019s decisions.",
            "Finally, let\u2019s look at the definition of an \u201cattention map\u201d from Jetley et al:",
            "attention map: a scalar matrix representing the relative importance of layer activations at different 2D spatial locations with respect to the target task",
            "i.e., an attention map is a grid of numbers that indicates what 2D locations are important for a task. Important locations correspond to bigger numbers and are usually depicted in red in a heat map. In the following example, the Grad-CAM attention maps for \u201cborder collie\u201d are shown, emphasizing the border collie\u2019s position in the original montage:",
            "Soft vs. Hard Attention",
            "You may have heard the terms \u201csoft attention\u201d and \u201chard attention\u201d floating around. The difference between them is as follows:",
            "Soft attention uses \u201csoft shading\u201d to focus on regions. Soft attention can be learned using good old backpropagation/gradient descent (the same methods that are used to learn the weights of a neural network model.) Soft attention maps typically contain decimals between 0 and 1. Hard attention uses image cropping to focus on regions. It cannot be trained using gradient descent because there\u2019s no derivative for the procedure \u201ccrop the image here.\u201d Techniques like REINFORCE can be used to train hard attention mechanisms. Hard attention maps consistent entirely of 0 or 1, and nothing in-between; 1 corresponds to a pixel that is kept, and 0 corresponds to a pixel that is cropped out.",
            "For more in-depth discussions of soft vs. hard attention, see this post (subsections \u201cWhat is Attention?\u201d, \u201cHard Attention,\u201d and \u201cSoft Attention\u201d) and this post (subsection \u201cSoft vs Hard Attention\u201d).",
            "The paper \u201cLearn to Pay Attention\u201d demonstrates one approach to soft trainable visual attention in a CNN model. The main task they consider is multiclass classification, in which the goal is to assign an input image to a single output class, e.g. assign a photo of a bear to the class \u201cbear.\u201d The authors demonstrate that soft trainable attention improves performance on multiclass classification by 7% on CIFAR-100, and they show example heat maps highlighting how the attention helps the model focus on parts of the image most relevant to the correct class label.",
            "Here\u2019s a diagram of their model, modified from Figure 2 of the paper:",
            "The model is based on the VGG convolutional neural network. There are different configurations of the VGG network, shown in Figure 2 here. (In case you\u2019re curious, the \u201cLearn to Pay Attention\u201d paper appears to be using a VGG configuration somewhere between configurations D an d E; specifically, there are three 256-channel layers like configuration D, but eight 512-channel layers like configuration E.)",
            "The main point of this figure is that the authors have made two key changes to the basic VGG setup:",
            "They have inserted attention estimators after layers 7, 10, and 13 (the layer numbers that I\u2019ve highlighted in yellow.) The attention estimator after layer 7 takes the output of layer 7 and calculates an \u201cattention mask\u201d of numbers between 0 and 1, which it then multiplies against the original output of layer 7 to produce \u201cg_a\u00b9\u201d (in the figure above.) The same process happens for the attention estimators after layers 10 and 13, to produce g_a\u00b2 and g_a\u00b3, respectively. The next big change is that the authors have taken away the fully connected layer that normally goes at the end of VGG to produce the prediction (there\u2019s normally another FC layer after the FC layer numbered \u201c16\u201d but they\u2019ve gotten rid of it.) Instead, the classification now happens through a new fully connected layer that takes in inputs from the three attention estimators.",
            "Before we dive into exactly how their attention mechanism works, here\u2019s a summary of the notation used in the paper, which we will be using throughout the remainder of the post:",
            "The \u201ccompatibility score\u201d is calculated using the local features l and the global feature vector g.",
            "The authors explain that the compatibility score is intended to have a high value when the image patch described by the local features \u201ccontains parts of the dominant image category.\u201d",
            "For example, if there\u2019s a cat in the image, we assume that the whole cat is described by the global feature vector g, and furthermore, we expect that a particularly \u201ccat-like\u201d patch (e.g. a patch over the cat\u2019s face) will produce local features l that produce a high compatibility score when combined with g.",
            "The authors propose two different ways of calculating the compatibility score c from local features l and global feature vector g:",
            "approach 1: \u201cparametrised compatibiliy\u201d or \u201cpc\u201d:",
            "approach 2: \u201cdot product\u201d or \u201cdp\u201d:",
            "(Spoiler alert: \u201cparametrised compatibility\u201d performed better in their results.)",
            "In \u201cparametrised compatibility\u201d we first add the local features to the global feature, l+g, and then we take the dot product with a learned vector u. Intuitively it seems that concatenation of l and g might make more sense than addition, but the authors state, \u201cgiven the existing free parameters between the local and the global image descriptors [\u2026] we can simplify the concatenation [\u2026] to an addition operation\u201d in order to \u201climit the parameters of the attention unit.\u201d (One of the reviewers also asked why they did addition instead of concatenation; see OpenReview.)",
            "In the \u201cdot product\u201d approach we simply take the dot product of the local features l and the global feature vector g. Note that for each conv layer where we apply attention (layers 7, 10, and 13) the local features l will be unique to that layer but the global feature vector g is the same.",
            "As it turns out, the l for conv layer 7 has 256 channels, but g has 512 channels. In order to add two vectors (pc approach) or take a dot product (dp approach), the vectors must be the same size.",
            "The authors say that if l and g are not the same size, then they first project g to the lower-dimensional space of l. The reason they don\u2019t instead project l to the higher-dimensional space of g is to limit the number of parameters. (By \u201cproject\u201d they mean apply a neural network layer to make l the same size as g.)",
            "Note that it\u2019s still an acceptable solution to project l to g; you\u2019ll just be making l bigger instead of making g smaller. This Pytorch implementation of \u201cLearn to Pay Attention\u201d projects l to g using the line \u201cc1, g1 = self.attn1(self.projector(l1), g)\u201d in which self.projector is a single convolutional layer that takes l which has an input of 256 channels and creates an output of 512 channels, to match g \u2018s 512 channels.",
            "All we\u2019re doing here is using a softmax to squish the compatibility scores c into the range (0,1) and we\u2019re calling the output a. For a review of the softmax operation, see this post.",
            "Here, we calculate the final output of the attention mechanism g_a for a particular layer s by taking a weighted combination of the l for that layer (recall that the l are just the outputs of that layer.) The weights we use are the attention weights a that we just calculated.",
            "Now we want to use the attention outputs g_a that we just calculated for layers 7, 10, and 13 to make a classification decision. The authors investigate two options:",
            "approach 1: \u201cconcat\u201d: first concatenate the attention outputs and then feed them together into a single fully connected layer to get the final predictions.",
            "approach 2: \u201cindep\u201d: feed each attention output into an independent fully connected layer to get intermediate predictions, then average those intermediate predictions to get the final predictions.",
            "(Spoiler alert: \u201cconcat\u201d performed better in their results.)",
            "The authors evaluate their attention mechanism on a variety of tasks, including multiclass classification with CIFAR-10, CIFAR-100, and SVHN. They find that use of their trainable attention mechanism improves performance over the \u201cno-attention\u201d baseline, in contrast to one of the post-hoc attention mechanisms ( CAM) which results in a decrease in performance (due to the architecture constraints imposed by the CAM approach.)",
            "The following variant of their method performed best: \u201cparametrised compatibility\u201d (calculating compatibility scores by adding l and g and then taking the dot product with a learned vector u) and \u201cconcat\u201d (concatenating the attention outputs g_a before feeding them into an fc layer to make predictions.)",
            "Here\u2019s part of Figure 3 from their paper showing some example attention maps focused on relevant objects:",
            "The appendix of their paper contains an interesting discussion on the utility of the global feature vector g in the calculation of the attention. Recall that both the \u201cpc\u201d and \u201cdp\u201d approaches for obtaining compatibility scores make use of the global feature vector g. The authors ran experiments where they swapped out which g they used, and found:",
            "for the dot-product-based attention mechanism [dp, worse-performing]: \u201cthe global vector plays a prominent role in guiding attention\u201d for the parametrised compatibility function [pc, better-performing]: \u201cthe global feature vector seems to be redundant. Any change in the global feature vector does not transfer to the resulting attention map. In fact, numerical observations show that the magnitudes of the global features are often a couple orders of magnitude smaller than those of the corresponding local features. Thus, a change in the global feature vector has little to no impact on the predicted attention scores. Yet, the attention maps themselves are able to consistently highlight object-relevant image regions. Thus, it appears that in the case of parametrised compatibility based attention, the object-centric high-order features are learned as part of the weight vector u .\u201d",
            "In other words, we might be able to get away with calculating the compatibility scores like this:",
            "without using g at all!",
            "Regardless of the exact role that g is playing, this \u201cLearn to Pay Attention\u201d method does improve results, and does produce nice attention maps \u2014 so the trainable attention is clearly doing something useful, even if it\u2019s not using g in the exact way the authors initially intended.",
            "Trainable attention mechanisms have attention weights that are learned during training, which help the model focus on key parts of images important for the task; Post-hoc attention mechanisms are techniques applied after a model is finished training, and are intended to provide insight into where the model is looking when it makes predictions; Hard attention is image cropping and can be trained using REINFORCE. Soft attention produces \u201chazier\u201d focus region(s) and can be trained using regular backpropagation. \u201cLearn to Pay Attention\u201d is an interesting paper demonstrating how soft trainable attention can improve image classification performance and highlight key parts of images.",
            "Attention in Neural Networks and How to Use It by Adam Kosiorek. This is a great article focused on attention in computer vision. It discusses why attention is useful, soft vs hard attention, Gaussian attention (with code), and Spatial Transformers (with code). Attention? Attention! by Lilian Weng. This is another great article, focused on attention in natural language processing. It discusses seq2seq, self-attention, soft vs. hard attention, global vs. local attention, Neural Turing Machines, Pointer Networks, Transformers, SNAIL, and Self-Attention GAN.",
            "Image Source: Performer with fire poi. Poi is a performing art focused on swinging tethered weights. It originated in New Zealand.",
            "Originally published at http://glassboxmedicine.com on August 10, 2019."
        ]
    },
    "https://towardsdatascience.com/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models-1ea4f67a9dbd": {
        "author": "Tarang Shah",
        "length": "9 min read",
        "title": "Measuring Object Detection models \u2014 mAP \u2014 What is Mean Average Precision?",
        "tags": [
            "machine-learning",
            "data-science",
            "deep-learning",
            "computer-vision",
            "object-detection"
        ],
        "content": [
            "For most common problems that are solved using machine learning, there are usually multiple models\u00a0available. Each one has its own quirks and would perform differently based on various factors.",
            "Each model is judged by its performance over a dataset, usually called the \u201cvalidation/test\u201d dataset. This performance is measured using various statistics \u2014 accuracy, precision, recall etc. The statistic of choice is usually specific to your particular application and use case. And for each application, it is critical to find a metric that can be used to objectively compare models.",
            "In this article, we will be talking about the most common metric of choice used for Object Detection problems\u200a\u2014\u200aThe Mean Average Precision aka, the mAP.",
            "Most\u00a0times, the metrics are easy to understand and calculate. For example, in binary classification, the precision and recall serve as an easy and intuitive statistic.",
            "Object detection on the other hand is a rather different and\u2026 interesting problem.",
            "Even if your object detector detects a cat in an image, it is not useful if you can\u2019t find where in the image it is located.",
            "Since you are predicting the occurence and position of the objects in an image, it is rather interesting how we calculate this metric.",
            "First, lets define the object detection problem, so that we are on the same page.",
            "By \u201cObject Detection Problem\u201d this is what I mean,",
            "Given an image, find the objects in it, locate their position and classify them.",
            "Object detection models are usually trained on a fixed set of classes, so the model would locate and classify only those classes in the image.",
            "Also, the location of the object is generally in the form of a bounding rectangle.",
            "So, object detection involves both localisation of the object in the image and classifying that object.",
            "Mean Average Precision, as described below, is particularly used for algorithms where we are predicting the location of the object along with the classes. Hence, from Image 1, we can see that it is useful for evaluating Localisation models, Object Detection Models and Segmentation models .",
            "Every image in an object detection problem could have different objects of different classes. As mentioned before, both the classification and localisation of a model need to be evaluated. Hence, the standard metric of precision used in image classification problems cannot be directly applied here. This is where mAP(Mean Average-Precision) is comes into the picture. I hope that at the end of this article you will be able to make sense of what it means and represents.",
            "For any algorithm, the metrics are always evaluated in comparison to the ground truth data. We only know the Ground Truth information for the Training, Validation and Test datasets.",
            "For object detection problems, the ground truth includes the image, the classes of the objects in it and the true bounding boxes of each of the objects **in that image.",
            "An example:",
            "We are given the actual image(jpg, png etc) and the other annotations as text(bounding box coordinates(x, y, width and height) and the class), the red box and text labels are only drawn on this image for us humans to visualise.",
            "So for this particular example, what our model gets during training is this",
            "And 3 sets of numbers defining the ground truth (lets assume this image is 1000x800px and all these coordinates are in pixels, also approximated)",
            "Now, lets get our hands dirty and see how the mAP is calculated.",
            "I will go into the various object detection algorithms, their approaches and performance in another article. For now, lets assume we have a trained model and we are evaluating its results on the validation set.",
            "Let\u2019s say the original image and ground truth annotations are as we have seen above. The training and validation data has all images annotated in the same way.",
            "The model would return lots of predictions, but out of those, most of them will have a very low confidence score associated, hence we only consider predictions above a certain reported confidence score.",
            "We run the original image through our model and this what the object detection algorithm returns after confidence thresholding,",
            "Image with bounding boxes -",
            "Now, since we humans are expert object detectors, we can say that these detections are correct. But how do we quantify this?",
            "We first need to know how much is the correctness of each of these detections. The metric that tells us the correctness of a given bounding box is the \u2014 IoU \u2014 Intersection over Union. It is a very simple visual quantity.",
            "In terms of words, some people would say the name is self explanatory, but we need a better explanation. I\u2019ll explain IoU in a brief manner, for those who really want a detailed explanation, Adrian Rosebrock has a really good article which you can refer to.",
            "Intersection over Union is a ratio between the intersection and the union of the predicted boxes and the ground truth boxes. This stat is also known as the Jaccard Index and was first published by Paul Jaccard in the early 1900s.",
            "To get the intersection and union values, we first overlay the prediction boxes over the ground truth boxes. (see image)",
            "Now for each class, the area overlapping the prediction box and ground truth box is the intersection area and the total area spanned is the union.",
            "The intersection and union for the horse class in the above would look like this,",
            "The intersection includes the overlap area(the area colored in Cyan), and the union includes the Orange and Cyan regions both.",
            "The IoU will then be calculated like this",
            "For calculating Precision and Recall, as with all machine learning problems, we have to identify True Positives, False Positives, True Negatives and False Negatives.",
            "To get True Positives and False Positives, we use IoU. Using IoU, we now have to identify if the detection(a Positive) is correct(True) or not(False). The most commonly used threshold is 0.5 \u2014 i.e. If the IoU is > 0.5, it is considered a True Positive, else it is considered a false positive. The COCO evaluation metric recommends measurement across various IoU thresholds, but for simplicity, we will stick to 0.5, which is the PASCAL VOC metric.",
            "For calculating Recall, we need the count of Negatives. Since every part of the image where we didnt predict an object is considered a negative, measuring \u201cTrue\u201d negatives is a bit futile. So we only measure \u201cFalse\u201d Negatives ie. the objects that our model has missed out.",
            "Also, another factor that is taken into consideration is the confidence that the model reports for every detection. By varying our confidence threshold we can change whether a predicted box is a Positive or Negative. Basically, all predictions(Box+Class) above the threshold are considered Positive boxes and all below it are Negatives.",
            "Now for every image, we have ground truth data which tells us the number of actual objects of a given class in that image.",
            "We now calculate the IoU with the Ground truth for every Positive detection box that the model reports. Using this value and our IoU threshold(say 0.5), we calculate the number of correct detections(A) for each class in an image. This is used to calculate the Precision for each class [TP/(TP+FP)]",
            "Precision = TP / (TP+FP)",
            "Since we already have calculated the number of correct predictions(A)(True Positives) and the Missed Detections(False Negatives) Hence we can now calculate the Recall (A/B) of the model for that class using this formula.",
            "Recall = TP / (TP+FN)",
            "The Mean Average Precision is a term which has different definitions. This metric is commonly used in the domains of Information Retrieval and Object Detection. Both these domains have different ways of calculating mAP. We will talk of the Object Detection relevant mAP.",
            "The currently popular Object Detection definition of mAP was first formalised in the PASCAL Visual Objects Classes(VOC) challenge in 2007, which included various image processing tasks. For the exact paper refer to this.",
            "We use the same approaches for calculation of Precision and Recall as mentioned in the previous section.",
            "But, as mentioned, we have atleast 2 other variables which determine the values of Precision and Recall, they are the IOU and the Confidence thresholds.",
            "The IOU is a simple geometric metric, which can be easily standardised, for example the PASCAL VOC challange evaluates mAP based on fixed 50% IOU. (The MSCOCO Challenge goes a step further and evaluates mAP at various threshold ranging from 5% to 95%). The confidence factor on the other hand varies across models, 50% confidence in my model design might probably be equivalent to an 80% confidence in someone else\u2019s model design, which would vary the precision recall curve shape. Hence the PASCAL VOC organisers came up with a way to account for this variation.",
            "We now need a metric to evaluate the models in a model agnostic way.",
            "The paper recommends that we calculate a measure called AP ie. the Average Precision",
            "For a given task and class, the precision/recall curve is computed from a method\u2019s ranked output. Recall is defined as the proportion of all positive examples ranked above a given rank. Precision is the proportion of all examples above that rank which are from the positive class. The AP summarises the shape of the precision/recall curve, and is de- fined as the mean precision at a set of eleven equally spaced recall levels [0,0.1,...,1]:",
            "This means that we chose 11 different confidence thresholds(which determine the \u201crank\u201d). The thresholds should be such that the Recall at those confidence values is 0, 0.1, 0.2, 0.3, \u2026 , 0.9 and 1.0. The AP is now defined as the mean of the Precision values at these chosen 11 Recall values. This results in the mAP being an overall view of the whole precision recall curve.",
            "The paper further gets into detail of calculating the Precision used in the above calculation.",
            "The precision at each recall level r is interpolated by taking the maximum precision measured for a method for which the corresponding recall exceeds r.",
            "Basically we use the maximum precision for a given recall value.",
            "The mAP hence is the Mean of all the Average Precision values across all your classes as measured above.",
            "This is in essence how the Mean Average Precision is calculated for Object Detection evaluation. There might be some variation at times, for example the COCO evaluation is more strict, enforcing various metrics with various IOUs and object sizes(more details here). If any of you want me to go into details of that, do let me know in the comments.",
            "So, to conclude, mean average precision is, literally, the average of all the average precisions(APs) of our classes in the dataset.",
            "Some important points to remember when we compare MAP values",
            "MAP is always calculated over a fixed dataset. Although it is not easy to interpret the absolute quantification of the model output, MAP helps us by bieng a pretty good relative metric. When we calculate this metric over popular public datasets, the metric can be easily used to compare old and new approaches to object detection. Depending on how the classes are distributed in the training data, the Average Precision values might vary from very high for some classes(which had good training data) to very low(for classes with less/bad data). So your MAP may be moderate, but your model might be really good for certain classes and really bad for certain classes. Hence it is advisable to have a look at individual class Average Precisions while analysing your model results. These values might also serve as an indicator to add more training samples.",
            "Originally published at tarangshah.com on January 27, 2018. Updated May 27, 2018"
        ]
    },
    "https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e": {
        "author": "Libor Vanek",
        "length": "6 min read",
        "title": "New Approaches to Object Detection",
        "tags": [
            "object-detection",
            "machine-learning",
            "tensorflow",
            "artificial-intelligence",
            "towards-data-science"
        ],
        "content": [
            "I will start with a short introduction of different approaches to object detection. After both traditional and newer approaches are presented, you can read about the most important parts of CenterNet and TTFNet. Many ideas in both models are similar, therefore they will be introduced together. We implemented a package inspired by both networks. Please check our GitHub if you are interested.",
            "Basic knowledge of deep learning and convolutional neural networks (CNNs) is required.",
            "As often in computer science, when we have a difficult problem without a solution, we try to change it to a problem whose solution we know or which is more simple. Good examples are so-called two-stage detection models. In this case, the more simple problem is an image classification or tagging. (Placing given image to a category or assigning tags to given image) Simply put, we can divide the image into multiple regions and then classify those, right? Yes \u2026 but it will take a lot of time. Therefore you need to be clever about it. An example of algorithms which uses this approach is R-CNN (2014). Which later evolved into Fast R-CNN (2015) and even Faster R-CNN (2016).",
            "Even though performance of those models is quite good, researchers were obviously asking themselves if this process can be made simple and therefore more efficient. In one stage, without the region proposals. One possible answer to this question is YOLO \u2014 You Only Look Once (2015). Which is now in the version 4 (2020). Or SSD \u2014 Single Shot multibox Detector (2015). Finally, RetinaNet (2017) should be mentioned as well, especially because it was used to introduced a focal loss for object detection, which is now used very often.",
            "In recent years, another idea has been gaining popularity. The object can be transformed to a set of points. And the detection task can be thought of as a keypoint estimation problem. This approach is introduced in CornerNet: Detecting Objects as Paired Keypoints. As the name suggests, an object is represented as a pair of keypoints, the top-left corner and the bottom-right corner.",
            "Similar idea is explored in Objects as Points paper, introducing CenterNet. There, we detect the center point of a bounding box using a heat map. Other properties such as size of the bounding box are predicted directly using regression.",
            "Weakness of this approach is a slower training. To address this issue, TTFNet (Training-Time-Friendly Network) was proposed. It follows the same basic ideas, therefore we decided to implement ideas from both networks in one package.",
            "Let\u2019s dive in! I will start from top, introducing you to the network layout. Then, important individual parts will be discussed, mainly the heatmaps and different loss functions.",
            "We can use either some network designed specifically for this kind of task. For example the Hourglass network. Or, as we have decided to do, take one of the standard image classification CNNs and modify it to our needs. We have chosen to test ResNet (18, 50) and EfficientNet (b0, b1, b2).",
            "As in all standard transfer learning tasks, we discarded the top dense layer. But the layer left at the top does not even remotely fit the output we need. Therefore, some upsampling is necessary. In addition to that, connections from lower layers improves the performance. When we have a layer of a correct size at the end of our network, we can \u201csplit\u201d it into the desired number of heads.",
            "In order to make the network faster, the heatmap side is just 1/4 of the input image. We have one heatmap per every class. Then, there are two other heads in CenterNet (for basic object detection) and one in TTFNet.",
            "For CenterNet, there is",
            "a size head, containing width and height of a bounding box offset head, containing x and y offset of the center which originated in using the down sampled heatmap.",
            "Both have only two filters \u2014 there could be only single object at any given point of the heatmap. If you are interested in other properties of an object, more heads can be added.",
            "In TTFNet, there is only one additional head with four filters to calculate the size \u2014 distances to the sides of an object.",
            "So, what does a heatmap look like? It is a matrix filled with values from 0.0 to 1.0. Peaks on such a map indicates the presence of of some object.",
            "Bellow, you see some generated training heatmaps. There is just one point with exactly 1.0. Around this point, the probabilities are slowly vanishing.",
            "The upsampling part of the network can be done in multiple ways. We have shown you the simple approach with concatenations, upsampling and standard convolutional layers. To improve the performance, deformable convolutions are used.",
            "There is not doubt that convolutional neural networks brought a great revolution to deep learning. They enabled us to extract feature which would be really hard to get from fully connected layers. And together with another layout improvements, out networks could become really deep. Nevertheless, the basic idea is still the same. Especially, the shape of the filters is always rectangular. Deformable Convolutions are trying to improve on that. They learn an offset to the standard grid and performs the convolutions with this \u201cdeformed\u201d kernel.",
            "Unfortunately, deformable convolutional layers are not yet implemented in TensorFlow nor TensorFlow Addons (TFA). We are using a fork of TFA with support for Deformable Conv2d and hoping it will be merged soon. (See pull request 1129.)",
            "Now, when we have the network layout and how the outputs look like, only one crucial thing is missing. Loss Functions.",
            "There is one problem with heatmaps \u2014 they are very sparse. Mostly there are no detection (zeros), only very occasionally we see an object (one, decreasing values in the surrounding). Standard metrics do not work very well in this setting. Fortunately, there is a solution \u2014 a focal loss. It is used in both CenteNet and TTFNet.",
            "To optimize bounding box size, CenterNet uses L1 loss. It is a simple sum of differences between true and predicted bounding box coordinates. It seems reasonable, but on the other hand, we do not use it for evaluation very often. We use IoU metric \u2014 Intersection over Union. So when we are interested in improving this metrics, why not use it for optimization as well? We can turn it to a loss by subtracting the IoU value from one. Or not?",
            "Unfortunately, the IoU is zero if there is no intersection. Therefore the loss is always one in such case. Two loss functions based on IoU were proposed. Generalized Intersection over Union loss, used in TTFNet, addresses this problem. Distance-IoU Loss focuses also on adding a distance information into the function, in other words, how far from the center of the bounding box we are.",
            "You can find our implementation on GitHub. I would like to thank Michal Lukac for working with me on this project, Ximilar for allowing us to opensource it. If you want to try the model without any coding, please check this blog post about object detection in Ximilar App."
        ]
    },
    "https://towardsdatascience.com/pp-yolo-surpasses-yolov4-object-detection-advances-1efc2692aa62": {
        "author": "Jacob Solawetz",
        "length": "8 min read",
        "title": "PP-YOLO Surpasses YOLOv4 \u2014 Object Detection Advances",
        "tags": [
            "computer-vision",
            "object-detection",
            "artificial-intelligence",
            "deep-learning"
        ],
        "content": [
            "PP-YOLO evaluation metrics show improved performance over YOLOv4, the incumbent state of the art object detection model. Yet, the Baidu authors write:",
            "This paper is not intended to introduce a novel object detector. It is more like a recipe, which tell you how to build a better detector step by step.",
            "Let\u2019s unpack that.",
            "YOLO was originally authored by Joseph Redmon to detect objects. Object detection is a computer vision technique that localizes and tags objects by drawing a bounding box around them and identifying the class label that a given box belongs too. Unlike massive NLP transformers, YOLO is designed to be tiny, enabling realtime inference speeds for deployment on device.",
            "YOLO-9000 was the second \u201cYOLOv2\u201d object detector published by Joseph Redmon, improving the detector and emphasizing the detectors ability to generalize to any object in the world.",
            "YOLOv3 made further improvements to the detection network and began to mainstream the object detection process. We began to publish tutorials on how to train YOLOv3 in PyTorch, how to train YOLOv3 in Keras, and compared YOLOv3 performance to EfficientDet (another state of the art detector).",
            "Then Joseph Redmon stepped out of the object detection game due to ethical concerns.",
            "Naturally, the open source community picked up the baton and continues to move YOLO technology forward.",
            "YOLOv4 was published recently this spring by Alexey AB in his for of the YOLO Darknet repository. YOLOv4 was primarily an ensemble of other known computer vision technologies, combined and validated through the research process. See here for a deep dive on YOLOv4. The YOLOv4 paper reads similarly to the PP-YOLO paper, as we will see below. We put together some great training tutorials on how to train YOLOv4 in Darknet.",
            "Then, just a few months ago YOLOv5 was released. YOLOv5 took the Darknet (C based) training environment and converted the network to PyTorch. Improved training techniques pushed performance of the model even further and created a great, easy to use, out of the box object detection model. Ever since, we have been encouraging developers using Roboflow to direct their attention to YOLOv5 for the formation of their custom object detectors via this YOLOv5 training tutorial.",
            "Enter PP-YOLO.",
            "PP is short for PaddlePaddle, a deep learning framework written by Baidu.",
            "If PaddlePaddle is new to you, then we are in the same boat. Primarily written in Python, PaddlePaddle seems akin to PyTorch and TensorFlow. A deep dive into the PaddlePaddle framework is intriguing, but beyond the scope of this article.",
            "The PP-YOLO paper reads much like the YOLOv4 paper in that it is a compilation of techniques that are known to work in computer vision. The novel contribution is to prove that the ensemble of these technologies improves performance, and to provide an ablation study of how much each step helps the model along the way.",
            "Before we dive into the contributions of PP-YOLO, it will be useful to review the YOLO detector architecture.",
            "The YOLO detector is broken into three main pieces.",
            "YOLO Backbone \u2014 The YOLO backbone is a convolutional neural network that pools image pixels to form features at different granularities. The Backbone is typically pretrained on a classification dataset, typically ImageNet.",
            "YOLO Neck \u2014 The YOLO neck (FPN is chosen above) combines and mixes the ConvNet layer representations before passing on to the prediction head.",
            "YOLO Head \u2014 This is the part of the network that makes the bounding box and class prediction. It is guided by the three YOLO loss functions for class, box, and objectness.",
            "The first PP YOLO technique is to replace the YOLOv3 Darknet53 backbone with the Resnet50-vd-dcn ConvNet backbone. Resnet is a more popular backbone, more frameworks are optimized for its execution, and it has fewer parameters than Darknet53. Seeing a mAP improvement by swapping this backbone is a huge win for PP YOLO.",
            "PP YOLO tracks the Exponential Moving Average of network parameters to maintain a shadow of the models weights for prediction time. This has been shown to improve inference accuracy.",
            "PP-YOLO bumps the batch size up from 64 to 192. Of course, this is hard to implement if you have GPU memory constraints.",
            "PP YOLO implements DropBlock regularization in the FPN neck (in the past, this has usually occurred in the backbone). DropBlock randomly removes a block of the training features at a given step in the network to teach the model to not rely on key features for detection.",
            "The YOLO loss function does not translate well to the mAP metric, which uses the Intersection over Union heavily in its calculation. Therefore, it is useful to edit the training loss function with this end prediction in mind. This edit was also present in YOLOv4.",
            "The PP-YOLO network adds a prediction branch to predict the model\u2019s estimated IOU with a given object. Including this IoU awareness when making the decision to predict an object or not improves performance.",
            "The old YOLO models do not do a good job of making predictions right around the boundaries of anchor box regions. It is useful to define box coordinates slightly differently to avoid this problem. This technique is also present in YOLOv4.",
            "Non-Maximum Suppression is a technique to remove over proposals of candidate objects for classification. Matrix NMS is a technique to sort through these candidate predictions in parallel, speeding up the calculation.",
            "CoordConv was motivated by the problems ConvNets were having with simply mapping (x,y) coordinates to a one-hot pixel space. The CoordConv solution gives the convolution network access to its own input coordinates. CoordConv interventions are marked with yellow diamonds above. More details are available in the CordConv paper.",
            "Spatial Pyramid Pooling is an extra block after the backbone layer to mix and pool spatial features. Also implemented in YOLOv4 and YOLOv5.",
            "The PP YOLO authors distilled down a larger ResNet model to serve as the backbone. A better pretrained model shows to improve downstream transfer learning as well.",
            "PP-YOLO outperforms the results YOLOv4 published on April 23, 2020.",
            "In fairness, the authors note this may be the wrong question to be asking. The authors\u2019 intent appears to not simply \u201cintroduce a new novel detector,\u201d rather to show the process of carefully tuning an object detector to maximize performance. Quoting the paper\u2019s introduction here:",
            "The focus of this paper is how to stack some effective tricks that hardly affect efficiency to get better performance\u2026 This paper is not intended to introduce a novel object detector. It is more like a recipe, which tell you how to build a better detector step by step. We have found some tricks that are effective for the YOLOv3 detector, which can save developers\u2019 time of trial and error. The final PP-YOLO model improves the mAP on COCO from 43.5% to 45.2% at a speed faster than YOLOv4",
            "(emphasis ours)",
            "The PP-YOLO contributions reference above took the YOLOv3 model from 38.9 to 44.6 mAP on the COCO object detection task and increased inference FPS from 58 to 73. These metrics are shown in the paper to beat the currently published results for YOLOv4 and EfficientDet.",
            "In benchmarking PP-YOLO against YOLOv5, it appears YOLOv5 still has the fastest inference time-to-accuracy performance (AP vs FPS) tradeoff on a V100. However, a YOLOv5 paper still remains to be released. Furthermore, it has been shown that training the YOLOv4 architecture on the YOLOv5 Ultralytics repository outperforms YOLOv5 and, transitively, YOLOv4 trained using YOLOv5 contributions would outperform the PP-YOLO results posted here. These results are still to be formally published but can be traced to this GitHub discussion.",
            "It is worth noting that many of the techniques (such as architecture search and data augmentation) that were used in YOLOv4 were not used in PP YOLO. This means that there is still room for the state of the art in object detection to grow as more of these techniques are combined and integrated together.",
            "Needless to say, is an exciting time to be implementing computer vision technologies.",
            "The PP-YOLO model shows the promise of state of the art object detection, but the improvements are incremental over other object detectors and it is written in a new framework. At this stage, the best thing to do is to develop your own empirical result by training PP-YOLO on your own dataset. (To be notified when you can easily use PP-YOLO on your dataset, subscribe to our newsletter.)",
            "In the meantime, I recommend checking out the following YOLO tutorials to get your object detector off the ground:",
            "How to Train YOLOv4 in Darknet How to Train YOLOv5 in PyTorch",
            "As always \u2014 happy training!"
        ]
    },
    "https://towardsdatascience.com/review-fsrcnn-super-resolution-80ca2ee14da4": {
        "author": "Sik-Ho Tsang",
        "length": "6 min read",
        "title": "Review: FSRCNN (Super Resolution)",
        "tags": [
            "machine-learning",
            "deep-learning",
            "data-science",
            "artificial-intelligence",
            "image-processing"
        ],
        "content": [
            "This time, FSRCNN, by CUHK, is reviewed. In this paper, a real-time super resolution approach is proposed. Fast Super-Resolution Convolutional Neural Network (FSRCNN) has been published in 2016 ECCV with nearly 300 citations when I was writing this story. (",
            "FSRCNN has a relatively shallow network which makes us easier to learn about the effect of each component. It is even faster with better reconstructed image quality than the previous SRCNN as the figure below.",
            "By comparing SRCNN and FSRCNN-s, FSRCNN-s (a small model size version of FSRCNN) has a better PSNR (image quality) and much shorter running time, in which 43.5 fps is obtained.",
            "By comparing SRCNN-Ex (A better SRCNN) and FSRCNN, FSRCNN has a better PSNR (image quality) and much shorter running time, in which 16.4 fps is obtained.",
            "So, let\u2019s see how it can achieve this.",
            "Brief Review of SRCNN FSRCNN Network Architecture Explanation of 1\u00d71 Convolution Used in Shrinking and Expanding Explanation of Multiple 3\u00d73 Convolutions in Non-Linear Mapping Ablation Study Results",
            "The above figure shows the network architectures of SRCNN and FSRCNN. In the figure, Conv(f,n,c) means the convolution with f\u00d7f filter size of n number of filters and c number of input channels.",
            "In SRCNN, the steps are as follows:",
            "Bicubic interpolation is done first to upsample to the desired resolution. Then 9\u00d79, 1\u00d71, 5\u00d75 convolutions are performed to improve the image quality. For the 1\u00d71 conv, it was claimed to be used for non-linear mapping of the low-resolution (LR) image vector and the high-resolution (HR) image vector.",
            "The computation complexity is:",
            "where it is linearly proportional to the size of HR image, SHR. The larger the HR image, the higher the complexity.",
            "In FSRCNN, 5 main steps as in the figure with more convolutions are involved:",
            "Feature Extraction : Bicubic interpolation in previous SRCNN is replaced by 5\u00d75 conv. Shrinking : 1\u00d71 conv is done to reduce the number of feature maps from d to s where s<<d. Non-Linear Mapping : Multiple 3\u00d73 layers are to replace a single wide one Expanding : 1\u00d71 conv is done to increase the number of feature maps from s to d. Deconvolution : 9\u00d79 filters are used to reconstruct the HR image.",
            "The overall structure above is called FSRCNN(d,s,m). And the computational complexity is:",
            "where it is linearly proportional to the size of LR image, SLR, which is much lower than that of SRCNN.",
            "PReLU is used as activation function. PReLU is the one with parametric leaky ReLU, which is claimed as better than ReLU. (If interested, please also read my PReLU review.)",
            "Cost function is just the standard mean square error (MSE):",
            "Suppose we need to perform 5\u00d75 convolution without the use of 1\u00d71 convolution as below:",
            "Number of operations = (14\u00d714\u00d748)\u00d7(5\u00d75\u00d7480) = 112.9M",
            "With the use of 1\u00d71 convolution:",
            "Number of operations for 1\u00d71 = (14\u00d714\u00d716)\u00d7(1\u00d71\u00d7480) = 1.5MNumber of operations for 5\u00d75 = (14\u00d714\u00d748)\u00d7(5\u00d75\u00d716) = 3.8MTotal number of operations = 1.5M + 3.8M = 5.3Mwhich is much much smaller than 112.9M !!!!!!!!!!!!!!!",
            "Network-In-Network (NIN) suggested 1\u00d71 conv introduces more non-linearity and improves the performance while GoogLeNet suggested 1\u00d71 conv helps to reduce the model size while maintaining the performance. (If interested, please read my GoogLeNet review.)",
            "Thus, 1\u00d71 is used in between two convolutions to reduce the number of connections (parameters). By reducing the parameters, we only need fewer multiplication and addition operations, and finally speed up the network. That\u2019s why FSRCNN is faster than SRCNN.",
            "By using 2 layers of 3\u00d73 filters, it actually have already covered 5\u00d75 area with fewer number of parameters as in the above figure.",
            "By using 1 layer of 5\u00d75 filter, number of parameters = 5\u00d75=25By using 2 layers of 3\u00d73 filters, number of parameters = 3\u00d73+3\u00d73=18Number of parameters is reduced by 28%",
            "With fewer parameters to be learnt, it is better for faster convergence, and reduced overfitting problem.",
            "This problem has been addressed in VGGNet. (If interested, please read my VGGNet review.)",
            "SRCNN-Ex : A better version of SRCNN, with 57184 parameters . Transition State 1 : Deconv is used, with 58976 parameters, higher PSNR is obtained. Transition State 2 : More convs are used at the middle, with 17088 parameters, even higher PSNR is obtained. FSRCNN (56,12, 4) : Smaller filter sizes and fewer filter number, with 12464 parameters, even higher PSNR is obtained. The improvement is due to fewer parameters to be trained, easier to converge.",
            "This shows there is contribution for each component.",
            "With higher m (m=4), higher PSNR.",
            "With m=4, d=56, s=12, it has the better tradeoff between the HR image quality (33.16dB) and the model complexity (12464 parameters).",
            "Finally, we got FSRCNN: FSRCNN (56,12,4). And a smaller version, FSRCNN-s: FSRCNN (32,5,1).",
            "Train the network from scratch using 91-image dataset under the upscaling factor 3, then fine-tune the deconvolutional layer only by adding the General-100 dataset under the upscaling factor 2 and 4. Data augmentation with scaling: 0.9, 0.8, 0.7, 0.6, and rotation of 90, 180, 270-degree.",
            "From the results above, FSRCNN and FSRCNN-s work well for upscaling factors 2 and 3. But for upscaling factor 4, FSRCNN and FSRCNN-s are slightly worse than SCN.",
            "From above figures, we can see that FSRCNN has much clearer image.",
            "In this paper, with such shallow network, we can know much about the effect of each component or technique, such as 1\u00d71 convolution and multiple 3\u00d73 convolutions.",
            "[2016 ECCV] [FSRCNN] Accelerating the Super-Resolution Convolutional Neural Network",
            "[SRCNN] [PReLU-Net] [GoogLeNet] [VGGNet]"
        ]
    },
    "https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7": {
        "author": "Sik-Ho Tsang",
        "length": "7 min read",
        "title": "Review: GoogLeNet (Inception v1)\u2014 Winner of ILSVRC 2014 (Image Classification)",
        "tags": [
            "machine-learning",
            "deep-learning",
            "convolutional-network",
            "neural-networks",
            "data-science"
        ],
        "content": [
            "In this story, GoogLeNet [1] is reviewed, which is the winner of the ILSVRC (ImageNet Large Scale Visual Recognition Competition) 2014, an image classification competition, which has significant improvement over ZFNet (The winner in 2013) [2] and AlexNet (The winner in 2012) [3], and has relatively lower error rate compared with the VGGNet (1st runner-up in 2014) [4].",
            "From the name \u201cGoogLeNet\u201d, we\u2019ve already known that it is from Google. And \u201cGoogLeNet\u201d also contains the word \u201cLeNet\u201d for paying tribute to Prof. Yan LeCun\u2019s LeNet [5]. This is a 2015 CVPR paper with about 9000 citations when I was writing this story. (",
            "It is also called Inception v1 as there are v2, v3 and v4 later on.",
            "The network architecture in this paper is quite different from VGGNet, ZFNet, and AlexNet. It contains 1\u00d71 Convolution at the middle of the network. And global average pooling is used at the end of the network instead of using fully connected layers. These two techniques are from another paper \u201cNetwork In Network\u201d (NIN) [6]. Another technique, called inception module, is to have different sizes/types of convolutions for the same input and stacking all the outputs.",
            "And authors also mentioned that the idea of the name \u201cInception\u201d, is coming from NIN and a famous internet meme below: WE NEED TO GO DEEPER.",
            "ImageNet, is a dataset of over 15 millions labeled high-resolution images with around 22,000 categories. ILSVRC uses a subset of ImageNet of around 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images and 100,000 testing images.",
            "The 1\u00d71 Convolution Inception Module Global Average Pooling Overall Architecture Auxiliary Classifiers for Training Testing Details",
            "The 1\u00d71 convolution is introduced by NIN [6]. 1\u00d71 convolution is used with ReLU. Thus, originally, NIN uses it for introducing more non-linearity to increase the representational power of the network since authors in NIN believe data is in non-linearity form. In GoogLeNet, 1\u00d71 convolution is used as a dimension reduction module to reduce the computation. By reducing the computation bottleneck, depth and width can be increased.",
            "I pick a simple example to illustrate this. Suppose we need to perform 5\u00d75 convolution without the use of 1\u00d71 convolution as below:",
            "Number of operations = (14\u00d714\u00d748)\u00d7(5\u00d75\u00d7480) = 112.9M",
            "With the use of 1\u00d71 convolution:",
            "Number of operations for 1\u00d71 = (14\u00d714\u00d716)\u00d7(1\u00d71\u00d7480) = 1.5MNumber of operations for 5\u00d75 = (14\u00d714\u00d748)\u00d7(5\u00d75\u00d716) = 3.8MTotal number of operations = 1.5M + 3.8M = 5.3Mwhich is much much smaller than 112.9M !!!!!!!!!!!!!!!",
            "Indeed, the above example is the calculation of 5\u00d75 conv at inception (4a).",
            "(We may think that, when dimension is reduced, actually we are working on the mapping from high dimension to low dimension in a non-linearity way. In contrast, for PCA, it performs linear dimension reduction.)",
            "Thus, inception module can be built without increasing the number of operations largely compared the one without 1\u00d71 convolution!",
            "1\u00d71 convolution can help to reduce model size which can also somehow help to reduce the overfitting problem!!",
            "The inception module (naive version, without 1\u00d71 convolution) is as below:",
            "Previously, such as AlexNet, and VGGNet, conv size is fixed for each layer.",
            "Now, 1\u00d71 conv, 3\u00d73 conv, 5\u00d75 conv, and 3\u00d73 max pooling are done altogether for the previous input, and stack together again at output. When image\u2019s coming in, different sizes of convolutions as well as max pooling are tried. Then different kinds of features are extracted.",
            "After that, all feature maps at different paths are concatenated together as the input of the next module.",
            "However, without the 1\u00d71 convolution as\u00a0above, we can imagine how large the number of operation is!",
            "Thus, 1\u00d71 convolution is inserted into the inception module for dimension reduction!",
            "Previously, fully connected (FC) layers are used at the end of network, such as in AlexNet. All inputs are connected to each output.",
            "Number of weights (connections) above = 7\u00d77\u00d71024\u00d71024 = 51.3M",
            "In GoogLeNet, global average pooling is used nearly at the end of network by averaging each feature map from 7\u00d77 to 1\u00d71, as in the figure above.",
            "Number of weights = 0",
            "And authors found that a move from FC layers to average pooling improved the top-1 accuracy by about 0.6%.",
            "This is the idea from NIN [6] which can be less prone to overfitting.",
            "After knowing the basic units as described above, we can talk about the overall network architecture.",
            "There are 22 layers in total!",
            "It is already a very deep model compared with previous AlexNet, ZFNet and VGGNet. (But not so deep compared with ResNet invented afterwards.) And we can see that there are numerous inception modules connected together to go deeper. (There are some intermediate softmax branches at the middle, we will describe about them in the next section.)",
            "Below is the details about the parameters if each layer. We actually can extend the example of 1\u00d71 convolution to calculate the number of operations by ourselves. :)",
            "As we can see there are some intermediate softmax branches at the middle, they are used for training only. These branches are auxiliary classifiers which consist of:",
            "5\u00d75 Average Pooling (Stride 3)1\u00d71 Conv (128 filters)1024 FC1000 FCSoftmax",
            "The loss is added to the total loss, with weight 0.3.",
            "Authors claim it can be used for combating gradient vanishing problem, also providing regularization.",
            "And it is NOT used in testing or inference time.",
            "7 GoogLeNet are used for ensemble prediction. This is already a kind of boosting approach from LeNet, AlexNet, ZFNet and VGGNet.",
            "Multi-scale testing is used just like VGGNet, with shorter dimension of 256, 288, 320, 352. (4 scales)",
            "Multi-crop testing is used, same idea but a bit different from and more complicated than AlexNet.",
            "First, for each scale, it takes left, center and right, or top, middle and bottom squares (3 squares). Then, for each square, 4 corners and center as well as the resized square (6 crops) are cropped as well as their corresponding flips (2 versions) are generated.",
            "The total is 4 scales\u00d73 squares\u00d76 crops\u00d72 versions=144 crops/image",
            "Softmax probabilities are averaged over all crops.",
            "With 7 models + 144 crops, the top-5 error is 6.67%.",
            "Compared with 1 model + 1 crop, there are large reduction from 10.07%.",
            "From this, we can observe that, besides the network design, the other stuffs like ensemble methods, multi-scale and multi-crop approaches are also essential to reduce the error rate!!!",
            "And these techniques actually are not totally new in this paper!",
            "Finally, GoogLeNet outperforms other previous deep learning networks, and won in ILSVRC 2014.",
            "I will review other deep learning networks as well as inception versions later on. If interested, please also visit the reviews of LeNet [7], AlexNet [8] , ZFNet [9], and VGGNet [10].",
            "[2015] [CVPR] [GoogLeNet] Going Deeper with Convolutions [2014 ECCV] [ZFNet] Visualizing and Understanding Convolutional Networks [2012 NIPS] [AlexNet] ImageNet Classification with Deep Convolutional Neural Networks [2015 ICLR] [VGGNet] Very Deep Convolutional Networks for Large-Scale Image Recognition [1998 Proc. IEEE] [LeNet-1, LeNet-4, LeNet-5, Boosted LeNet-4] Gradient-Based Learning Applied to Document Recognition [2014 ICLR] [NIN] Network in Network Review of LeNet-1, LeNet-4, LeNet-5, Boosted LeNet-4 (Image Classification) Review of AlexNet, CaffeNet \u2014 Winner of ILSVRC 2012 (Image Classification) Review of ZFNet \u2014 Winner of ILSVRC 2013 (Image Classification) Review of VGGNet \u2014 1st Runner-Up of ILSVLC 2014 (Image Classification)",
            "Best Crypto APIs for Developers The Best Crypto Trading Bots The Best Bitcoin Hardware wallet The Best Crypto Tax Software Best Crypto Trading Platforms Best Wallet for Uniswap Best Crypto Lending Platforms Top DeFi Projects Bitsgap review \u2014 A Crypto Trading Bot That Makes Easy Money Quadency Review - A Crypto Trading Bot Made For Professionals 3commas Review | An Excellent Crypto Trading Bot 3Commas vs Cryptohopper The Idiots Guide to Margin Trading on Bitmex The Definitive Guide to Crypto Swing Trading Bitmex Advanced Margin Trading Guide Crypto arbitrage guide: How to make money as a beginner Top Bitcoin Node Providers Best Crypto Charting Tool",
            "Get Best Software Deals Directly In Your Inbox"
        ]
    },
    "https://becominghuman.ai/tensorflow-object-detection-api-tutorial-training-and-evaluating-custom-object-detector-ed2594afcf73": {
        "author": "Vatsal Sodha",
        "length": "5 min read",
        "title": "TensorFlow Object Detection API tutorial \u2014 Training and Evaluating Custom Object Detector",
        "tags": [
            "tensorflow",
            "object-detection",
            "tutorial",
            "deep-learning",
            "artificial-intelligence"
        ],
        "content": [
            "We all are driving cars, it is easy right? But what if someone asks you to fly an airplane, what you will do? Yes, you guessed right you will look at the instruction manual. Similarly, consider this tutorial as a manual to configure the complex API and I hope this tutorial helps you to take a safe flight.",
            "First thing first, clone the TensorFlow object detection repository, and I hope you have installed TensorFlow.",
            "git clone https://github.com/tensorflow/models.git",
            "In the classical machine learning, what we do is with the use of .csv file we will train and test the model. But here, what we have to do at rudimentary level is shown below:",
            "Before proceeding further, I want to discuss directory structure that I will use throughout the tutorial.",
            "data/ \u2014 Will have records and csv files. images/ \u2014 This directory will contain our dataset. training/ \u2014 In this directory we will save our trained model. eval/ \u2014 Will save results of evaluation on trained model.",
            "1. How I used machine learning as inspiration for physical paintings",
            "2. MS or Startup Job \u2014 Which way to go to build a career in Deep Learning?",
            "3. TOP 100 medium articles related with Artificial Intelligence",
            "Step 1: Generating CSV files from Images",
            "This step is pretty simple, I won\u2019t dive much deeper but I will mention here some of the good sources. The goal is to label the image and generate train.csv and test.csv files.",
            "Label the image using lablelImg . The detailed explanation can be found here . We need to convert XML into csv files which is demonstrated here",
            "The reason being I am not mentioning in detail is there are various ways to generate the csv files from images depending on type of data sets we are dealing with.",
            "In my project, I want to detect Lung nodules using LUNA dataset, we already had co-ordinates of nodules to be detected, so for us it is pretty simple to make csv files. To detect nodules we are using 6 co-ordinates as show below:",
            "Instead of class nodules, your file will have different classes name, else will remain the same. Once, our labelled image data is turned into number we are good to go for generating TFRecords",
            "Step 2: Generating TFRecords",
            "TensorFlow object detection API doesn\u2019t take csv files as an input, but it needs record files to train the model. I have used this file to generate tfRecords.",
            "Download this file, and we need to just make a single change, on line 31 we will change our label instead of \u201cracoon\u201d. For example, in my case it will be \u201cnodules\u201d . You can add multiple class if you need to detect multiple objects.",
            "Note: Label\u2019s return value should start from 1 not from zero. Let\u2019s say, if you have to detect 3 labels then corresponding return values will be 1,2 and 3",
            "To generate train.record file use the code as shown below:",
            "python generate_tfRecord.py --csv_input=data/train.csv  --output_path=data/train.record",
            "To generate test.record file use the code as shown below:",
            "python generate_tfrecord.py \u2014 csv_input=data/test.csv \u2014 output_path=data/test.record",
            "Step 3: Training the Model",
            "Once our records files are ready, we are almost ready to train the model.",
            "Decide the pre-trained model to be used . There\u2019s a trade off between detection speed and accuracy, higher the speed lower the accuracy and vice versa. I am using ssd_mobilenet_v1_coco for demonstration purpose. After deciding the model to be used download the config file for the same model. In my case, I will download ssd_mobilenet_v1_coco.config. Make a new file object-detection.pbtxt which looks like this:",
            "item { id: 1 name: 'nodule' }",
            "Give class name i.e nodule in my case. If in case you have multiple classes, increase id number starting from 1 and give appropriate class name.",
            "Now, it\u2019s time to configure the ssd_mobilenet_v1_coco.config file. I am mentioning here the lines to be change in the file.",
            "Change the number of classes in the file according to our requirement.",
            "#before num_classes: 90 #After num_classes: 1",
            "If your desktop do not have good GPU then you need to decrease the batch_size.",
            "batch_size: 24",
            "Give path to downloaded model i.e ssd_mobilenet_v1_coco; the model we decided to use in step 1.",
            "#before fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" #after fine_tune_checkpoint: \"ssd_mobilenet_v1_coco/model.ckpt\"",
            "Need to give path to train.record file.",
            "#before train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } #after train_input_reader: { tf_record_input_reader { input_path: \"data/train.record\" } label_map_path: \"data/object-detection.pbtxt\" }",
            "Need to give path for test.record file",
            "#before eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"  shuffle: false num_readers: 1} #after eval_input_reader: { tf_record_input_reader { input_path: \"data/test.record\" } label_map_path: \"data/object-detection.pbtxt\" shuffle: false num_readers: 1}",
            "Now, copy data/, images/ directories to models/research/object-detection directory. If it prompts to merge the directory, merge it.",
            "We have to use train.py residing inside object-detection/ directory.",
            "cd models/research/object-detection",
            "Make a new directory training/ inside object-detection/ directory. The trained model will be saved in training/ Copy the config file ssd_mobilenet_v1_coco.config to training/ directory. You can train the model using this command:",
            "python train.py --logtostderr \\ --train_dir=training/ \\ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config",
            "If everything goes right, you will see the loss at particular step.",
            "Step 4: Evaluating the model",
            "The final step is to evaluate the trained model saved in training/ directory. We have to use eval.py file and can evaluate using following command:",
            "python eval.py \\ --logtostderr \\ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config \\ --checkpoint_dir=training/ \\ --eval_dir=eval/",
            "This will save the eval results in eval/ directory. To visualize the results we will use tensor board.",
            "#To visualize the eval results tensorboard --logdir=eval/ #TO visualize the training results tensorboard --logdir=training/",
            "Open the link in browser and under Images tag you can see the results as demonstrated below.",
            "That\u2019s all, you have successfully configured the TensorFlow Object Detection API.",
            "Please mention any errors in the comment section, you encounter while configuring the API, as I had faced many errors while configuring it.",
            "One of the common error is:",
            "No module named deployment on object_detection/train.py",
            "The solution is paste this command:",
            "# From tensorflow/models/research/ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim",
            "If you are wondering on how to update the parameters of the Faster-RCNN/SSD models in API, do refer this story.",
            "Thank you!",
            "SodhA"
        ]
    },
    "https://blog.searce.com/tips-tricks-for-using-google-vision-api-for-text-detection-2d6d1e0c6361": {
        "author": "Monark Unadkat",
        "length": "3 min read",
        "title": "Tips & tricks for using Google Vision API for text detection.",
        "tags": [
            "machine-learning",
            "python3",
            "ocr",
            "cloud-vision-api"
        ],
        "content": [
            "The Google Cloud Vision API enables developers to create vision based machine learning applications based on object detection, OCR, etc. without having any actual background in machine learning.",
            "The Google Cloud Vision API takes incredibly complex machine learning models centered around image recognition and formats it in a simple REST API interface. It encompasses a broad selection of tools to extract contextual data on your images with a single API request. It uses a model which trained on a large dataset of images, similar to the models used to power Google Photos, so there is no need to develop and train your own custom model.",
            "For the purpose of this article, I\u2019ll only focus on OCR capability of Google Cloud Vision API and provide you all with some tips and tricks for using the API for OCR text detection.",
            "A code snippet below makes a DOCUMENT_TEXT_DETECTION request using python API library.",
            "The response contains a AnnotateImageResponse, which is a json consisting of a list of Image Annotation results.",
            "First is text_annotations which is a contains only word level information, i.e, words and their particular location coordinates. A simple text_annotation element will look like this.",
            "{ \"responses\": [ { \"textAnnotations\": [ { \"locale\": \"en\", \"description\": \"Wake up human!\\n\", \"boundingPoly\": { \"vertices\": [ { \"x\": 29, \"y\": 394 }, { \"x\": 570, \"y\": 394 }, { \"x\": 570, \"y\": 466 }, { \"x\": 29, \"y\": 466 } ] } }, ...... .....",
            "2. Another one which we\u2019ll be using is full_text_annotation which contains character level data descriptions. The full_text_annotation contains a structured representation of OCR extracted text which is like this : full_text_annotation -> Page -> Block -> Paragraph -> Word ->Symbols.",
            "Group of symbols create words, group of words create a paragraph and so on. Each representation has properties like language and bounding_box. You can use this type of representation to divide the text content and process them separately as you want. Suppose you are extracting information from text documents with fixed format, using this representation will make your work easier.",
            "The code below uses the response to draw bounding boxes around the feature we specify, in this case it is a word.",
            "If we do it for block, we get",
            "The block-wise separation helps to identify different parts of the image and helps to extract important information ignoring unwanted text. It doesn\u2019t look so useful here, but suppose you are extracting information from some image of identification documents",
            "Suppose in the above image, you want to extract the amount of Loans, including overdrafts, for that you need the location of that keyword, in this case, it is \u2018Overdrafts\u2019. You can use this below code to search for the location of that word, and based on the coordinates of the word, you can then extract that particular amount.",
            "Now you have the coordinates of the word \u2018Overdrafts\u2019. To extract the amount, assume a box starting from the right of the word \u2018Overdrafts\u2019 having same width as of overdrafts. We need to extract text inside this box. You can use the below code to do that.",
            "Extracting data from user forms or identification documents. Extracting text from scanned images containing text. Scanning user passbooks, and many such use cases.",
            "CLOUD VISION API https://cloud.google.com/vision/ Vision Docs https://cloud.google.com/vision/docs/fulltext-annotations Code https://github.com/monark12/vision-api/blob/master/vision_api.ipynb"
        ]
    },
    "https://towardsdatascience.com/train-without-labeling-data-using-self-supervised-learning-by-relational-reasoning-b0298ad818f9": {
        "author": "Chien Vu",
        "length": "9 min read",
        "title": "Train without labeling data using Self-Supervised Learning by Relational Reasoning",
        "tags": [
            "self-supervised-learning",
            "deep-learning",
            "machine-learning",
            "data-science",
            "editors-pick"
        ],
        "content": [
            "In a modern deep learning algorithm, the dependence on manual annotation of unlabeled data is one of the major limitations. To train a good model, usually, we have to prepare a vast amount of labeled data. In the case of a small number of classes and data, we can use the pre-trained model from the labeled public dataset and fine-tune a few last layers with your data. However, in real life, it\u2019s easily faced with the problem when your data is considerably large (the products in the store or the face of a human,..) and it will be difficult for the model to learn with just a few trainable layers. Furthermore, the amount of unlabeled data (e.g. document text, images on the Internet) is uncountable. Labeling all of them for the task is almost impossible but not utilizing them is definitely a waste.",
            "In this case, training a deep model again from scratch with a new dataset will be an option but it takes a lot of time and effort for labeling data while using a pre-trained deep model seems no longer helpful. That is the reason why Self-supervised learning was born. The idea behind this is simple, which serves two main tasks:",
            "Surrogate task: the deep model will learn generalizable representations from unlabeled data without annotation, and then will be able to self-generate a supervisory signal exploiting implicit information. Downstream task: representations will be fine-tuned for supervised-learning tasks e.g. classification and image retrieval with less number of labeled data (the number of labeled data depending on the performance of model based on your requirement)",
            "There are much different training approaches proposed to learn such representations: Relative position [1]: the model needs to understand the spatial context of objects to tell the relative position between parts; Jigsaw puzzle [2]: the model needs to place 9 shuffled patches back to the original locations; Colorization [3]: the model has trained to color a grayscale input image; precisely the task is to map this image to a distribution over quantized color value outputs; Counting features [4]: The model learns a feature encoder using feature counting relationship of input images transforming by Scaling and Tiling; SimCLR [5]: The model learns representations for visual inputs by maximizing agreement between differently augmented views of the same sample via a contrastive loss in the latent space.",
            "However, I would like to introduce one interesting approach that is able to recognize things like a human. The key factor in human learning is the acquisition of new knowledge by comparing relating and different entities. So, it is a nontrivial solution if we can apply a similar mechanism in self-supervised machine learning via the Relational reasoning approach [6].",
            "The relational reasoning paradigm is based on a key design principle: the use of a relation network as a learnable function on the unlabeled dataset to quantify the relationships between views of the same object (intra-reasoning) and relationships between different objects in different scenes (inter-reasoning). The possibility to exploit a similar mechanism in self-supervised machine learning via relational reasoning was evaluated by the performance on standard datasets (CIFAR-10, CIFAR-100, CIFAR-100\u201320, STL-10, tiny-ImageNet, SlimageNet), learning schedule, and backbones (bothshallow and deep). The results show that the Relational reasoning approach largely outperforms the best competitor in all conditions by an average 14% accuracy and the most recent state-of-the-art method by 3% indicating in this paper [6].",
            "For the simplest explanation, Relational Reasoning is just a methodology that tries to help learners understanding relations between different objects (ideas) rather than learning objects individually. That could help learners easily distinguish and remember the object based on their difference. There are two main components in the Relational reasoning system [6]: backbone structure and relation head. The relation head was used in the pretext task phase for supporting the underlying neural network backbone learning useful representations in the unlabeled dataset and then it will be discarded. The backbone structure was used in downstream tasks such as classification or image retrieval after training in the pretext task.",
            "Previous work: focus on within-scene relation, meaning that all the elements in the same object belong to the same scene (e.g. balls from a basket); training on label dataset and the main goal is the relation head [7]. New approach: focus on relations between different views of the same object ( intra-reasoning ) and between different objects in different scenes ( inter-reasoning ); use relational reasoning on unlabeled data and the relation head is a pretext task for learning useful representations in the underlying backbone.",
            "Let\u2019s discuss the important point in some part of the Relational reasoning system:",
            "Mini-batch augmentation:",
            "As mentioned before, this system introduced intra-reasoning and inter-reasoning? So why we need them? It is not possible to create pairs of similar and dissimilar objects when labels are not given. To solve this problem, the bootstrapping technique was applied and resulted in forming intra-reasoning and inter-reasoning, where:",
            "Intra-reasoning consists of sampling random augmentations of the same object {A1; A2 } (positive pair) (eg. different views of same basketball) Inter-reasoning consists of coupling two random objects {A1; B1} (negative pair) (eg. basketball with random ball)",
            "Furthermore, the utilization of the random augmentations functions (e.g. geometric transformation, color distortion) is also considered to make between-scenes reasoning more complicated. The benefit of these augmentations functions forces the learner (backbone) to pay attention to the correlation between a wider set of features (e.g. color, size, texture, etc.). For instance, in the pair {foot ball, basket ball}, the color alone is a strong predictor of the class. However, with the random changing of color as well as the shape size, the learner now is difficult to discriminate the difference between this pair. The learner has to take a look at another feature, consequently, it results in better representation.",
            "2. Metric learning",
            "The aim of metric learning s to use a distance metric to bring closer representations of similar inputs (positives) while moving away representations of dissimilar inputs (negatives). However, in Relational reasoning, metric learning is fundamentally different:",
            "3. Loss function",
            "The learning objective is a binary classification problem over the presentation pairs. Therefore we can use a binary cross-entropy loss to the maximization of a Bernoulli log-likelihood, where the relation score y represents a probabilistic estimate of representation membership inducing through a sigmoid activation function.",
            "Finally, this paper [6] also supplied the result of Relational reasoning on standard datasets (CIFAR-10, CIFAR-100, CIFAR-100\u201320, STL-10, tiny-ImageNet, SlimageNet), different backbones (shallow and deep), same learning schedule (epochs). The results are below, for further information you can check out his paper.",
            "In this article, I want to reproduce the Relational reasoning system on the public image dataset STL-10. This dataset comprises of 10 classes (airplane, bird, automobile, cat, deer, dog, horse, monkey, ship, truck) with 96x96 pixels color.",
            "First of all, we need to import some important library",
            "STL-10 dataset consists of 1300 labeled images (500 for training and 800 for testing). However, it also includes 100000 unlabeled images from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set",
            "And then we will create the Relational reasoning class based on the suggestion of the author",
            "To compare the performance of Relational reasoning methodology on the shallow and deep model, we will create a shallow model (Conv4) and use the structure of a deep model (Resnet34).",
            "backbone = Conv4() # shallow model backbone = models.resnet34(pretrained = False) # deep model",
            "Some hyperparameters and augmentation strategies were set based on the suggestion of the author. We will train our backbone with relation head on unlabeled STL-10 dataset.",
            "Up to now, we\u2019ve already created everything necessary to train our model. Now we will train the backbone and relation head model in 10 epochs and 16 augmentation images (K), it took 4 hours with the shallow model (Conv4) and 6 hours on the deep model (Resnet34) by 1 GPU Tesla P100-PCIE-16GB (you can freely change the number of epochs as well as another hyperparameter to obtain better results)",
            "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\") backbone.to(device) model = RelationalReasoning(backbone, feature_size) model.train(tot_epochs=tot_epochs, train_loader=train_loader) torch.save(model.backbone.state_dict(), 'model.tar')",
            "After training our backbone model, we discard the relation head and use only the backbone for the downstream tasks. We need to fine-tune our backbone with labeled data in STL-10 (500 images) and test the final model in the test set (800 images). Training and testing datasets will load in Dataloader without augmentations.",
            "We will load the pretrained backbone model and use a simple linear model to connect the output feature with a number of classes in the dataset.",
            "# linear model linear_layer = torch.nn.Linear(64, 10) # if backbone is Conv4 linear_layer = torch.nn.Linear(1000, 10) # if backbone is Resnet34 # defining a raw backbone model backbone_lineval = Conv4() # Conv4 backbone_lineval = models.resnet34(pretrained = False) # Resnet34 # load model checkpoint = torch.load('model.tar') # name of pretrain weight backbone_lineval.load_state_dict(checkpoint)",
            "In this time, only the linear model will be trained, the backbone model will be frozen. First, we will see the result of fine-tuned Conv4",
            "And then check on the test set",
            "Conv4 obtained 49.98% accuracy on the test set, it means that the backbone model could learn useful feature in the unlabeled dataset, we just need to fine-tune with few epochs to achieve a good result. Now let check the performance of the deep model.",
            "Then evaluating on the test dataset",
            "It\u2019s much better, we can obtain 55.38% accuracy on the test set. In this article, the main goal is to reproduce and evaluate the Relational reasoning methodology to teach the model distinguishing the object without the label, therefore, these results were very promising. If you feel unsatisfied, you can freely do the experiment by changing the hyperparameter such as the number of augmentation, epochs, or model structure.",
            "Self-supervised relational reasoning is effective in both a quantitative and qualitative manners, and with backbones of different size from shallow to deep structure. Representations learned through comparison can be easily transferred from one domain to another, they are fine-grained and compact, which may be due to the correlation between accuracy and number of augmentations. In relational reasoning, the number of augmentations has a primary role affecting the quality of the clusters of objects based on the author\u2019s experiment [4]. Self-supervised learning has a strong potential to become the future of machine learning in many aspects.",
            "You can contact me if you want further discussion. Here is my Linkedin",
            "Enjoy!!! \ud83d\udc66\ud83c\udffb",
            "[1] Carl Doersch et. al, Unsupervised Visual Representation Learning by Context Prediction, 2015.",
            "[2] Mehdi Noroozi et. al, Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles, 2017.",
            "[3] Zhang et. al, Colorful Image Colorization, 2016.",
            "[4] Mehdi Noroozi et. al, Representation Learning by Learning to Count, 2017.",
            "[5] Ting Chen et. al, A Simple Framework for Contrastive Learning of Visual Representations, 2020.",
            "[6] Massimiliano Patacchiola et. al, Self-Supervised Relational Reasoning forRepresentation Learning, 2020.",
            "[7] Adam Santoro et. al, Relational recurrent neural networks, 2018."
        ]
    },
    "https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8": {
        "author": "Thom Lane",
        "length": "10 min read",
        "title": "Transposed Convolutions explained with\u2026 MS Excel!",
        "tags": [
            "machine-learning",
            "convolutional-network",
            "mxnet",
            "deep-learning",
            "computer-vision"
        ],
        "content": [
            "You have successfully navigated your way around 1D Convolutions, 2D Convolutions and 3D Convolutions. You have conquered multi-input and multi-output channels too. But for the last blog post in the convolution series we are onto the boss level understanding the transposed convolution.",
            "So let\u2019s start with the name and see what we\u2019re dealing with. A transpose \u201ccauses (two or more things) to change places with each other\u201d. When we\u2019re transposing matrices we change the order of their dimensions, so for a 2D matrix we essentially \u2018flip\u2019 values with respect to the diagonal. We won\u2019t be covering this in the series, but it\u2019s possible to represent operations (such as rotations, translations, and convolutions) as matrices. See Section 4.1 of Dumoulin & Visin if you\u2019re interested. When we\u2019re transposing convolutions we change the order of the dimensions in this convolution operation matrix, which has some interesting effects and leads to different behaviours to the regular convolutions we\u2019ve learnt about so far.",
            "Sometimes you\u2019ll see this operation referred to as a \u2018deconvolution\u2019 but they are not equivalent. A deconvolution attempts to reverse the effects of a convolution. Although transposed convolutions can be used for this, they are more flexible. Other valid names for transposed convolutions you might see in the wild are \u2018fractionally strided convolutions\u2019 and \u2018up convolutions\u2019.",
            "One of the best ways for us to gain some intuition is by looking at examples from Computer Vision that use the transposed convolution. Most of these examples start with a series of regular convolutions to compress the input data into an abstract spatial representation, and then use transposed convolutions to decompress the abstract representation into something of use.",
            "A convolutional auto-encoder is tasked with recreating its input image, after passing intermediate results through a \u2018bottleneck\u2019 of a limited size. Uses of auto-encoders include compression, noise removal, colourisation and in-painting. Success depends on being able to learn dataset specific compression in the convolution kernels and dataset specific decompression in the transposed convolution kernels. Why stop there though?",
            "With \u2018super resolution\u2019 the objective is to upscale the input image to higher resolutions, so transposed convolutions can be used as an alternative to classical methods such as bicubic interpolation. Similar arguments to convolutions using learnable kernels over hand crafted kernels apply here.",
            "Semantic segmentation is an example of using transposed convolution layers to decompress the abstract representation into a different domain (from the RGB image input). We output a class for each pixel of the input image, and then just for visualisation purposes, we render each class as a distinct colour (e.g. the person class shown in red, cars in dark blue, etc.).",
            "Clearly transposed convolutions are more flexible than classical upsampling methods (like bicubic or nearest neighbour interpolation), but there are a few disadvantages. You can\u2019t apply transposed convolutions without learning the optimal kernel weights first, as you could with classical upsampling methods. And there can be checkerboard artifacts in the output.",
            "Advanced: to avoid checkerboard artifacts, an alternative upsampling method that\u2019s gaining popularity is to apply classical upsampling followed by a regular convolution (that preserves the spatial dimensions).",
            "Unlike for regular convolutions, where explanations are pretty consistent and diagrams are often intuitive, the world of transposed convolutions can be a little more daunting. You\u2019ll often read different (seemingly disconnected) ways to think about the computation. So in this blog post I\u2019ll take two mental models of transposed convolutions and help you join the dots using our trusty friend\u2026 MS Excel. And we\u2019ll code things up in Apache MXNet because we\u2019ll probably want to use them in practice some day!",
            "Advanced: the transposed convolution operation is equivalent to the gradient calculation for a regular convolution (i.e. the backward pass of a regular convolution). And vice versa. Consider this while reading the next section.",
            "Our first mental model is more intuitive (at least for me), and we\u2019ll work step-by-step towards the second mental model that\u2019s closer to how transposed convolutions are implemented in deep learning frameworks.",
            "Let\u2019s start from the perspective of a single value in the input. We take this value and \u2018distribute\u2019 it to a neighbourhood of points in the output. A kernel defines exactly how we do this, and for each output cell we multiply the input value by the corresponding weight of the kernel. We repeat this process for every value in the input, and accumulate values in each output cell. Check out Figure 4 for an example of this accumulation (with unit input and kernel).",
            "Our kernel values are hidden in the animation above, but it is important to understand that the kernel is defining the amount of the input value that\u2019s being distributed to each of the output cells (in the neighbourhood). We can see this more clearly in the spreadsheet in Figure 5, even with a unit kernel.",
            "Advanced: if you\u2019re observant you may have spotted that the edges of the output get less accumulation than the centre cells. Often this isn\u2019t an issue because kernel weights learn to adjust for this and can be negative too.",
            "With Apache MXNet we can replicate this using the Transpose blocks. We have got two spatial dimensions so we\u2019ll use Conv2DTranspose. Similarly MXNet defines Conv1DTranspose and Conv3DTranspose.",
            "input_data = mx.nd.ones(shape=(4,4)) kernel = mx.nd.ones(shape=(3,3)) conv = mx.gluon.nn.Conv2DTranspose(channels=1, kernel_size=(3,3)) # see appendix for definition of `apply_conv` output_data = apply_conv(input_data, kernel, conv) print(output_data) # [[[[1. 2. 3. 3. 2. 1.] #    [2. 4. 6. 6. 4. 2.] #    [3. 6. 9. 9. 6. 3.] #    [3. 6. 9. 9. 6. 3.] #    [2. 4. 6. 6. 4. 2.] #    [1. 2. 3. 3. 2. 1.]]]] # <NDArray 1x1x6x6 @cpu(0)>",
            "Another way of thinking about transposed convolutions is from the perspective of a cell in the output, rather than a value in the input as we did with first mental model. When we do this we end up with something strangely familiar, something very similar to a regular convolution!",
            "One step at a time we\u2019ll convert what we already know to this new way of thinking. We start with the animation in Figure 6. It highlights a single cell in the output, and looks at the input values that distribute into it. You should pay close attention to the kernel weights used for each of the input values.",
            "We can make this even more obvious in Figure 7 by colour coding the input values by the kernel weight that they get multiplied with before the accumulation. You should notice how the kernel on the input has \u2018flipped\u2019 about the centre; i.e. the dark blue weight of the kernel was bottom right when distributing, but it\u2019s moved to top left when we think about collecting.",
            "We\u2019ve just created a convolution! Check the freeze frame in Figure 8 if you don\u2019t believe me. We\u2019re using the \u2018flipped\u2019 kernel, that despite the name, \u2018transposed convolution\u2019, isn\u2019t actually a transpose of the distribution kernel.",
            "Advanced: if you\u2019re observant you may have spotted that applying the Conv2D like so would actually result in a 2x2 output. Conv2DTranspose with no padding is equivalent to having 2x2 padding ((kernel_size + 1) / 2) now that we have mapped the operation to a Conv2D, giving us a 6x6 output as we had before.",
            "Collecting values with 2D Convolutions allows us to write explicit formulas for the output: ideal for MS Excel and also code implementations too. So we\u2019d have the following formula for the top left cell of the output:",
            "We can confirm our results with the Apache MXNet code seen previously:",
            "# define input_data and kernel as above # input_data.shape is (4, 4) # kernel.shape is (3, 3) conv = mx.gluon.nn.Conv2DTranspose(channels=1, kernel_size=(3,3)) output_data = apply_conv(input_data, kernel, conv) print(output_data) # [[[[ 1.  5. 11. 14.  8.  3.] #    [ 1.  6. 15. 18. 12.  3.] #    [ 4. 13. 21. 21. 15. 11.] #    [ 5. 17. 28. 27. 25. 11.] #    [ 4.  7.  9. 12.  8.  6.] #    [ 6.  7. 14. 13.  9.  6.]]]] # <NDArray 1x1x6x6 @cpu(0)>",
            "We\u2019ve just seen a strange example of a Conv2DTranspose with no padding (appearing to have padding of 2x2 when thinking about it as a Conv2D) but things get even more mysterious when we start adding padding.",
            "With regular convolutions, padding is applied to the input which has the effect of increasing the size of the output. With transposed convolutions, padding has the reverse effect and it decreases the size of the output. So I\u2019m coining \u2018gniddap\u2019 in the hope you\u2019ll remember the reverse \u2018padding\u2019.",
            "for pad in range(3): conv = mx.gluon.nn.Conv2DTranspose(channels=1, kernel_size=(3,3), padding=(pad,pad)) output_data = apply_conv(input_data, kernel, conv) print(\"With padding=({pad}, {pad}) the output shape is {shape}\" .format(pad=pad, shape=output_data.shape)) # With padding=(0, 0) the output shape is (1, 1, 6, 6) # With padding=(1, 1) the output shape is (1, 1, 4, 4) # With padding=(2, 2) the output shape is (1, 1, 2, 2)",
            "We can think about padding for transposed convolutions as the amount of padding that\u2019s included in the complete output. Sticking with our usual example (where the complete output is 6x6), when we define padding of 2x2 we\u2019re essentially saying that we don\u2019t care about the outer cells of the output (with width of 2) because that was just padding, leaving us with a 2x2 output. When thinking about transposed convolutions as regular convolutions we remove padding from the input by the defined amount. See Figure 11 for an example with MS Excel, and notice how the outputs are identical to the central values of the output in Figure 10 when there was no padding.",
            "# define input_data and kernel as above # input_data.shape is (4, 4) # kernel.shape is (3, 3) conv = mx.gluon.nn.Conv2DTranspose(channels=1, kernel_size=(3,3), padding=(2,2)) output_data = apply_conv(input_data, kernel, conv) print(output_data) # [[[[21. 21.] #    [28. 27.]]]] # <NDArray 1x1x2x2 @cpu(0)>",
            "Strides are also reversed. With regular convolution we stride over the input, resulting in a smaller output. But when thinking about transposed convolutions from a distribution perspective, we stride over the output, which increases the size of the output. Strides are responsible for the upscaling effect of transposed convolutions. See Figure 12.",
            "Advanced: checkerboard artifacts can be seen in the example below, which can start to become an issue when using strides (even after stacking multiple layers).",
            "Although things are clear from the distributional perspective above, things get a little strange when we think about things from a collection perspective and try to implement this using a convolution. Stride over the output is equivalent to a \u2018fractional stride\u2019 over the input, and this is where the alternative name for transposed convolutions called \u2018fractionally strided convolutions\u2019 comes from. A stride of 2 over the output would be equivalent to a stride of 1/2 over the input: a fractional stride. We implement this by introducing empty spaces between our input values, the amount proportional to the stride, and then stride by one. As a result we\u2019re applying the kernel to a region of the input that\u2019s smaller than the kernel itself! See Figure 13 for an example.",
            "# define input_data and kernel as above # input_data.shape is (2, 2) # kernel.shape is (3, 3) conv = mx.gluon.nn.Conv2DTranspose(channels=1, kernel_size=(3,3), strides=(2,2)) output_data = apply_conv(input_data, kernel, conv) print(output_data) # [[[[ 3.  6. 12.  6.  9.] #    [ 0.  3.  0.  3.  0.] #    [ 7.  5. 16.  5.  9.] #    [ 0.  1.  0.  1.  0.] #    [ 2.  1.  4.  1.  2.]]]] # <NDArray 1x1x5x5 @cpu(0)>",
            "As with regular convolutions, each input channel will use a separate kernel and the results for each channel will be summed together to give a single output channel. We repeat this process for every output channel required, using a different set of kernels. All these kernels are kept in a single kernel array with shape:",
            "(input channels, output channels, kernel height, kernel width)",
            "Which is different from the kernel array shape used for a regular convolution:",
            "(output channels, input channels, kernel height, kernel width)",
            "# define input_data and kernel as above # input_data.shape is (3, 5, 5) # kernel.shape is (3, 3, 3) kernel = kernel.expand_dims(0).transpose((1,0,2,3)) # kernel.shape is now (3, 1, 3, 3) conv = mx.gluon.nn.Conv2DTranspose(channels=1, kernel_size=(3,3)) output_data = apply_conv(input_data, kernel, conv) print(output_data) # [[[[ 4.  2.  1.  5.  2.  2.  0.] #    [ 9.  6.  7. 13.  9.  1.  4.] #    [11. 14. 12. 14. 17. 11.  4.] #    [ 5. 17. 19. 25. 18. 14.  6.] #    [ 6. 13. 25. 21. 22.  6.  6.] #    [ 1.  3. 20.  9. 17. 15.  0.] #    [ 0.  3.  4. 11. 11.  5.  2.]]]] # <NDArray 1x1x7x7 @cpu(0)>",
            "All the examples shown in this blog posts can be found in this Excel Spreadsheet (and Google Sheet too). Click on the cells of the output to inspect the formulas and try different kernel values to change the outputs. After replicating your results in MXNet Gluon, I think you can officially add \u2018convolution wizard\u2019 as a title on your LinkedIn profile!",
            "You\u2019ve made it to the end of this excellent series on convolutions. I hope you learnt something useful, and now feel ready to apply these techniques to real world problems with Apache MXNet. Any questions? Just drop a comment below or check out the MXNet Discussion forum. Shares and claps would also be greatly appreciated. Many thanks!"
        ]
    },
    "https://towardsdatascience.com/pytorch-widedeep-deep-learning-for-tabular-data-9cd1c48eb40d": {
        "author": "Javier Rodriguez Zaurin",
        "length": "17 min read",
        "title": "pytorch-widedeep: deep learning for tabular data",
        "tags": [
            "pytorch",
            "deep-learning",
            "tabular-data",
            "transformers"
        ],
        "content": [
            "This is the third of a series of posts introducing pytorch-widedeepa flexible package to combine tabular data with text and images (that could also be used for \u201cstandard\u201d tabular data alone). The previous two posts, and the original version of this post are hosted in my own blog, just in case.",
            "While writing this post I will assume that the reader is not familiar with the previous two posts. Of course, reading them would help, but in order to understand the content of this post and then being able to use pytorch-widedeep on tabular data, is not a requirement.",
            "To start with, as always, just install the package:",
            "pip install pytorch-widedeep",
            "This will install v0.4.8 hopefully the last beta version*. Code-wise I think this could be already v1, but before that I want to try it in a few more datasets and select good default values. In addition, I also intend to implement other algorithms, in particular TabNet [1], for which a very nice implementation already exists.",
            "Moving on, and as I mentioned earlier, pytorch-widedeep's main goal is to facilitate the combination of images and text with tabular data via wide and deep models. To that aim, wide and deep models can be built with up to four model components: wide, deeptabular, deeptext and deepimage, that will take care of the different types of input datasets (\"standard\" tabular, i.e. numerical and categorical features, text and images). This post focuses only on the so called deeptabular component, and the 3 different models available in this library that can be used to build that component. Nonetheless, and for completion, I will briefly describe the remaining components first.",
            "The wide component of a wide and deep model is simply a liner model, and in pytorch-widedeep such model can be created via the Wide class. In the case of the deeptext component, pytorch-widedeep offers one model, available via the DeepText class. DeepText builds a simple stack of LSTMs, i.e. a standard DL text classifier or regressor, with flexibility regarding the use of pre-trained word embeddings, of a Fully Connected Head (FC-Head), etc. For the deepimage component, pytorch-widedeep includes two alternatives: a pre-trained Resnet model or a \"standard\" stack of CNNs to be trained from scratch. The two are available via the DeepImage class which, as in the case of DeepText, offers some flexibility when building the architecture.",
            "To clarify the use of the term \u201cmodel\u201d and \u201cwide and deep model component\u201d (in case there is some confusion), let\u2019s have a look to the following code:",
            "wide_model = Wide(...) text_model = DeepText(...) image_model = DeepImage(...) # we use the previous models as the wide and deep model components wdmodel = WideDeep( wide=wide_model, deeptext=text_model, deepimage=image_model ) ...",
            "Simply, a wide and deep model has model components that are (of course) models themselves. Note that any of the four wide and deep model components can be a custom model by the user. In fact, while I recommend using the models available in pytorch-widedeep for the wide and deeptabular components, it is very likely that users will want to use their own models for the deeptext and deepimage components. That is perfectly possible as long as the custom models have an attribute called output_dim with the size of the last layer of activations, so that WideDeep can be constructed (see this example notebook in the repo). In addition, any of the four components can be used independently in isolation. For example, you might want to use just a wide component, which is simply a linear model. To that aim, simply:",
            "wide_model = Wide(...) # this would not be a wide and deep model but just wide wdmodel = WideDeep(wide=wide_model) ...",
            "If you want to learn more about different model components and the models available in pytorch-widedeep please, have a look to the Examples folder in the repo, the documentation or the companion posts. Let's now take a deep dive into the models available for the deeptabular component",
            "*Check the repo or this post for a caveat in the installation if you are using Mac, python 3.8 or Pytorch 1.7+. Note that this is not directly related with the package, but the interplay between Mac and OpenMP, and the new defaults of the multiprocessing library for Mac).",
            "As I was developing the package I realised that perhaps one of the most interesting offerings in pytorch-widedeep was related to the models available for the deeptabular component (perhaps\u2026). Remember that each component can be used independently in isolation. With that in mind, building a WideDeep model comprised only by a deeptabular component would be what is normally referred as DL for tabular data. Of course, such model is not a wide and deep model, is \"just\" deep.",
            "Currently, pytorch-widedeep offers three models that can be used as the deeptabular component. In order of complexity, these are:",
            "TabMlp : this is very similar to the tabular model in the fantastic fastai library, and consists simply in embeddings representing the categorical features, concatenated with the continuous features, and passed then through a MLP. TabRenset : This is similar to the previous model but the embeddings are passed through a series of ResNet blocks built with dense layers. TabTransformer : Details on the TabTransformer can be found in: TabTransformer: Tabular Data Modeling Using Contextual Embeddings . Again, this is similar to the models before but the embeddings are passed through a series of Transformer encoder blocks.",
            "A lot has been (and is being) written about the use of DL for tabular data, and certainly each of these models would deserve a post themselves. Here, I will try to describe them with some detail and illustrate their use within pytorch-widedeep. A proper benchmark exercise will be carried out in a not-so-distant future.",
            "The following figure illustrates the TabMlp model architecture.",
            "The TabMlp is the simples architecture and is very similar to the tabular model available in the fantastic fastai library. In fact, the implementation of the dense layers of the MLP is mostly identical to that in that library. The dashed-border boxes in the figure indicate that these components are optional. For example, we could use TabMlp without categorical components, or without continuous components, if we wanted.",
            "Let\u2019s have a look and see how this model is used with the well known adult census dataset. I assume you have downloaded the data and place it at data/adult/adult.csv.zip . The following snippet is a simple preparation of the data and not directly related to pytorch-widedeep (remember, a notebook you can run can be found here):",
            "Now we just need to define the columns that will be represented as embeddings and the numerical (aka continuous) columns. pytorch-widedeep comes with a series of handy preprocessing utilities that will prepare the data for us:",
            "The code up until here is going to be common to all models with some minor adaptations for the TabTransformer. Internally, TabPreprocessor label encodes the \"embedding columns\" and standardises the numerical columns. Note that one could chose not to standardise the numerical columns and then use a BatchNorm1D layer when building the model. That is also a valid approach. Alternatively, one could use both, as I will.",
            "At this stage the data is prepared and we are ready to build the model",
            "One important thing to mention before I move on, common to all models, is that pytorch-widedeep models (in this case TabMlp) do not build the last connection, i.e. the connection with the output neuron or neurons depending whether this is a regression, binary or multi-class classification. Such connection is built by the WideDeep \u201cconstructor\u201d class. This means that even if we wanted to use a single-component model, the model still needs to be built with the WideDeep class.",
            "This is because the library is, a priori, intended to build WideDeep models (and hence its name). Once the model is built it is passed to the Trainer (as we will see now). The Trainer class is coded to receive a parent model of class WideDeep with children that are the model components. This is very convenient for a number of aspects in the library.",
            "Effectively this simply requires one extra line of code (line 10 in Snippet 3)",
            "Let\u2019s have a look to the model we just built and how it relates to Fig 1 (note that in order to keep the size of the post \u201ctractable\u201d, I will only print the architecture of the TabMlp model. If you wanted to see the architecture of the TabRensetand the TabTransformer have a look to the original post).",
            "model WideDeep( (deeptabular): Sequential( (0): TabMlp( (embed_layers): ModuleDict( (emb_layer_education): Embedding(17, 8, padding_idx=0) (emb_layer_marital_status): Embedding(8, 6, padding_idx=0) (emb_layer_occupation): Embedding(16, 8, padding_idx=0) (emb_layer_race): Embedding(6, 6, padding_idx=0) (emb_layer_relationship): Embedding(7, 6, padding_idx=0) (emb_layer_workclass): Embedding(10, 6, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (norm): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (tab_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=44, out_features=200, bias=True) (2): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=200, out_features=100, bias=True) (2): ReLU(inplace=True) ) ) ) ) (1): Linear(in_features=100, out_features=1, bias=True) ) )",
            "We can see that our model is a object of class WideDeep formed by one single component, deeptabular, which is itself a model of class TabMlp .TabMlp consists in a series of columns that will be represented as embeddings (e.g. emb_layer_education), which are concatenated to form a tensor of dim (bsz, 40) , where bsz is batch size. Then, the \"batchnormed\" continuous columns are also concatenated with that tensor, resulting in a tensor of dim (bsz, 44), that will be passed to tab_mlp, the 2-layer MLP [200 -> 100] . Finally, the output tensor of dim (bsz, 100) is \u201cplugged\u201d into the final neuron. In summary: Embeddings + continuous+ MLP.",
            "We are now ready to train it. The code below simply runs with defaults. one could use any torch optimizer, learning rate schedulers, etc. Just have a look to the docs or the Examples folder in the repo.",
            "Once we understand what TabMlp does, TabResnet should be pretty straightforward",
            "The following figure illustrates the TabResnet model architecture.",
            "The TabResnetmodel is similar to the TabMlp, but the embeddings (or the concatenation of embeddings and continuous features, normalised or not) are passed through a series of Resnet blocks built with dense layers. The dashed-border boxes indicate that the component is optional and the dashed lines indicate the different paths or connections present depending on which components we decide to include.",
            "This is probably the most flexible of the three models discussed in this post in the sense that there are many variants one can define via the parameters. For example, we could chose to concatenate the continuous features, normalised or not via a BatchNorm1d layer, with the embeddings and then pass the result of such a concatenation trough the series of Resnet blocks. Alternatively, we might prefer to concatenate the continuous features with the results of passing the embeddings through the Resnet blocks. Another optional component is the MLP before the output neuron(s). If not MLP is present, the output from the Resnet blocks or the results of concatenating that output with the continuous features (normalised or not) will be connected directly to the output neuron(s).",
            "Each of the Resnet block is comprised by the following operations:",
            "Let\u2019s build a TabResnet model, which is mostly the same code required to build a TabMlpmodel.",
            "As we did previously with the TabMlp, let's \"walk through\" the model. Again, if you wanted to see a full print of the architecture, please visit the original post. In this case, model is an instance of a WideDeep object formed by a single component, deeptabular that is a TabResnetmodel. TabResnet is formed by a series of Embedding layers (e.g. emb_layer_education ) a series of so-called dense Resnet blocks ( tab_resnet ) and a MLP ( tab_resnet_mlp ). The embeddings are concatenated themselves and then, further concatenated with the normalised continuous columns. The resulting tensor of dim (bsz, 44) is then passed through two dense Resnet blocks. The output of one Resnet block is the input of the next. Therefore, when setting blocks_dim = [200, 100, 100] we are generating two blocks with input/output 200/100 and 100/100 respectively. The output of the second Resnet blocks, of dim (bsz, 100) is passed through tab_resnet_mlp , the 2-layer MLP, and finally \"plugged\" into the output neuron. In summary: Embeddings + continuous + dense Renset + MLP.",
            "To run it, the code is, as one might expect identical to the one shown before for the TabMlp.",
            "And now, last but not least, the last addition to the library, the TabTransformer.",
            "The TabTransformer is described in detail in TabTransformer: Tabular Data Modeling Using Contextual Embeddings [2], by the clever guys at Amazon. Is an entertaining paper that I, of course, strongly recommend if you are going to use this model on your tabular data (and also in general if you are interested in DL for tabular data).",
            "My implementation is not the only one available. Given that the model was conceived by the researchers at Amazon, it is also available in their fantastic autogluon library (which you should definitely check). In addition, you can find another implementation here by Phil Wang, whose entire github is simply outstanding. My implementation is partially inspired by these but has some particularities and adaptations so that it works within the pytorch-widedeep package.",
            "The following figure illustrates the TabTransformer model architecture.",
            "The dashed-border boxes in the figure indicate that the component is optional.",
            "As in previous cases, there are a number of variants and details to consider as one builds the model. I will describe some here, but for a full view of all the possible parameters, please, have a look to the docs.",
            "I don\u2019t want to go into the details of what is a Transformer [3] in this post. There is an overwhelming amount of literature if you wanted to learn about it, with the most popular being perhaps The Annotated Transformer. Also check this post and if you are a math \u201cmaniac\u201d you might like this paper [4]. However, let me just briefly describe here it so I can introduce the little math we will need for this post. In one sentence, a Transformer consists of a multi-head self-attention layer followed by feed-forward layer, with element-wise addition and layer-normalization being done after each layer.",
            "As most of you will know, a self-attention layer comprises three matrices, Key, Query and Value. Each input categorical column, i.e. embedding, is projected onto these matrices (although see the fixed_attention option later in the post) to generate their corresponding key, query and value vectors. Formally, let K \u2208 R^{e \u00d7 d}, Q \u2208 R^{e \u00d7 d} and K \u2208 R^{e \u00d7 d} be the Key, Query and Value matrices of the embeddings where e is the embeddings dimension and d is the dimension of all the Key, Query and Value matrices. Then every input categorical column, i.e embedding, attends to all other categorical columns through an attention head:",
            "where",
            "And that is all the math we need.",
            "As I was thinking in a figure to illustrate a transformer block, I realised that there is a chance that the reader has seen every possible representation/figure. Therefore, I decided to illustrate the transformer block in a way that relates directly to the way it is implemented.",
            "The letters in parenthesis indicate the dimension of the tensor after the operation indicated in the corresponding box. For example, the tensor attn_weights has dim (b, h, s, s).",
            "As the figure shows, the input tensor X is projected onto its key, query and value matrices. These are then \u201cre-arranged into\u201d the multi-head self-attention layer where each head will attend to part of the embeddings. We then compute A (Eq 2), which is then multiplied by V to obtain what I refer as attn_score (Eq 1). The attn_score tensor is then re-arranged, so that we \"collect\" the attention scores from all the heads, and projected again to obtain the results, attn_out. attn_out will be added to the input (X) and normalised, resulting in Y. Finally Y goes through the Feed-Forward layer and a further Add + Norm operation.",
            "Before moving to the code related to building the model itself, there are a couple of details in the implementation that are worth mentioning. These options are also available in the Autogluon library.",
            "FullEmbeddingDropout",
            "when building a TabTransformer model, there is the possibility of dropping entirely the embedding corresponding to a categorical column. This is set by the parameter full_embed_dropout: bool, which points to the class FullEmbeddingDropout.",
            "SharedEmbeddings",
            "when building a TabTransformer model, it is possible for all the embeddings that represent a categorical column to share a fraction of their embeddings, or define a common separated embedding per column that will be added to the column's embeddings.",
            "The idea behind this so-called \u201ccolumn embedding\u201d is to enable the model to distinguish the classes in one column from those in the other columns. In other words, we want the model to learn representations not only of the different categorical values in the column, but also of the column itself. This is attained by the shared_embed group of parameters: share_embed : bool, add_shared_embed: bool and frac_shared_embed: int. The first simply indicates if embeddings will be shared, the second sets the sharing strategy and the third one the fraction of the embeddings that will be shared, depending on the strategy. They all relate to the class SharedEmbeddings",
            "For example, let\u2019s say that we have a categorical column with 5 different categories that will be encoded as embeddings of dim 8. This will result in a lookup table for that column of dim (5, 8). The two sharing strategies are illustrated in Fig 6.",
            "fixed_attention",
            "As I mentioned before my implementation of fixed attention is inspired by that at the Autogluon library. so go and check the two it if you have the time.",
            "When using \"fixed attention\", the key and query matrices are not the result of any projection of the input tensor X, but learnable matrices (referred as fixed_key and fixed_query) of dim (number of categorical columns x embeddings dim defined separately, as you instantiate the model. fixed_attention does not affect how the Value matrix V is computed.",
            "Let me go through an example with numbers to clarify things. Let\u2019s assume we have a dataset with 5 categorical columns that will be encoded by embeddings of dim 4 and we use a batch size (bsz) of 6. Figure 7 shows how the key matrix will be computed for a given batch (same applies to the query matrix) with and without fixed attention.",
            "As I mentioned multiple times (only fair), this implementation is inspired by that at the Autogluon library. Since the guys at Amazon are the ones that came up with the TabTransformer, is only logical to think that they found a use for this implementation of attention. However, at the time of writing such use is not 100% clear to me, and I did not find any useful information in the paper. Nonetheless, it is known that, in problems like machine translation, most attention heads learn redundant patterns (see e.g. Alessandro Raganato et al., 2020 [4] and references therein). Therefore, maybe the fixed attention mechanism discussed here helps reducing redundancy for problems involving tabular data.",
            "Overall, the way I interpret fixed_attention,in layman\u2019s terms, is the following: when using fixed attention, the Key and the Query matrices are defined as the model is instantiated, and do not know of the input until the attention weights (attn_weights) are multiplied by the value matrix to obtain what I refer as attn_score in figure 5. Those attention weights, which are in essence the result of a matrix multiplication between the key and the query matrices (plus softmax and normalization), are going to be the same for all the heads, for all samples in a given batch. Therefore, my interpretation is that when using fixed attention, we reduce the attention capabilities of the transformer, which will focus on less aspects of the inputs, reducing potential redundancies.",
            "Anyway, enough speculation. Time to have a look to the code. Note that, since we are going to stack the embeddings (instead of concatenating them) they all must have the same dimensions. Such dimension is set as we build the model instead that at the pre-processing stage. To avoid input format conflicts we use the for_tabtransformer parameter at pre-processing time.",
            "And now, simply",
            "If you go to the original post and you have a look to the model architecture, you will see that, as always, model is an instance of a WideDeep object formed by a single component, deeptabular that is aTabTransformermodel. TabTransformeris formed by a series of embedding layers (e.g. emb_layer_education) , a series of transformer encoder blocks** ( blks) and a MLP ( tab_transformer_mlp ). The embeddings here are of class SharedEmbeddings, which I described before. These embeddings are stacked and passed through three transformer blocks. The output for all the categorical columns is concatenated, resulting in a tensor of dim (bsz, 192) where 192 is equal to the number of categorical columns (6) times the embedding dim (32). This tensor is then concatenated with the \"layernormed\" continuous columns, resulting in a tensor of dim (bsz, 196). As usual, this tensor goes through tab_transformer_mlp , which following the guidance in the paper (\u201c The MLP layer sizes are set to {4 \u00d7 l, 2 \u00d7 l}, where l is the size of its input.\u201d) is [784 -> 392] , and \u201coff we go\". In summary SharedEmbeddings + continuous + Transformer encoder blocks + MLP.",
            "To run it, the code is, as one might expect identical to the one shown before for the TabMlp and TabRenset.",
            "**Note that there is a small inconsistency in the naming of the TabTransformer main components relative to the other two models. If you installed the package via pypi, the transformer encoder blocks are named blks . A name more consistent with the other models would be, for example, tab_transformer_blks. I realised of such inconsistency just after publishing v0.4.8 to pypi. Such small issue is not worthy of another sub-version. However, this is fixed if you install the package from github and both the pypi and the github versions will be consistent in future releases.",
            "In this post my intention was to illustrate how one can use pytorch-widedeep as a library for \"standard DL for tabular data\", i.e. without building wide and deep models and for problems that do not involve text and/or images (if you wanted to learn more about the library please visit the repo, the documentation, or the previous posts). To that aim the only component that we need is the deeptabular component, for which pytorch-widedeep comes with 3 models implemented \"out of the box\": TabMlp, TabResnet and TabTransformer. In this post I have explained their architecture in detail and how to use them within the library. In the no-so-distant future I intend to implement TabNet and perhaps Node [6] as well as performing a proper benchmarking exercise so I can set robust defaults and then release version 1.0. Of course, you can help me by using the package in your datasets \ud83d\ude42. If you found this post useful and you like the library, please give a star to the repo. Other than that, happy coding.",
            "[1] TabNet: Attentive Interpretable Tabular Learning, Sercan O. Arik, Tomas Pfister, arXiv:1908.07442v5",
            "[2] TabTransformer: Tabular Data Modeling Using Contextual Embeddings. Xin Huang, Ashish Khetan, Milan Cvitkovic, Zohar Karnin, 2020. arXiv:2012.06678v1",
            "[3] Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, et al., 2017. arXiv:1706.03762v5",
            "[4] A Mathematical Theory of Attention, James Vuckovic, Aristide Baratin, Remi Tachet des Combes, 2020. arXiv:2007.02876v2",
            "[5] Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation. Alessandro Raganato, Yves Scherrer, J\u00f6rg Tiedemann, 2020. arXiv:2002.10260v3",
            "[6] Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data. Sergei Popov, Stanislav Morozov, Artem Babenko, arXiv:1909.06312v2"
        ]
    },
    "https://towardsdatascience.com/an-introduction-to-graph-neural-network-gnn-for-analysing-structured-data-afce79f4cfdc": {
        "author": "Shanon Hong",
        "length": "12 min read",
        "title": "An Introduction to Graph Neural Network(GNN) For Analysing Structured Data",
        "tags": [
            "machine-learning",
            "neural-networks",
            "graph",
            "graph-neural-networks",
            "graph-convolution"
        ],
        "content": [
            "Graph Neural Network(GNN) recently has received a lot of attention due to its ability to analyze graph structural data. This article gives a gentle introduction to Graph Neural Network. It covers some graph theories for the ease to understand graphs and the problems in analyzing graphs. It then introduces Graph Neural Network in different forms and their principles. It also covers what GNN can do and some applications of GNN.",
            "First, we need to know what is a graph.",
            "A graph is a data structure consisting of two components: vertices, and edges. It is used as a mathematical structure to analyze the pair-wise relationship between objects and entities. Typically, a graph is defined as G=(V, E), where V is a set of nodes and E is the edges between them.",
            "A graph is often represented by an Adjacency matrix, A. If a graph has N nodes, then A has a dimension of (NxN). People sometimes provide another feature matrix to describe the nodes in the graph. If each node has F numbers of features, then the feature matrix X has a dimension of (NxF).",
            "Firstly, a graph does not exist in a Euclidean space, which means it cannot be represented by any coordinate systems that we are familiar with. This makes the interpretation of graph data much harder as compared to other types of data such as waves, images, or time-series signals(\u201ctext\u201d can also be treated as time-series), which can be easily mapped to a 2-D or 3-D Euclidean space.",
            "Secondly, a graph does not have a fixed form. Why? Look at the example below. Graph (A) and Graph (B) have a completely different structure and visually different. But when we convert it to adjacency matrix representation, the two graphs have the same adjacency matrix (if we don\u2019t consider the weight of edges). So should we consider these two graphs are the same or different?",
            "And lastly, a graph is in general hard to visualize for human interpretation. I\u2019m not talking about small graphs like the examples above. I\u2019m talking about giant graphs that involve hundreds or thousands of nodes. The dimension is very high and nodes are densely grouped, making it even difficult for a human to understand the graph. Therefore, it is challenging to train a machine for this task. The example below shows a graph modeling the logic gates in an integrated circuit.",
            "The reasons that people choose to work on graphs can be summarized in a few points as listed below:",
            "Graphs provide a better way of dealing with abstract concepts like relationships and interactions. They also offer an intuitively visual way of thinking about these concepts. Graphs also form a natural basis for analyzing relationships in a social context. Graphs can solve more complex problems by simplifying the problems into simpler representations or transform the problems into representations from different perspectives. Graph Theories and concepts are used to study and model Social Networks, Fraud patterns, Power consumption patterns, Virality and Influence in Social Media. Social Network Analysis (SNA) is probably the best-known application of Graph Theory for Data Science .",
            "Traditional methods are mostly algorithm-based, such as :",
            "searching algorithms, e.g. BFS, DFS shortest path algorithms, e.g. Dijkstra\u2019s algorithm, Nearest Neighbour spanning-tree algorithms, e.g. Prim\u2019s algorithm clustering methods, e.g. Highly Connected Components, k-mean",
            "The limitation of such algorithms is that we need to gain prior knowledge of the graph at certain confidence before we can apply the algorithm. In other words, it provides no mean for us to study the graph itself. And most importantly, there is no way to perform graph level classification.",
            "Graph Neural Network, as how it is called, is a neural network that can directly be applied to graphs. It provides a convenient way for node level, edge level, and graph level prediction task.",
            "There are mainly three types of graph neural networks in the literature:",
            "Recurrent Graph Neural Network Spatial Convolutional Network Spectral Convolutional Network",
            "The intuition of GNN is that nodes are naturally defined by their neighbors and connections. To understand this we can simply imagine that if we remove the neighbors and connections around a node, then the node will lose all its information. Therefore, the neighbors of a node and connections to neighbors define the concept of the node.",
            "Having this in mind, we then give every node a state (x) to represent its concept. We can use the node state (x) to produce an output (o), i.e. decision about the concept. The final state (x_n) of the node is normally called \u201cnode embedding\u201d. The task of all GNN is to determine the \u201cnode embedding\u201d of each node, by looking at the information on its neighboring nodes.",
            "We will start with the most pioneer version of Graph Neural Network, Recurrent Graph Neural Network, or RecGNN.",
            "As introduced in the original GNN paper, RecGNN is built with an assumption of Banach Fixed-Point Theorem. Banach Fixed-Point Theorem states that: Let (X,d) be a complete metric space and let (T:X\u2192X) be a contraction mapping. Then T has a unique fixed point (x\u2217) and for any x\u2208X the sequence T_n(x) for n\u2192\u221e converges to (x\u2217). This means if I apply the mapping T on x for k times, x^k should be almost equal to x^(k-1), i.e.:",
            "RecGNN defines a parameterized function f_w:",
            "where l_n, l_co, x_ne, l_ne represents the features of the current node [n], the edges of the node [n], the state of the neighboring nodes, and the features of the neighboring nodes. (In the original paper, the author referred node features as node labels. This might make some confusion.)",
            "Finally, after k iterations, the final node state is used to produce an output to make a decision about each node. The output function is defined as:",
            "The intuition of Spatial Convolution Network is similar to that of the famous CNN which dominates the literature of image classification and segmentation tasks. To understand CNN on images, you can check out this post which explains CNN in detail.",
            "In short, the idea of convolution on an image is to sum the neighboring pixels around a center pixel, specified by a filter with parameterized size and learnable weight. Spatial Convolutional Network adopts the same idea by aggregate the features of neighboring nodes into the center node.",
            "As compared to other types of GNN, this type of graph convolution network has a very strong mathematics foundation. Spectral Convolutional Network is built on graph signal processing theory. And by simplification And approximation of graph convolution.",
            "By Chebyshev polynomial approximation (Hammond et al. 2011), graph convolution can be simplified to below form:",
            "After further simplification, the GCN paper suggests a 2-layered neural network structure, which can be described in one equation as below:",
            "where A_head is the pre-processed Laplacian of original graph adjacency matrix A. (Details of the mathematics can be found in GCN paper. It will take much effort to fully explain.)",
            "This formula looks very familiar if you have some experience in machine learning. This is nothing but two fully-connected layer structure that is commonly used. But it indeed serves as graph convolution in this case. I will show why it can perform graph convolution below.",
            "Let\u2019s consider we have a simple graph with 4 nodes. Each of these nodes is assigned a feature matrix as shown in the figure above. It is easy to come out with a graph adjacency matrix and feature matrix as shown below:",
            "Note that the diagonal of the adjacency matrix is purposely changed to \u20181\u2019 to add a self-loop for every node. This is to include the feature of every node itself when we perform feature aggregation.",
            "We then perform AxX (let\u2019s forget about the Laplacian of A and the weight matrix W first for simplicity of explanation.)",
            "The result of matrix multiplication is shown in the rightmost matrix. Let\u2019s look at the resulted feature of the first node as an example. It is not hard to notice that the result is a sum of all features of [node 1] including the feature of [node 1] itself, and features in [node 4] are not included since its not the neighbor of [node 1]. Mathematically, the adjacency matrix of the graph has value \u20181\u2019 only when there is an edge, and \u20180\u2019 otherwise. This makes the matrix multiplication become the summation of features of nodes that are connected to the reference node.",
            "Therefore, Spectral Convolutional Network and Spatial Convolutional Network, although started on a different basis, share the same propagation rule.",
            "All convolutional graph neural networks currently available share the same format. They all try to learn a function to pass the node information around and update node state by this message passing process.",
            "Any Graph Neural Network can be expressed as a Message Passing Neural Network (J. Gilmer et al. , 2017) with a message-passing function, a node update function and a readout function.",
            "The problems that GNN solve can be broadly classified into three categories:",
            "Node Classification Link Prediction Graph Classification",
            "In node classification, the task is to predict the node embedding for every node in a graph. This type of problem is usually trained in a semi-supervised way, where only part of the graph is labeled. Typical applications for node classification include citation networks, Reddit posts, Youtube videos, and Facebook friends relationships.",
            "In link prediction, the task is to understand the relationship between entities in graphs and predict if two entities have a connection in between. For example, a recommender system can be treated as link prediction problem where the model is given a set of users\u2019 reviews of different products, the task is to predict the users\u2019 preferences and tune the recommender system to push more relevant products according to users\u2019 interest.",
            "In graph classification, the task is to classify the whole graph into different categories. It is similar to image classification but the target changes into graph domain. There is a wide range of industrial problems where graph classification can be applied, for example, in chemistry, biomedical, physics, where the model is given a molecular structure and asked to classify the target into meaningful categories. It accelerates the analysis of atom, molecule or any other structured data types.",
            "Having understand what types of analysis that GNN can perform, you must be wondering what are the real things that I can do with graphs. Well, this section will give you more insights into GNN\u2019s real-world applications.",
            "GNN is widely used in Natural Language Processing (NLP). Actually, this is also where GNN initially gets started. If some of you have experience in NLP, you must be thinking that text should be a type of sequential or temporal data which can be best described by an RNN or an LTSM. Well, GNN approaches the problem from a completely different angle. GNN utilized the inner relations of words or documents to predict the categories. For example, the citation network is trying to predict the label of each paper in the network given by the paper citation relationship and the words that are cited in other papers. It can also build a syntactic model by looking at different parts of a sentence instead of purely sequential as in RNN or LTSM.",
            "Many CNN based methods have achieved state-of-the-art performance in object detections in images, but yet we do not know the relationships of the objects. One successful employment of GNN in CV is using graphs to model the relationships between objects detected by a CNN based detector. After objects are detected from the images, they are then fed into a GNN inference for relationship prediction. The outcome of the GNN inference is a generated graph that models the relationships between different objects.",
            "Another interesting application in CV is image generation from graph descriptions. This can be interpreted as almost the reverse of the application mentioned above. The traditional way of image generation is text-to-image generation using GAN or autoencoder. Instead of using text for image description, graph to image generation provides more information on the semantic structures of the images.",
            "The most interesting application I would like to share is zero-shot learning (ZSL). You can find this post for a comprehensive introduction to ZSL. In short, ZSL is trying to learn to classify a class given NO training samples (of the target classes) at all. It was quite challenging because if no training samples were given, we need to let the model \u201cthink\u201d logically to recognize a target. For example, if we were given three images (as shown in the figure below) and told to find \u201cokapi\u201d among them. We may not have seen an \u201cokapi\u201d before. But if we were also given the information that an \u201cokapi\u201d is a deer-face animal with four legs and has zebra-striped skin, then it is not hard for us to figure out which one is \u201cokapi\u201d. Typical methods are simulating this \u201cthinking process\u201d by converting the detected features into text. However, text encodings are independent among each other. It is hard to model the relationships between the text descriptions. In other hard, graph representations well model these relationships, making the machine to think in a more \u201chuman-like\u201d manner.",
            "More practical applications of GNN include human behavior detection, traffic control, molecular structure study, recommender system, program verification, logical reasoning, social influence prediction, and adversarial attack prevention. Below shows a graph that models the relationships of people in a social network. GNN can be applied to cluster people into different community groups.",
            "We went through some graph theories in this article and emphasized on the importance to analyze graphs. People always see machine learning algorithm as a \u201cblack box\u201d. Most machine learning algorithms only learn from the features of training data but there is no actual logic to perform. With graphs, we might be able to pass some \u201clogics\u201d to the machine and let it \u201cthink\u201d more naturally.",
            "GNN is still a relatively new area and is worthy of more research attention. It is a powerful tool to analyze graph data. Yet it is not limited to only problems in graphs. It can be easily generalized to any studies that can be modeled by graphs. And graph modeling is a natural way to analyze a problem.",
            "F.Scarselli, M.Gori, \u201cThe graph neural network model,\u201d IEEE Transactions on Neural Networks, 2009 T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in Proc. of ICLR , 2017. Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, Philip S. Yu, \u201cA Comprehensive Survey on Graph Neural Networks\u201d, arXiv:1901.00596 D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei, \u201cScene graph generation by iterative message passing,\u201d in Proc. of CVPR , vol. 2, 2017 J. Johnson, A. Gupta, and L. Fei-Fei, \u201cImage generation from scene graphs,\u201d in Proc. of CVPR , 2018 X. Wang, Y. Ye, and A. Gupta, \u201cZero-shot recognition via semantic embeddings and knowledge graphs,\u201d in CVPR 2018"
        ]
    },
    "https://medium.com/neo4j/article-recommendation-with-personalized-pagerank-and-full-text-search-c0203dd833e8": {
        "author": "Mark Needham",
        "length": "4 min read",
        "title": "Article recommendation with Personalized PageRank and Full Text Search",
        "tags": [
            "neo4j",
            "pagerank",
            "recommendations",
            "graph-algorithms",
            "data-science"
        ],
        "content": [
            "6 months ago Tomaz Bratanic wrote a great blog post showing how to build an article recommendation engine using NLP techniques and the Personalized PageRank algorithm from the Graph Algorithms library.",
            "In the post Tomaz extracts key words for each article using the GraphAware NLP library, and then runs PageRank in the context of articles based on these key words.",
            "I was curious whether I could create a poor man\u2019s version of Tomaz\u2019s work using the Full Text Search functionality that was added in Neo4j 3.5, and so here we are!",
            "Tomaz explains how to import the data in his post, so we\u2019ll continue from there. The diagram below shows the graph model that we\u2019ll be working with. We have articles written by authors, and those articles can reference each other.",
            "The first thing we need to do is create a Full Text Search index for our Article nodes. We\u2019ll index the title and abstract properties on these nodes.",
            "CALL db.index.fulltext.createNodeIndex('articlesAll', ['Article'], ['title', 'abstract'])",
            "We can check on the progress of the index creation by running the following query:",
            "CALL db.indexes()",
            "It will have a state of POPULATING while node properties are being added to the index. This state will change to ONLINE once it\u2019s done. The following query will block until the index is online:",
            "CALL db.index.fulltext.awaitIndex(\"articlesAll\")",
            "Now that we\u2019ve done this, let\u2019s get on with the algorithms.",
            "Tomaz first explores articles that contain the phrase \u201csocial networks\u201d. Let\u2019s create a parameter containing that search term:",
            ":param searchTerm => '\"social networks\"'",
            "Not that we\u2019ve put the search term in quotes. We do this so that Full Text Search will treat the term as a phrase rather than interpreting each term separately.",
            "Now we want to call the PageRank algorithm from the point of view of articles that contain this search term. Let\u2019s first see how many articles the full text index comes back with:",
            "CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node, score RETURN count(*)",
            "Just under 15,000 nodes, or around 0.5% of all articles are returned by the query. The following query will return the top 10 articles for the search term:",
            "CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node, score RETURN node.id, node.title,  score LIMIT 10",
            "Now we can feed these nodes into the PageRank algorithm as the sourceNodes config parameter. This will bias the results of the algorithm around these nodes.",
            "The following query will find us the most influential articles about social networks:",
            "CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node WITH collect(node) as articles CALL algo.pageRank.stream('Article', 'REFERENCES', { sourceNodes: articles }) YIELD nodeId, score WITH nodeId,score ORDER BY score DESC LIMIT 10 RETURN algo.getNodeById(nodeId).title as article, score",
            "As in Tomaz\u2019s post, Sergey Brin and Larry Page\u2019s paper describing Google shows up in first place.",
            "In the next part of the post, Tomaz shows how we can write queries to find papers that would be interesting to researchers in different fields.",
            "Recommendation of articles described by keyword \u201centropy\u201d from the point of view of Jose C. Principe.",
            "Let\u2019s setup parameters:",
            ":param authorName => \"Jose C. Principe\"; :param searchTerm => \"entropy\"",
            "And now run the query:",
            "MATCH (a:Article)-[:AUTHOR]->(author:Author) WHERE author.name=$authorName WITH author, collect(a) as articles CALL algo.pageRank.stream( 'CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node RETURN id(node) as id', 'MATCH (a1:Article)-[:REFERENCES]->(a2:Article) RETURN id(a1) as source,id(a2) as target', { sourceNodes: articles, graph:'cypher', params: {searchTerm: $searchTerm}}) YIELD nodeId, score WITH author, nodeId, score WITH algo.getNodeById(nodeId) AS n, score WHERE not(exists((author)-[:AUTHOR]->(n))) RETURN n.title as article, score, [(n)-[:AUTHOR]->(author) | author.name][..5] AS authors order by score desc limit 10",
            "We\u2019ll see these results:",
            "And what about if we run the same query for a different author?",
            ":param authorName => \"Hong Wang\";",
            "We\u2019ll see this results:",
            "We don\u2019t get exactly the same results as Tomaz, but we do still get a different set of results for the different authors.",
            "So in summary, it does seem that we can get a reasonable approximation of Tomaz\u2019s post using Neo4j\u2019s Full Text Search functionality.",
            "If you have any other ideas of what we can do with this dataset, let me know by emailing devrel@neo4j.com"
        ]
    },
    "https://medium.com/@francesco.murdaca91/automatic-knowledge-construction-of-a-grakn-knowledge-graph-4cb62a71a1ea": {
        "author": "Francesco Murdaca",
        "length": "8 min read",
        "title": "Automatic Knowledge Construction of a Grakn Knowledge Graph",
        "tags": [
            "big-data",
            "intelligent-assistant"
        ],
        "content": [
            "Grakn is a knowledge graph. Knowledge graphs contain two fundamentals components, semantic and size, as described here:",
            "A knowledge graph is necessarily built on semantics. Semantics, they argue, is the basis for creating new inferences from the data which would otherwise go unseen. It\u2019s the difference between something that generates new knowledge and a database laying dormant, waiting to be queried. Anything less is just a labeled graph. A knowledge graph isn\u2019t like any other database; it is supposed to provide new insights, which can be used to infer new things about the world. If it\u2019s just a bunch of labeled arrows, then that doesn\u2019t comport with the concept of a knowledge graph as an artificial intelligence technique. At that point, it\u2019s just a fancy database.",
            "In Grakn, a knowledge graph is made of two layers: the SCHEMA layer and the DATA layer. This post illustrates an approach for the automatic generation of the latter one. I will talk about the automatic creation of the SCHEMA layer in a future post.",
            "In general, once the SCHEMA layer has been created, the user focuses on its population. The DATA layer in Grakn contains all the instances of entities, attributes, relationships that can be queried from the user.",
            "In order to allow automatic creation of Graql insert queries it\u2019s necessary to implement functions that generate automatically the queries for Grakn client libraries in Java, Python and Node.js according to a general structure that minimizes the reimplementation of the pipeline.",
            "This post will focus on:",
            "the description of a general structure for the automatic migration of data instances into Grakn knowledge graph; the description of examples of the structure to allow transactions through the Grakn API; Simple DEMO on the new approach; the advantages of the new approach.",
            "ASSUMPTION: SCHEMA layer is already in the Grakn keyspace. For the example, I will use the phone_calls SCHEMA layer created by Soroush Saffari in one of his article regarding the generation of the Grakn schema.",
            "CODE: it\u2019s available open source on the GitHub repository GraknAutoPOP.",
            "In a previous article, I introduced the architecture of smart-dog, a framework used for the semi-automatic generation and population of a knowledge graph, respectively of the SCHEMA layer and DATA layer. The Grakn Module in the smart-dog architecture is a wrapper of the Grakn Python client. The automation is required because the system using smart-dog learning once a new type of source is inserted in its \u2018memory\u2019.",
            "In the frame of the Design Engineering Assistant (DEA) project, the Grakn technology will be used in many ways (Graph Convolutional Network, Word Sense Disambiguation, Recommender system, Query/Answering,\u2026) therefore the automatic generation of the Grakn knowledge graph is mandatory in order to boost all these tasks and deal with big data.",
            "The data instances require specific information to be recognized as valid queries from the Grakn client API. Therefore it is fundamental to generate a structure for the different possible data instances allowed in Grakn. This structure is the core element for the automation of the process. It is described in a json file, but it could be adapted to other formats of exchange files (.csv, .xml). The choice is obvious as this format can be easily parsed in different programming languages.",
            "In particular, there are three main elements that shall be always present in the structure:",
            "the name of the keyspace (type String) the query type (at the moment only \u201cinsert\u201d is allowed) (type String) the transactions (type List of Dict)",
            "A transaction, in the context of a database, is a logical unit that is independently executed for data retrieval or updates. All the transactions are in a List of Dict and every Dict is a transaction for an instance of an entity or for instance of relationship. Ideally, the list of transactions can be \u201cinfinite\u201d.",
            "The structure that allows the automatic generation of graql insert queries required the analysis of the Grakn concepts:",
            "an entity can have attributes and play roles in relationships; an attribute expresses a value of a specified datatype . It can have attributes of its own and play roles in relationships; a relationship , has at least one roleplayer, can have attributes and play roles in other relationships.",
            "This paragraph will show examples of a single transaction for several cases.",
            "INSTANCE OF ENTITY",
            "CASE 1 : an entity with a single attribute",
            "{ \"keyspace\": \"keyspace_name\", \"query_type\":\"insert\", \"transactions\":  [ { \"entity\":\"entity_name\", \"attributes_and_values\": {\"attribute:\"value\"} } ] }",
            "CASE 2: an entity with multiple attributes",
            "{ \"keyspace\": \"keyspace_name\", \"query_type\":\"insert\", \"transactions\":  [ {\"entity\":\"entity_name\", \"attributes_and_values\": {\"attribute1:\"value1\", \"attribute2:\"value2\", \"attributeN:\"valueN\"} } ] }",
            "INSTANCE OF RELATIONSHIP",
            "CASE 3: a relationship with two roles (entity with multiple attributes and no attributes for the relationship)",
            "{ \"keyspace\": \"keyspace_name\", \"query_type\":\"insert\", \"transactions\":  [ {\"entity\":[\"entity1\", \"entity2\"], \"attributes_and_values\": [{\"attribute1_entity1:\"value11\", \"attribute2_entity1:\"value12\", \"attributeN_entity1:\"value1N\"}, {\"attribute1_entity2:\"value21\", \"attribute2_entity2:\"value22\", \"attributeN_entity2:\"value2N\"}], \"relationship\":\"relationship_name\", \"r_attributes_and_values\":{}, \"roles\":[\"role1\",\"role2\"] } ] }",
            "CASE 4 : a relationship with two roles, entity with multiple attributes and attributes for the relationship",
            "{ \"keyspace\": \"keyspace_name\", \"query_type\":\"insert\", \"transactions\":  [ {\"entity\":[\"entity1\", \"entity2\"], \"attributes_and_values\": [{\"attribute1_entity1:\"value11\", \"attribute2_entity1:\"value12\", \"attributeN_entity1:\"value1N\"}, {\"attribute1_entity2:\"value21\", \"attribute2_entity2:\"value22\", \"attributeN_entity2:\"value2N\"}], \"relationship\":\"relationship_name\", \"r_attributes_and_values\":{ \"r_attribute1\":\"r_value1\", \"r_attribute2\":\"r_value2\", \"r_attributeN\":\"r_valueN\"}, \"roles\":[\"role1\",\"role2\"] } ] }",
            "CASE 5 : a relationship with one role (entity with multiple attributes without relationship attributes)",
            "{ \"keyspace\": \"keyspace_name\", \"query_type\":\"insert\", \"transactions\": [ {\"entity\":[\"entity1\", \"entity2\"], \"attributes_and_values\": [{\"attribute1_entity1:\"value11\", \"attribute2_entity1:\"value12\", \"attributeN_entity1:\"value1N\"}, {\"attribute1_entity2:\"value21\", \"attribute2_entity2:\"value22\", \"attributeN_entity2:\"value2N\"}], \"relationship\":\"relationship_name\", \"r_attributes_and_values\":{}, \"roles\":[\"role\"] } ] }",
            "CASE 6: a relationship with one role (entities with multiple attributes and relationship with attributes)",
            "{ \"keyspace\": \"keyspace_name\", \"query_type\":\"insert\", \"transactions\": [ {\"entity\":[\"entity1\", \"entity2\"], \"attributes_and_values\": [{\"attribute1_entity1:\"value11\", \"attribute2_entity1:\"value12\", \"attributeN_entity1:\"value1N\"}, {\"attribute1_entity2:\"value21\", \"attribute2_entity2:\"value22\", \"attributeN_entity2:\"value2N\"}], \"relationship\":\"relationship_name\", \"r_attributes_and_values\":{ \"r_attribute1\":\"r_value1\", \"r_attribute2\":\"r_value2\", \"r_attributeN\":\"r_valueN\"}, \"roles\":[\"role\"] } ] }",
            "CASE 7 : a relationship with three or more roles (entities with multiple attributes and a relationship with or without attributes)",
            "{ \"keyspace\": \"keyspace_name\", \"query_type\":\"insert\", \"transactions\": [ {\"entity\":[\"entity1\", \"entity2\"], \"attributes_and_values\": [{\"attribute1_entity1:\"value11\", \"attribute2_entity1:\"value12\", \"attributeN_entity1:\"value1N\"}, {\"attribute1_entity2:\"value21\", \"attribute2_entity2:\"value22\", \"attributeN_entity2:\"value2N\"}, {\"attribute1_entity3:\"value31\", \"attribute2_entity3:\"value32\", \"attributeN_entity3:\"value3N\"}], \"relationship\":\"relationship_name\", \"r_attributes_and_values\":{ \"r_attribute1\":\"r_value1\", \"r_attribute2\":\"r_value2\", \"r_attributeN\":\"r_valueN\"}, \"roles\":[\"role1\",\"role2\",\"role3\"] } ] }",
            "In the previous paragraphs, I illustrated the general structure of the transactions through several examples.",
            "But how do we avoid the creation of specific functions that are different in every domain analyzed by the user?",
            "Using the structure described in section 2, I started to work on a wrapper of the Python Grakn client that allows me to perform:",
            "graql insert query type; graql match query type.",
            "These functions parse automatically each information in transaction create the graql insert query. All the functions use READ and WRITE transaction of the Grakn Python Client, respectively to match something inside and to insert something inside the Grakn Knowledge Graph.",
            "In particular, in the graql insert class there are two functions:",
            "graql_insert_entity_instance",
            "This function allows the insertion of an entity instance inside the Grakn Knowledge Graph. The requested inputs are provided by the information in each transaction automatically.",
            "graql_insert_relationship_instance",
            "This function instead allows the insertion of a relationship instance inside the Grakn Knowledge Graph.",
            "Both functions described above rely on the graql match functions.",
            "graql_match_entity_instance",
            "graql_match_relationship_instance",
            "Before inserting any new instance (entity or relationship) the graql match functions create a query for the Grakn Knowledge Graph to check if the instance is already inside. Using this type of check no insertion of duplicates of instances is allowed.",
            "One of the most complex and important factor to take into account is the datatype. I implemented a specific function that checks the datatype before creating the query according to Grakn datatype ADD REFERENCE. This is fundamental to guarantee consistency with the Grakn schema created.",
            "I will compare this new approach with the process shown by Soroush Saffari, about the population and query of the Grakn Knowledge Graph from json, xml, csv.",
            "STEP 1: Create the input file with all transactions",
            "I took the 4 inputs file in json format (companies, people, calls, contracts) to create the input file required for the function to enter automatically all the transactions. This function called generate_phone_calls_input_fileis created to manage the different inputs and convert them in single transactions for the functions that interact with the Grakn knowledge graph.",
            "STEP 2: Start automatic population",
            "The only script used to run all the transactions (241) is shown in the picture below and it is called grakn_kg_population_main.",
            "There are several checks to avoid inconsistency in the generation of the graql query for the transactions. Indeed, if the generated query contains data which don\u2019t follow the standard structure described in section 1, an error with explanation will pop up. The error flag shows the type of error, where is located the transaction, why this transaction has that error and suggestions on how to create the standard structure for a data instance.",
            "Another check is performed for the number of entities and the number of dictionaries in the list of attributes if not matched.",
            "At the end it is also possible to verify the number of transactions performed, so how many instances and relationship have been inserted or not.",
            "STEP 3: Query the phone_calls knowledge graph",
            "You can also check that querying the knowledge graph like in Soroush\u2019s article, you will obtain the same results, therefore the test of these functions is reliable, but of course, I would be happy to receive feedbacks, opinions or work together to improve it. I believe in the open source and in the power of the community.",
            "This type of generalized structure have different advantages:",
            "no need to worry about writing variables in the graql query; reduce the possibility to write inconsistent graql query; possibility to trace changes for the instances; save time for the developer; improve consistency (you cannot insert two equal instances twice!!)",
            "Thanks Soroush and all Grakn Team for all great work you are doing!",
            "Contacts",
            "francesco.murdaca@strath.ac.uk",
            "Francesco",
            "PhD student",
            "____________________________________________",
            "ICE Lab",
            "the Intelligent Computational Engineering Laboratory",
            "www.icelab.uk",
            "info@icelab.uk",
            "_____________________________________________",
            "Aerospace Centre of Excellence",
            "Department of Mechanical & Aerospace Engineering",
            "Strathclyde University",
            "James Weir Building, 75 Montrose Street",
            "G1 1XJ Glasgow"
        ]
    },
    "https://medium.com/cgwire/cg-pipeline-the-best-graph-database-for-your-cg-production-data-4cedc9e49065": {
        "author": "Frank Rousseau",
        "length": "11 min read",
        "title": "CG Pipeline: The Best Graph Database for Your CG Production Data",
        "tags": [
            "database",
            "graph",
            "graph-database",
            "cg-pipeline"
        ],
        "content": [
            "As we mentioned in a previous blog post, A CG production can be represented as a graph structure. A movie is made of shots which are generated from scene files which are themselves made of elements linked by relationships. Nevertheless, when we store production data into a database, we tend to use a flat description of the data. And when it\u2019s time to chose a database, the most common choice is to rely on relational databases.",
            "Using a relational database is a good choice: it\u2019s safe and does the job well. But, nowadays, a few database technologies propose to store your data directly formatted as graphs. Initially, they are mostly used to deal with social networks or banking use cases. But it\u2019s no suprise that they caught the attention of many Technical Directors and Developers from CG studios. Because of the growing interest for graph databases, we decided to look closer at them.",
            "The information of a graph will make you more agile. Graph storage allows to save the dependencies of all your assets and set the versions of the elements casted in a shot. And because stored graphs are directed, you can easily compute a sequence of operations to build or rebuild an element of the scene. Which means more reactivity when the director wants to try new things.",
            "Now we have a good incentive to use graph databases, we are going to have a look at major open source graph databases available on the market.",
            "To explore these databases, we propose to implement the data graph of the props animation described in our previous article named CG production as a Graph. The approach will be to store the steps required to build the props and include it in a given shot.",
            "The most common thing we want to do with graph is to obtain all the impacts of a change on a given element. To illustrate this, we will perform a query that retrieve the elements impacted by the change on the mesh of the props.",
            "We\u2019ll provide Python snippets to show how to use each database. Then we\u2019ll run a quick benchmark. We will compare how long it takes to run 10 000 times our sample query on a i7\u20136700 CPU @ 3.40GHz . Note that this benchmark includes the Python client, we consider that you will only use your database through it. That\u2019s why we include it in our measures.",
            "The main databases we will study are the following:",
            "Neo4j (Java) ArangoDB (C++) Cayley (Go) OrientDB (Java)",
            "Cayley is a graph database distributed by Google written in Go. It looks promising on many aspects (configurable backend, community driven) but currently the documentation is close to inexistant. The best source of information is a forum dedicated to it. Whatever, let\u2019s see what we can do with.",
            "First, download the binaries related to your platform, initialize the database and run the http server which will that allow us to perfoms queries. Database initialization doesn\u2019t mean you have to give data, it\u2019s just needed to create the database files.",
            "./cayley init -db bolt -dbpath /tmp/testdb ./cayley http --dbpath=/tmp/testdb  --host 0.0.0.0 --port 64210",
            "You can notice here that another DB technology is involved (Bolt). It\u2019s because Cayley is a layer above an existing database. You can either use traditional key value stores or relational database as backend.",
            "Now let\u2019s go with the Python client code. We want to store all our assets, scenes, shots and their relations. To achieve that, we need to install the Python driver:",
            "pip install pyley",
            "Cayley is based on the concept of triplet. Everything is a vertex linked to another one: the triplet is made of three vertices: the two elements we want to link and the link vertex (kind of edge). You can add a label on each triplet, so in Cayley the term for this data structure is \u201cquads\u201d. Unfortunately the Python client is not complete and does not support Quad creation. So we need to create our quads via requests, a standard Python HTTP client (Cayley provied a REST API):",
            "def create_quad(quad): path = \u201c http://localhost:64210/api/v1/write \" return requests.post(path, json=[quad])",
            "Now let\u2019s proceed to the quad creation:",
            "quads = [ { \u201csubject\u201d: \u201cprops1-concept\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cprops1-texture\u201d }, { \u201csubject\u201d: \u201cprops1-concept\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cprops1-mesh\u201d }, { \u201csubject\u201d: \u201cprops1-texture\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cprops1-model\u201d }, { \u201csubject\u201d: \u201cprops1-mesh\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cprops1-model\u201d }, { \u201csubject\u201d: \u201cprops1-mesh\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cprops1-rig\u201d }, { \u201csubject\u201d: \u201cprops1-mesh\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cprops1-keys\u201d } { \u201csubject\u201d: \u201cprops1-rig\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cprops1-keys\u201d }, { \u201csubject\u201d: \u201cprops1-model\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cshot1-image-sequence\u201d }, { \u201csubject\u201d: \u201cprops1-keys\u201d, \u201cpredicate\u201d: \u201cdependencyof\u201d, \u201cobject\u201d: \u201cshot1-image-sequence\u201d } ] for quad in quads: create_quad(quad)",
            "That\u2019s it. As you can see we already have stored all our data and set relation between them. If you create again similar quads, nothing will change and there will be no duplicates.",
            "Now let\u2019s perform our query about the impact of a rig change on the production:",
            "from pyley import CayleyClient, GraphObject client = CayleyClient(\" http://localhost:64210 \", \"v1\") graph = GraphObject() query = graph.V(\u201cprops1-mesh\u201d) .Out() .All()",
            "To get our desired data, we had to specify which vertex (here our texture) of which we want to study the impact of. Then we just asked the outer the vertex of wich the texture is element of. We can chain the call depending on the depth of the impact we want to study. A recursive traversal is available but the Python client doesn\u2019t implement it yet. Finally we made our performance tests. It took 50 seconds to run ten thousands time this query.",
            "The visualization UI doesn\u2019t work well and is not very intuitive to use. Which is sad because Neo4j and Arango have working UIs that allow to display your graph.",
            "Cayley is a very simple database. With a single concept, the quad representation, it allows to represent our data. Querying is very easy too and based on standard graph query language such as Gremlin (you can chose your favorite query language). Unfortunately the project is still poorly documented and the Python client is uncomplete. That\u2019s why despite its clean and simple design we cannot recommend to use Cayley in production.",
            "Neo4j is the most mature solution of all. The enterprise behind it offers compelling entreprise solution for support and extra features (monitoring, backup, improved querying\u2026). That\u2019s a big advantage if you need to feel very safe due to hard contracts with your clients. But to start with it, we reommend using the community edition. This is this version that we\u2019ll cover in this article.",
            "Because we are just experimenting, we are going to use the official Docker to play with Neo4j:",
            "docker run \\ --publish=7474:7474 --publish=7687:7687 \\ --volume=$HOME/neo4j/data:/data \\ neo4j",
            "Now we can install the Python driver:",
            "pip install neo4j-driver",
            "First things first, let\u2019s initialize the connection with the database and the query session. At first connection they will ask you to set a password, you can do it through the last line of the snippet below:",
            "from neo4j.v1 import GraphDatabase, basic_auth driver = GraphDatabase.driver( \"bolt://localhost:7687\", auth=basic_auth(\"neo4j\", \"tests\") ) session = driver.session() # session.run(\"CALL dbms.changePassword('tests')\")",
            "Then let\u2019s add helpers to create asset nodes, shot nodes and relation edges. The python client does not provide a strong API, it justs allow to perform requests directly with the in-house language of Neo4j named Cypher. There is CREATE command but we\u2019ll use MERGE because it acts as CREATE if not exists:",
            "def create_asset(name): session.run( \"MERGE (a:Asset { name: $name })\", name=name ) def create_shot(name): session.run( \"MERGE (a:Shot { name: $name })\", name=name ) def create_relation(asset1, asset2): session.run( \"MATCH (a:Asset { name: $asset1 }), (b:Asset { name: $asset2 })\" \"MERGE (a)-[r:ELEMENT_OF]->(b)\", asset1=asset1, asset2=asset2 ) def create_casting(asset, shot): session.run( \"MATCH (a:Asset { name: $asset }), (b:Shot { name: $shot })\" \"MERGE (a)-[r:CASTED_IN]->(b)\", asset=asset, shot=shot )",
            "As you can see the syntax is easy to read and learn. We can add as many fields we want on a single node.",
            "Now we have our functions, let\u2019s populate our graph:",
            "create_asset(\"Props 1 concept\") create_asset(\"Props 1 mesh\") create_asset(\"Props 1 texture\") create_asset(\"Props 1 rig\") create_asset(\"Props 1 model\") create_asset(\"Props 1 keys\") create_shot(\"Shot 1\") create_relation(\"Props 1 concept\", \"Props 1 texture\") create_relation(\"Props 1 concept\", \"Props 1 mesh\") create_relation(\"Props 1 mesh\", \"Props 1 model\") create_relation(\"Props 1 texture\", \"Props 1 model\") create_relation(\"Props 1 mesh\", \"Props 1 rig\") create_relation(\"Props 1 mesh\", \"Props 1 keys\") create_relation(\"Props 1 rig\", \"Props 1 keys\") create_casting(\"Props 1 model\", \"Shot 1\") create_casting(\"Props 1 keys\", \"Shot 1\")",
            "Now we can take advantage of the expressive query language to perform our traversal. Note the star inside the arrow. It means that will traverse all nodes until there is no more out connections.",
            "result = session.run( \"MATCH (:Asset { name: 'Props 1 mesh' })-[*]->(out)\" \"RETURN out.name as name\" ) for record in result: print(\"%s\" % record[\"name\"]) session.close()",
            "We\u2019re done! Result records are easy to display and analyze. They are Python dicts containing the fields specified at creation. Running ten thousand times our request lasted 3.5 seconds (it drops to 17 seconds if you open/close the session each time).",
            "Overall, Neo4j is full featured and does the job well and it\u2019s fast compared to others. Its strong query language and its many features will allow to perform the most common use cases you will have with your graph. The official Python client is a bit thin, but the community provides an interesting alternative with a client built like an ORM. Last but not least, the database is here since a long time and the entreprise behind it is very active. So, it makes Neo4j the safer choice of this review.",
            "NB: here is a real life feedback about Neo4j.",
            "ArangoDB is a versatile database that allows document storage and graph storage all along. Recently, it have gained in popularity, it\u2019s the reason why we included it to the test. It comes with handful features like easy deployment on a cloud infrastructure and helpers to build REST API. But for this article we\u2019ll focus on the graph storage and its query system.",
            "Let\u2019s code! To make our testing we need first an Arango instance up and running. Let\u2019s use Docker again to spawn it:",
            "docker run -p 8529:8529 -e ARANGO_ROOT_PASSWORD=openSesame arangodb/arangodb:3.2.1",
            "Then we install the Python client:",
            "pip install python-arango",
            "Now we can write our Python script, the first step will be to initialize our database:",
            "from arango.client import ArangoClient client = ArangoClient(username='root', password='openSesame') db = client.create_database('cgproduction')",
            "As you can see the database creation is very straightforward. The only problem is that it raises an exception if the database already exists. It means that if you want to achieve idempotence with your script, you will have to write your own \u201cget or create\u201d method. It\u2019s the same for every creation we\u2019ll do in the following. Be prepared to augment this Python driver.",
            "The next step is to define our graph and configure the collections that will store vertices and edges information:",
            "dependencies = db.create_graph('dependencies') shots = dependencies.create_vertex_collection('shots') assets = dependencies.create_vertex_collection('assets') casting = dependencies.create_edge_definition( name='casting', from_collections=['assets'], to_collections=['shots'] ) elements = dependencies.create_edge_definition( name='element', from_collections=['assets'], to_collections=['assets'] )",
            "Arango graph storage is based on its own document storage system. Each vertex is stored as a json entry in a collection. Edges are a little bit different. They are stored in a similar fashion, but the collection definition requires more information: the inner vertex collection and the outer one. Edges are always directed.",
            "Now we have our database properly configured, we can add our data:",
            "# Insert vertices assets.insert( {'_key': 'props1-concept', 'name': 'Props 1 Concept'}) assets.insert( {'_key': 'props1-texture', 'name': 'Props 1 Texture'}) assets.insert( {'_key': 'props1-mesh', 'name': 'Props 1 Mesh'}) assets.insert({'_key': 'props1-rig', 'name': 'Props 1 Rig'}) assets.insert({'_key': 'props1-model', 'name': 'Props 1 Model'}) assets.insert({'_key': 'props1-keys', 'name': 'Props 1 Keys'}) shots.insert( {'_key': 'shot1-image-sequence', 'name': 'Shot 1 Image sequence'}) # Insert edges elements.insert( {'_from': 'assets/props1-concept', '_to': 'assets/props1-texture'}) elements.insert( {'_from': 'assets/props1-concept', '_to': 'assets/props1-mesh'}) elements.insert( {'_from': 'assets/props1-texture', '_to': 'assets/props1-model'}) elements.insert( {'_from': 'assets/props1-mesh', '_to': 'assets/props1-rig'}) elements.insert( {'_from': 'assets/props1-mesh', '_to': 'assets/props1-model'}) elements.insert( {'_from': 'assets/props1-mesh', '_to': 'assets/props1-keys'}) elements.insert( {'_from': 'assets/props1-rig', '_to': 'assets/props1-keys'}) casting.insert( {'_from': 'assets/props1-model', '_to': 'shots/shot1-image-sequence'}) casting.insert( {'_from': 'assets/props1-keys', '_to': 'shots/shot1-image-sequence'})",
            "Once our data properly imported, we can proceed to our query:",
            "traversal_results = dependencies.traverse( start_vertex=\u2019assets/props1-mesh\u2019, direction=\u2019outbound\u2019 ) for result in traversal_results[\u201cvertices\u201d]: print(result[\u201cname\u201d])",
            "With this simple request we get all our impact of a modification of the props 1 mesh. The result is easy to analyze and the query is configurable (for instance you can chose between a depth first traversal and a breath first traversal).",
            "Arango provides a traversal object that allows you to build particular path. Some helpers are available too, like shortest path finding or path length retrieval. It should cover most of your needs in term of graph querying.",
            "Last but not least, you can visualize your graph in the Arango web UI:",
            "Overall, the ArangoDB and Python client are simple to understand and well documented. It provides many helpers to play with our graph and the visualization tools makes things even easier. But it looks slower than neo4j. Running 10 000 times our query took 26s. Despite these results, it\u2019s still our favorite database of this test. Arango is very developer-friendly. It is the best choice to experiment quickly with graph databases. And because the company behind looks very active, it seems to be a safe choice for a production usage too.",
            "OrientDB is here for a while now (since 2010). But because of the very bad feedback about it (see comments too), we decided to not cover this database in this article. It\u2019s too risky to use it in a CG production environment.",
            "There are still alternatives. By playing with traditional database, you can have similar features as with graph database. One option is to use Postgres with its recursive joins. It will allow you to cover simple use cases of graph traversal.",
            "Another option, which looks great if you want to be able to do fuzzy searches, is to use Elastic Search and store all vertices and edges as JSON documents (similar approach as ArangoDB). Read this full article to have more information about the subject.",
            "Having graph data is great but you may want to build tools that shows your data at some point (and outside of the built-in UIs).",
            "There are two good libraries for Qt that allows to build graph easily:",
            "ZodiacGraph : a powerful C++ library which is fast and flexible. Nodz : a Python library easy to use.",
            "Another option is to use Javascript libraries for in-browser or Electron applications. Here are some:",
            "SigmaJS : fast and well documented library Cytoscape : versatile and robust. d3.js : harder to use but limitless.",
            "From our study, it looks like ArangoDB is the most user friendly database and its document storage aspect will make your production data management easier. But it\u2019s still a young DB. If you need speed or if there is a lot of money at stake and if you are looking for a safer choice go for Neo4j, which does the job well and looks more robust. Finally Cayley looks good on many aspects has a great design and could be the best choice to complement an already existing relational database, but is still too undocumented and young to be used in production. So, to sum up: try ArangoDB first!",
            "The question about what problems solve graph representation and storage for pipeline TDs remain. The main use case for us is to generate easily the sequence of actions needed to rebuild a shot when a change occurs. The other one is to provide easily a representation of the production on which people can discuss.",
            "We hope you enjoy this article. We are still very new to graph databases. We would be glad to know what you think about it and read your production experience with these technologies: comments are welcome!",
            "This blog is dedicated to CG pipeline and production management. If you are interested in graph databases for CG productions, you will probably enjoy all our articles. Read our first blog post to know more about us!"
        ]
    },
    "https://towardsdatascience.com/conceptualizing-the-knowledge-graph-construction-pipeline-33edb25ab831": {
        "author": "Nayantara Jeyaraj (Taro)",
        "length": "10 min read",
        "title": "Conceptualizing the Knowledge Graph Construction Pipeline",
        "tags": [
            "big-data",
            "knowledge-base",
            "knowledge-graph",
            "psl",
            "machine-learning"
        ],
        "content": [
            "The advent of the internet has granted access to a large number of content creators to generate information. Owing to this, there is a massive amount of data that is now present on the web. In order to provide useful insights, we need an efficient way to represent all this data. One such efficient knowledge representation method is via knowledge graphs. In brief, a knowledge graph is a large network of interconnected data. Knowledge graphs are constructed from knowledge bases. Knowledge bases gather their information from free text on web pages, databases, and audio and video content. The basic pipeline of a knowledge graph\u2019s construction process is shown in Figure 1.",
            "Now, let\u2019s go through the processes that take place within this pipeline in detail.",
            "During the first phase of the pipeline, we identify facts from free text. Initially, we scour the internet to filter useful information by identifying the entities and the relationships that the entities are involved in from free text. This identification process takes place using natural language processing techniques, such as named entity resolution, lemmatization, and stemming. Hence, the data extracted from free text in the first step may resemble the form of the following statement.",
            "\u201cThe Louvre is located in Paris\u201d",
            "Proceeding to the second phase of the pipeline, the statements are generalized in the form of triples within knowledge bases; these triples will be categorized under different ontologies using an ontology extraction process that can harness the capabilities of natural language processing techniques as well. A triple is composed of a subject, the predicate, and its object. The subject and object are entities that are involved in a relationship defined by the predicate. Hence, for the previous statement identified from free text, we break this down in the following form of a triple for the knowledge base.",
            "Subject : Louvre Predicate : is located Object : Paris",
            "So within a knowledge base, we will have the above relationship in the form of islocated(Louvre, Paris). This is a single triple within a knowledge base. In practice, knowledge bases include millions of such triples, which we also term as facts. These facts are grouped under ontologies in knowledge bases. An ontology is an identifying category for a particular domain of facts. Hence, an ontology explains what sort of entities exist within that category. For example, if the ontology is \u2018airport\u2019, then, some of the entities that fall under this category may include \u2018addison airport\u2019, \u2018charles de gaulle airport\u2019, \u2018mandelieu airport\u2019, and so on.",
            "Knowledge bases can either be domain-specific or generic. Medical knowledge bases and academic research paper knowledge bases are some domain-specific knowledge bases. However, generic knowledge bases do not constrain their knowledge to a particular domain. They have a broader coverage of general worldly facts and multiple domains.",
            "Before we move forth to the final phase of the pipeline, which is the knowledge graph, refer to the table below for some characteristics of various knowledge bases as comprehended from their original papers. The table lists knowledge bases that have been of prime importance over the past decades.",
            "Table 1. Knowledge bases and their characteristics",
            "^1 Wikibase API : https://en.wikipedia.org/w/api.php",
            "With regard to knowledge bases, let\u2019s further explicate the NELL knowledge base, as we\u2019ll be considering the way in which NELL handles its facts, as a sample for the knowledge graph construction phase of the pipeline that we\u2019ll be discussing later.",
            "Never-Ending Language Learner (NELL) was a project that was initiated at the Carnegie-Mellon University in 2010 [5]. It was modeled to gap the difference between a learning system and actual human learning. As such, it was based on the concept that continuous learning of facts shapes expertise. NELL has been continuously learning facts since 2010. This knowledge base primarily performs two tasks.",
            "Information extraction: Scouring the semantic web to discover new facts, accumulating those facts and extending its knowledge base continuously. Enhance the learning process: Based on its previous experience in extracting information, NELL tries to improve its learning ability by returning to the page from which it learned its facts the previous day, and searches for newer facts.",
            "NELL\u2019s facts are based on an ontological classification: the entity or the relation. Entity-based ontological classification consists of sub-domains of instances that could occur in that domain, whereas relation-based ontological classification comprises sub-domains of facts based on the relationship that connects the entity instances. The facts in NELL are in the form of triples (subject-object-predicate). For example,",
            "Sample fact : \u201cThe Statue of liberty is located in New York\u201d",
            "As a triple, the above fact can be represented as locatedIn (statueOfLiberty, newYork) where,",
            "Subject: statueOfLiberty Predicate: locatedIn Object: newYork",
            "NELL\u2019s facts are extracted using text context patterns, orthographic classifiers, URL-specified ML patterns, learning embedding, image classifiers and ontology extenders. Currently NELL is constrained as it cannot modify its defined process of learning. If the process of learning can be dynamically enhanced based on previous learning experiences, NELL can improve the quality of its facts and the performance of accruing its facts.",
            "Now, let\u2019s move onto the final phase of the pipeline to see how the triples in knowledge bases are converted into a knowledge graph.",
            "A knowledge graph is a large network of interconnected entities. The connections are created based on the triples from knowledge bases. The main intent of the knowledge graph is to identify the missing links between entities. In order to explicate this further, let\u2019s consider the following sample relationships that we have gathered from the knowledge base.",
            "Friends (Anne, Jane) Friends (Jane, Jim) LivesIn (Anne, Paris) LivesIn (Jim, Brazil) LivesIn (Jane, Brazil) BornIn (Anne, Paris) BornIn (Jim, Paris)",
            "If we try to build a basic knowledge graph based on only the above relationships, we will be able to visualize the following graph.",
            "On the other hand, there are some unknown relationships that were not explicitly retrieved from the knowledge bases, such as,",
            "Are Anne and Jim friends? What is Jane\u2019s birthplace?",
            "This means that such relationships can be considered as missing links.",
            "These missing links are inferred using statistical relational learning (SRL) frameworks. These SRL frameworks compute the relational confidence of an inferred/predicted link. There are different ways in which previous works have attempted to discover new/missing information as well as compute the confidence in inferencing those information. These are discussed in brief in the following paragraphs.",
            "In the first phase of the pipeline, where we extract facts from free text, we often end up with erroneous facts as well. In order to identify a stable knowledge graph from these facts, Cohen et al. proposed a methodology to jointly evaluate the extracted facts [6]. The issue with this method was its consideration of only a trivial set of possible errors that could occur in extracted facts.",
            "As the second phase of the pipeline, we find triples from extracted facts and these triples will make up the knowledge base. Proceeding this, during the final phase, we need to discover new facts by inferring missing links from the knowledge base triples. For this purpose, following Cohen, Jiang et al. resorted to Markov Logic Networks to discover relationships between extracted facts [7]. They defined ontological constraints that are specified in the form of first order logic rules. These constraints would administer the possible relationships that can be inferred. However, with the Markov Logic Network, the logical relationships, that we term as \u2018predicates\u2019, could only take a boolean value for its variables. This posed a disadvantage in inferring a confidence for the facts.",
            "This led to the definition of the Probabilistic Soft Logic (PSL), which uses the concepts of Jiang et al and the Markov Logic Network, and defines a sophisticated statistical relational framework that jointly reasons over all the facts, to discover new/missing information based on the previous facts [8]. In addition to that, PSL probabilistically computes a confidence value, which is a soft truth value within the range of [0,1], inclusive, to indicate how far the PSL program believes that the fact is true, based on what\u2019s been provided.",
            "Once the new/missing information are discovered, and their confidences are calculated, we can build a knowledge graph with highly confident facts. This will provide us a graph where new information that cannot be explicitly driven, are available, in addition to the original facts that were extracted. And this is how we build a knowledge graph with the facts from knowledge bases and the newly discovered facts based on the available observations.",
            "Finally, as we summarize these cascaded steps of the knowledge graph pipeline, on a higher level, the following are the processes that take place in building a knowledge graph [9].",
            "Phase 1: Extracting facts from Free Text",
            "Data is extracted from free text, unstructured data sources and semi structured data sources. This raw data is processed in order to extract information. This involves the extraction of entities, relations, and attributes, which are the properties that further define entities and relations. If data is already structured, unlike in step 1, that data will directly proceed forth to be fused with information from third-party knowledge bases. Following this, various natural language processing techniques will be applied on top of the fused knowledge and the processed data. This includes the co-reference resolution, named entity resolution, entity disambiguation, and so on.",
            "Phase 2: Formulating triples from extracted facts",
            "The above steps conclude the pre-processing of information for knowledge bases. Then, an ontology extraction process is carried out to categorize the extracted entities and the relations under their respective ontologies. Proceeding the ontology formalization, the facts will be refined and stored as triples in the knowledge base.",
            "Phase 3: Constructing the knowledge graph with new links and confidences",
            "In order to construct the knowledge graph from the knowledge base, statistical relational learning (SRL) will be applied on these triples. The SRL process computes a confidence for each fact as opposed to the entire domain in order to identify how far those facts would hold true. In constructing the knowledge graph, missing links will be identified using the confidence and the newly inferred relational links will be formed.",
            "Since the confidences in the inference are incorporated in the knowledge graph, once the graph has been constructed, the decision on how far the facts will be considered to be true can be based on the confidences as well. As such, a sample knowledge graph of a movie actors\u2019 domain, generated by Cayley [10], is shown below.",
            "Subsequently, such knowledge graphs can be used in information retrieval systems, chatbots, web applications, knowledge management systems, etc., to efficiently provide responses to user queries.",
            "Thus far, we\u2019ve provided an abstract explanation of how the entire knowledge graph pipeline works. Using the techniques specified in these phases will guarantee the discovery of missing links. Nevertheless, an open-unknown that still floats around in the knowledge graph community is the identification of erroneous facts or triples according to human perspectives. Currently, we have methods that compute the confidence of existing and discovered relationships based on the domain and the set of facts. However, this does not provide a sure-footed way to say if the fact will be evaluated as a valid fact by an actual human evaluator. Hence, in our following post, we\u2019ll look further into a detailed elucidation of how we infer missing links using a statistical relational frameworks such as the probabilistic soft logic and how a sufficient level of supervision can be correlated into the model to align the facts with the crowd-sourced truths in the knowledge graph.",
            "[1] Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., & Ives, Z. (2007). Dbpedia: A nucleus for a web of open data. In The semantic web (pp. 722\u2013735). Springer, Berlin, Heidelberg.",
            "[2] Lenat, D. B., & Guha, R. V. (1991). The evolution of CycL, the Cyc representation language. ACM SIGART Bulletin, 2(3), 84\u201387.",
            "[3] Bollacker, K., Evans, C., Paritosh, P., Sturge, T., & Taylor, J. (2008, June). Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data (pp. 1247\u20131250). AcM.",
            "[4] Vrande\u010di\u0107, D., & Kr\u00f6tzsch, M. (2014). Wikidata: a free collaborative knowledgebase. Communications of the ACM, 57(10), 78\u201385.",
            "[5] Betteridge, J., Carlson, A., Hong, S. A., Hruschka Jr, E. R., Law, E. L., Mitchell, T. M., & Wang, S. H. (2009). Toward Never Ending Language Learning. In AAAI spring symposium: Learning by reading and learning to read (pp. 1\u20132).",
            "[6] Cohen, W. W., Kautz, H., & McAllester, D. (2000, August). Hardening soft information sources. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 255\u2013259). ACM.",
            "[7] Jiang, S., Lowd, D., & Dou, D. (2012, December). Learning to refine an automatically extracted knowledge base using markov logic. In Data Mining (ICDM), 2012 IEEE 12th International Conference on (pp. 912\u2013917). IEEE.",
            "[8] Brocheler, M., Mihalkova, L., & Getoor, L. (2012). Probabilistic similarity logic. arXiv preprint arXiv:1203.3469.",
            "[9] \u5218\u5ce4, \u674e\u6768, \u6bb5\u5b8f, \u5218\u7476, & \u79e6\u5fd7\u5149. (2016). \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6280\u672f\u7efc\u8ff0. \u8ba1\u7b97\u673a\u7814\u7a76\u4e0e\u53d1\u5c55, 53(3), 582\u2013600.",
            "[10] Open source graph database : https://cayley.io/",
            "Image credits: https://www.iconfinder.com/"
        ]
    },
    "https://medium.com/swlh/create-rest-api-with-django-and-neo4j-database-using-django-nemodel-1290da717df9": {
        "author": "SIHEM BOUHENNICHE",
        "length": "8 min read",
        "title": "Create REST API With Django and Neo4j Database Using Django_nemodel",
        "tags": [
            "neo4j",
            "django",
            "rest-api",
            "graph-database",
            "python"
        ],
        "content": [
            "In this tutorial, I\u2019ll show you how to create a REST API using Django and Neo4j database through a simple example. In order to create this API, we need to use an Object Graph Mapper (OGM) to request the graph database, that\u2019s why we will use django_neomodel which is a Django integration of the awesome OGM neomodel.",
            "Django : Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Neo4j : Neo4j is a NoSQL graph database management system (DBMS). Django_neomodel : a module that allows you to use the neo4j graph database with Django using neomodel. Create and manage some objects using Django shell. Create views and urls patterns. Request our API.",
            "Installing dependencies. Create and set up the project. Create the graph structure that Django_neomodel ORM will manage.",
            "Before jumping into the installation, you should have already downloaded python. You can check the actual version for python by this command:",
            "$ python --version",
            "The first step is to create a virtual environment that will will englobe all the requirements. So let\u2019s start by installing virtualenv and activate the virtual environment:",
            "$ pip install virtualenv $ virtualenv env $ env\\Scripts\\activate",
            "Then we install django and django_neomodel:",
            "$ pip install Django $ pip install django_neomodel",
            "Finally, we install Neo4j. For that i recommend you to use the official documentation of Neo4j. You can find it here.",
            "We are ready now to start creating our django API.",
            "First of all, we start by creating a neo4j project named \u201cTutorial\u201d. Then we create a graph database inside it. (look at the figure below)",
            "Now, it\u2019s time to create django project. For that, we use this command:",
            "$ django-admin startproject myproject",
            "Then, according to the django project structure we must create an application inside this project:",
            "$ cd myproject $ python manage.py startapp myapi",
            "After that, we need to register our application in myproject/settings.py file so Django can recognize this new application. We must also register django_neomodel and set up the connexion to our neo4j database.",
            "# Application definition INSTALLED_APPS = [ # django.contrib.auth etc 'myapi.apps.MyapiConfig', 'django_neomodel' ] # Database # https://docs.djangoproject.com/en/3.0/ref/settings/#databases NEOMODEL_NEO4J_BOLT_URL = os.environ.get('NEO4J_BOLT_URL','bolt://username:password@localhost:7687') # you are free to add this configurations NEOMODEL_SIGNALS = True NEOMODEL_FORCE_TIMEZONE = False NEOMODEL_ENCRYPTED_CONNECTION = True NEOMODEL_MAX_POOL_SIZE = 50",
            "Our example is very simple. We will have two entities Person and City and two relationships between them(LivesIn and Friend). see the structure below:",
            "Let\u2019s define our graph structure inside myapi/models.py file like that :",
            "from neomodel import StructuredNode, StringProperty, IntegerProperty,UniqueIdProperty, RelationshipTo # Create your models here. class City(StructuredNode): code = StringProperty(unique_index=True, required=True) name = StringProperty(index=True, default=\"city\") class Person(StructuredNode): uid = UniqueIdProperty() name = StringProperty(unique_index=True) age = IntegerProperty(index=True, default=0) # Relations : city = RelationshipTo(City, 'LIVES_IN') friends = RelationshipTo('Person','FRIEND')",
            "You can find more details about how to define node entities and relationships in the neomodel official documentation.",
            "After that, we must create constraints and indexes for our labels to ensure that everything is right by typing this command :",
            "$ python manage.py install_labels",
            "You can ensure that constraints and were applied in the Neo4j browser like this :",
            "Now, let\u2019s ensure that everything is all right by populating our database by some persons and cities. For that, we will use simply Django shell:",
            "$ python manage.py shell Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. (InteractiveConsole) >> from myapi.models import * >> all_persons = Person.nodes.all() [] >> all_cities = City.nodes.all() [] >> algiers = City(code=\"ALG\",name=\"Algiers\") >> print(algiers) {code: \"ALG\", name: \"Algiers\"} >> algiers.save() <City: {code: \"ALG\", name: \"Algiers\", id: 0}> >> sihem = Person(name='Sihem', age=24) >> print(sihem) {'uid': '56329dc', 'name': 'Sihem', 'age': 24} >> sihem.save() <Person: {'uid': '56329dc', 'name': 'Sihem', 'age': 24, 'id': 1}> >> sihem.city.connect(algiers) True >> if sihem.city.is_connected(algiers): ...     print(\"sihem lives in Algeirs\") ... sihem lives in Algeirs >> oussama= Person(name='Oussama', age=22) >> oussama.save() <Person: {'uid': 'c139f04', 'name': 'Oussama', 'age': 22, 'id': 21}> >> oussama.city.connect(algiers) True >> oussama.friends.connect(sihem) True",
            "After that, you can check our objects in Neo4j database by using Cypher queries.",
            "You can create more objects using either Neo4j browser or Django shell.",
            "Arriving at this stage, it\u2019s time to create our person and city views. So let\u2019s delete myapi/views file and use instead of it a new package named myapi/views. We wil have the following application structure:",
            "myapi |_migrations |_models | |_ __init__.py | |_person.py | |_city.py |_views | |_ __init__.py | |_ person.py | |_ city.py | |_ connectors.py |_ ......",
            "Now, lets define the content of each file of mayapi/views package :",
            "person.py : this file contains the following methods :",
            "personDetails : in which we will put our CRUD operations (create new person, update, delete and get details of a person). getAllPersons : which will render a list of existing Person instances.",
            "from django.http import JsonResponse from myapi.models import Person from django.views.decorators.csrf import csrf_exempt import json def getAllPersons(request): if request.method == 'GET': try: persons = Person.nodes.all() response = [] for person in persons : obj = { \"uid\": person.uid, \"name\": person.name, \"age\": person.age, } response.append(obj) return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) @csrf_exempt def personDetails(request): if request.method == 'GET': # get one person by name name = request.GET.get('name', ' ') try: person = Person.nodes.get(name=name) response = { \"uid\": person.uid, \"name\": person.name, \"age\": person.age, } return JsonResponse(response, safe=False) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'POST': # create one person json_data = json.loads(request.body) name = json_data['name'] age = int(json_data['age']) try: person = Person(name=name, age=age) person.save() response = { \"uid\": person.uid, } return JsonResponse(response) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'PUT': # update one person json_data = json.loads(request.body) name = json_data['name'] age = int(json_data['age']) uid = json_data['uid'] try: person = Person.nodes.get(uid=uid) person.name = name person.age = age person.save() response = { \"uid\": person.uid, \"name\": person.name, \"age\": person.age, } return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'DELETE': # delete one person json_data = json.loads(request.body) uid = json_data['uid'] try: person = Person.nodes.get(uid=uid) person.delete() response = {\"success\": \"Person deleted\"} return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False)",
            "city.py : this file contains the following methods :",
            "cityDetails : in which we will put our CRUD operations (create new city, update, delete and get details of a city). getAllCities : which will render a list of existing City instances.",
            "from django.http import JsonResponse from myapi.models import City from django.views.decorators.csrf import csrf_exempt import json def getAllCities(request): if request.method == 'GET': try: cities = City.nodes.all() response = [] for city in cities: obj = { \"code\": city.code, \"name\": city.name, } response.append(obj) return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) @csrf_exempt def cityDetails(request): if request.method == 'GET': # get one city by name name = request.GET.get('name', ' ') try: city = City.nodes.get(name=name) response = { \"code\": city.code, \"name\": city.name, } return JsonResponse(response, safe=False) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'POST': # create one city json_data = json.loads(request.body) name = json_data['name'] code = json_data['code'] try: city = City(name=name, code=code) city.save() response = { \"code\": city.code, } return JsonResponse(response) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'PUT': # update one city json_data = json.loads(request.body) name = json_data['name'] code = json_data['code'] try: city = City.nodes.get(code=code) city.name = name city.save() response = { \"code\": city.code, \"name\": city.name, } return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'DELETE': # delete one city json_data = json.loads(request.body) code = json_data['code'] try: city = City.nodes.get(code=code) city.delete() response = {\"success\": \"City deleted\"} return JsonResponse(response) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False)",
            "connectors.py : this file contains the following methods :",
            "connectPaC : which will create a connexion between a person and a city. connectPaP: which will create a connexion between two persons.",
            "from django.http import JsonResponse from myapi.models import * from django.views.decorators.csrf import csrf_exempt import json @csrf_exempt def connectPaC(request): if request.method == 'PUT': json_data = json.loads(request.body) uid = json_data['uid'] code = json_data['code'] try: person = Person.nodes.get(uid=uid) city = City.nodes.get(code=code) res = person.city.connect(city) response = {\"result\": res} return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) @csrf_exempt def connectPaP(request): if request.method == 'PUT': json_data = json.loads(request.body) uid1 = json_data['uid1'] uid2 = json_data['uid2'] try: person1 = Person.nodes.get(uid=uid1) person2 = Person.nodes.get(uid=uid2) res = person1.friends.connect(person2) response = {\"result\": res} return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False)",
            "__init__.py : this file makes Python treat myapi/views directory as one module.",
            "from myapi.views.person import * from myapi.views.city import * from myapi.views.connectors import *",
            "After that, we create new file inside myapi/ directory called urls in which we will register our list routes URLs to our views.",
            "from django.conf.urls import url from myapi.views import * from django.urls import path urlpatterns = [ path('person',personDetails), path('getAllPersons',getAllPersons), path('city',cityDetails), path('getAllCities',getAllCities), path('connectPaC',connectPaC), path('connectPaP',connectPaP) ]",
            "Then, we must register this list routes URLs inside myproject/urls.py file like that :",
            "from django.contrib import admin from django.urls import path from django.conf.urls import include urlpatterns = [ path('admin/', admin.site.urls), path('', include('myapi.urls')), ]",
            "Finally, we will request our API to ensure that everything works correctly. so let\u2019s start our API by runing this command :",
            "$ python manage.py runserver Watching for file changes with StatReloader Performing system checks... System check identified no issues (0 silenced). July 29, 2020 - 16:48:13 Django version 3.0.8, using settings 'myproject.settings' Starting development server at http://127.0.0.1:8000/",
            "To request the API, i used Postman which simplify making CRUD requests.",
            "At the end, we can illustrate the architeture of our API like that :",
            "Thank you for reading, if you have any questions or remarks please do not hesitate to leave a comment below",
            "Here is the repository of the whole project in this github link :",
            "github.com"
        ]
    },
    "https://medium.com/octavian-ai/deep-learning-with-knowledge-graphs-3df0b469a61a": {
        "author": "Andrew Jefferson",
        "length": "8 min read",
        "title": "Deep Learning with Knowledge Graphs",
        "tags": [
            "machine-learning",
            "deep-learning",
            "data-science",
            "graph-database",
            "knowledge-graph"
        ],
        "content": [
            "Last week I gave a talk at Connected Data London on the approach that we have developed at Octavian to use neural networks to perform tasks on knowledge graphs.",
            "Here\u2019s the recording of the talk, from Connected Data London:",
            "In this post I will summarize that talk (including most of the slides) and provide links to the papers that have most significantly influenced us.",
            "To find out more about our vision for a new approach to building the next generation of database query engine see our recent article.",
            "We are using a property graph or attributed graph model. Nodes (vertices) and relations (edges) can have properties. In addition our Neural Network has a global state that is external to the graph. The slide shows two representations of this model one from Neo4j and the other from DeepMind (n.b. these are effectively identical).",
            "Graphs have a rich history, starting with Leohnard Euler in the 18th century to a whole range of graphs today. Within the field of computer science there are many applications of graphs: graph databases, knowledge graphs, semantic graphs, computation graphs, social networks, transport graphs and many more.",
            "Graphs have played a key role in the rise of Google (their first breakthrough was using PageRank to power searches, today their Knowledge Graph has grown in importance) and Facebook. From politics to low cost international air travel, graph algorithms have had a major impact on our world.",
            "Deep learning is a branch of machine learning centered around training multi layer (\u201cdeep\u201d) neural networks using gradient descent. The basic building block of these neural networks is the dense (or fully connected) network.",
            "Using deep learning has allowed us to train computers to tackle a range of previously challenging tasks from playing Go to image recognition with superhuman performance.",
            "In general machine learning is a simple concept. We create a model of how we think things work e.g. y = mx + c this could be:",
            "house_price = m \u2022 number_of_bedrooms + c",
            "We train (fit) the parameters of our model (m and c in the example) using the data that we have. Once our training is done we have some learned parameter values and we have a model that we can use to make predictions.",
            "Sometimes the parameters are useful by themselves (e.g. when we use a neural network to train a word embedding such as word2vec).",
            "At Octavian one of the questions we asked ourselves is: how would we like machine learning on graphs to look from 20,000ft?",
            "To help answer this question, we compared traditional forms of deep learning to the world of graph learning:",
            "We identified three graph-data tasks which we believe require graph-native implementations: Regression, Classification and Embedding.",
            "Aside: there are other graph-specific tasks such as link prediction that don\u2019t easily fit into the three tasks above.",
            "We observed that many existing techniques for machine learning on graphs have some fundamental limitations:",
            "Some do not work on unseen graphs (because they require first training a graph embedding) Some require converting the graph into a table and discarding its structure (e.g. sampling from a graph using random walks)",
            "Much of the existing work using Deep Learning on graphs focuses on two areas.",
            "Making predictions about molecules (including proteins), their properties and reactions. Node classification/categorisation in large, static graphs.",
            "It\u2019s often said that Deep Learning works well with unstructured data \u2014 images, free text, reinforcement learning etc.",
            "But our superhuman neural networks are actually dealing with very specifically structured information and the neural network architectures are engineered to match the structure of the information they work well with.",
            "Images are in a sense structured: they have a rigid 2D (or 3D) structure where pixels that are close to each-other are more relevant to each-other than pixels that are far apart. Sequences (e.g. over time) have a 1D structure where items that are adjacent are more relevance to one another than items that are far apart.",
            "When working with images and sequences, dense layers (e.g. where every input is connected to every output) doesn\u2019t work well. Neural network layers that reflect and exploit the structure of the input medium achieve the best results.",
            "For sequences Recurrent Neural Networks (RNNs) are used and for images Convolutional Neural Networks (CNNs) are used.",
            "In a convolutional neural network each pixel in the hidden layer only depends on a group of nearby pixels in the input (compare this to a dense layer where every hidden layer pixel depends on every input pixel).",
            "Nodes in graphs do not have fixed relations like nearby pixels in an image or adjacent items in a sequence. To make deep learning successful with graphs it\u2019s not enough to convert graphs to matrix representation and put that input into existing Neural Network models. We have to figure out how to create Neural Network models that work well for graphs.",
            "We aren\u2019t the only people thinking about this. Some very clever people at DeepMind, Google Brain, MIT and University of Edinburgh lay out a similar position in their paper on Relational Inductive Biases. I recommend this paper to anyone interested in deep learning on graphs.",
            "The paper introduces a general algorithm for propagating information through a graph and argues that by using neural networks to learn six functions to perform aggregations and transforms within the structure of the graph they can achieve state of the art performance on a selection of graph tasks.",
            "By propagating information between nodes principally using the graph edges the authors argue they are maintaining the relational inductive biases present in the graph structure.",
            "The MacGraph neural network architecture that we have been developing at Octavian has similarities to the relational inductive biases approach. It employes a global state that exists outside the graph and also propagates information between the graph nodes",
            "Before I can tell you about our results at Octavian I have to mention the task that we used to test our neural graph architecture.",
            "You can read more about CLEVR-Graph here. It\u2019s a synthetic (procedurally generated) dataset which consists of 10,000 fictional transit networks loosely modelled on the London underground. For each randomly generated transit network graph we have a single question and correct answer.",
            "The crucial thing about this task is that each graph used to test the network is one the network has never seen before. Therefore it cannot memorise the answers to the questions but must learn how to extract the answer from new graphs.",
            "At time of writing MacGraph is achieving almost-perfect results on tasks requiring 6 different skills:",
            "I think that one of the most exciting skills is MacGraph\u2019s ability to answer \u201cHow many stations are between{station} and {station}\u201d because to solve that question it\u2019s necessary to determine the shortest path between the stations (Dijkstra\u2019s algorithm) which is a complex and graph-specific algorithm.",
            "It\u2019s not sufficient to just propagate information between nodes in the graph using transformation and aggregation functions. To answer natural language questions about a graph with natural language answers it\u2019s necessary to transform the input question into a graph state that results in the correct answer being reached and it\u2019s necessary to extract the answer information from the graph state and transform it into the desired answer.",
            "Our solution for transforming between natural language and graph state is to use attention. You can read more about how this works here.",
            "Attention cells are radically different to dense layers. Attention cells work with lists of information, extracting individual elements depending on their content or location.",
            "These properties make attention cells great for selecting from the lists of nodes and edges that make up a graph.",
            "In MacGraph write attention is used to input a signal to the nodes in the graph based on the query and the properties of the nodes. This signal should then prime the graph message passing to interact with the nodes most relevant to the question.",
            "After information has propagated through the graph\u2019s nodes, attention is used to extract the answer from the graph:",
            "Combining write and read attention with propagation between nodes in the graph using the graph structure we are get the core of MacGraph.",
            "There is a strong case that achieving superhuman results on graph-based tasks requires graph-specific neural network architectures.",
            "We have shown with MacGraph that a neural network can learn to extract properties from nodes within a graph in response to questions and that a neural network can learn to perform graph algorithms (such as finding the shortest path) on graphs that it has never encountered before."
        ]
    },
    "https://towardsdatascience.com/from-text-to-knowledge-the-information-extraction-pipeline-b65e7e30273e": {
        "author": "Tomaz Bratanic",
        "length": "11 min read",
        "title": "From Text to Knowledge: The Information Extraction Pipeline",
        "tags": [
            "neo4j",
            "nlp",
            "graph",
            "spacy",
            "information-extraction"
        ],
        "content": [
            "I am thrilled to present my latest project I have been working on. If you have been following my posts, you know that I am passionate about combining natural language processing and knowledge graphs. In this blog post, I will present my implementation of an information extraction data pipeline. Later on, I will also explain why I see the combination of NLP and graphs as one of the paths to explainable AI.",
            "What exactly is an information extraction pipeline? To put it in simple terms, information extraction is the task of extracting structured information from unstructured data such as text.",
            "My implementation of the information extraction pipeline consists of four parts. In the first step, we run the input text through a coreference resolution model. The coreference resolution is the task of finding all expressions that refer to a specific entity. To put it simply, it links all the pronouns to the referred entity. Once that step is finished, it splits the text into sentences and removes the punctuations. I have noticed that the specific ML model used for named entity linking works better when we first remove the punctuations. In the named entity linking part of the pipeline, we try to extract all the mentioned entities and connect them to a target knowledge base. The target knowledge base, in this case, is Wikipedia. Named entity linking is beneficial because it also deals with entity disambiguation, which can be a big problem.",
            "Once we have extracted the mentioned entities, the IE pipeline tries to infer relationships between entities that make sense based on the text\u2019s context. The IE pipeline results are entities and their relationships, so it makes sense to use a graph database to store the output. I will show how to save the IE information to Neo4j.",
            "I\u2019ll use the following excerpt from Wikipedia to walk you through the IE pipeline.",
            "Elon Musk is a business magnate, industrial designer, and engineer. He is the founder, CEO, CTO, and chief designer of SpaceX. He is also early investor, CEO, and product architect of Tesla, Inc. He is also the founder of The Boring Company and the co-founder of Neuralink. A centibillionaire, Musk became the richest person in the world in January 2021, with an estimated net worth of $185 billion at the time, surpassing Jeff Bezos. Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. He briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. He transferred to the University of Pennsylvania two years later, where he received dual bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University, but decided instead to pursue a business career. He went on co-founding a web software company Zip2 with his brother Kimbal Musk.",
            "Text is copied from https://en.wikipedia.org/wiki/Elon_Musk and is available under CC BY-SA 3.0 license.",
            "As mentioned, the coreference resolution tries to find all expressions in the text that refer to a specific entity. In my implementation, I have used the Neuralcoref model from Huggingface that runs on top of the SpaCy framework. I have used the default parameters of the Neuralcoref model. One thing I did notice along the way is that the Neuralcoref model doesn\u2019t work well with location pronouns. I have also borrowed a small improvement code from one of the GitHub issues. The code for the coreference resolution part is the following:",
            "If we run our example text through the coref_resolution function, we\u2019ll get the following output:",
            "Elon Musk is a business magnate, industrial designer, and engineer. Elon Musk is the founder, CEO, CTO, and chief designer of SpaceX. Elon Musk is also early investor, CEO, and product architect of Tesla, Inc. Elon Musk is also the founder of The Boring Company and the co-founder of Neuralink. A centibillionaire, Musk became the richest person in the world in January 2021, with an estimated net worth of $185 billion at the time, surpassing Jeff Bezos. Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. Elon Musk briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. Elon Musk transferred to the University of Pennsylvania two years later, where Elon Musk received dual bachelor's degrees in economics and physics. Elon Musk moved to California in 1995 to attend Stanford University, but decided instead to pursue a business career. Elon Musk went on co-founding a web software company Zip2 with Elon Musk brother Kimbal Musk.",
            "In this example, there are no advanced coreference resolution techniques required. The Neuralcoref model changed a couple of pronouns \u201cHe\u201d to \u201cElon Musk\u201d. While it might seem very simple, this is an important step that will increase the overall efficiency of our IE pipeline.",
            "Just recently, I have published a blog post using Named Entity Linking to construct a knowledge graph. Here, I wanted to use a different named entity linking model. I first tried to use the Facebook BLINK model, but I quickly realized it wouldn\u2019t work on my laptop. It needs at least 50GB of free space, which is not a big problem per se, but it also requires 32GB of RAM. My laptop has only 16GB of RAM, and we still need other parts of the pipeline to work. So I reverted to use the good old Wikifier API, which has already shown to be useful. And it\u2019s totally free. If you want to find more information about the API, look at my previous blog post or the official documentation.",
            "Before we run our input text through the Wikifier API, we will split the text into sentences and remove the punctuations. Overall, the code for this step is as follows:",
            "I forgot to mention that the Wikifier API returns all the classes that an entity belongs to. It looks at the INSTANCE_OF and SUBCLASS_OF classes and traverses all the way through the class hierarchy. I decided to filter out entities with categories that would belong to a person, organization, or location. If we run our example text through the Named Entity Linking part of the pipeline, we will get the following output.",
            "A nice thing about the wikification process is that we also get the corresponding WikiData ids for entities along with their titles. Having the WikiData ids takes care of the entity disambiguation problem. You might wonder then what happens if an entity does not exist on Wikipedia. In that case, unfortunately, the Wikifier will not recognize it. I wouldn\u2019t worry too much about it, though, as Wikipedia has more than 100 million entities if I recall correctly.",
            "If you look closely at the results, you\u2019ll notice that Pretoria is wrongly classified as an Organization. I tried to solve this issue, but the Wikipedia class hierarchy is complicated and usually spans five or six hops. If there are some Wiki class experts out there, I will happily listen to your advice.",
            "I have already presented all of the concepts until this point. I have never delved into relationship extraction before. So far, we have only played around with co-occurrence networks. So, I am excited to present a working relationship extraction process. I spend a lot of time searching for any open-source models that might do a decent job. I was delighted to stumble upon the OpenNRE project. It features five open-source relationship extraction models that were trained on either the Wiki80 or Tacred dataset. Because I am such a big fan of everything Wiki, I decided to use the Wiki80 dataset. Models trained on the Wiki80 dataset can infer 80 relationship types. I haven\u2019t tried the models trained on the Tacred dataset. You might try that on your own. In the IE pipeline implementation, I have used the wiki80_bert_softmax model. As the name implies, it uses the BERT encoder under the hood. One thing is sure. If you don\u2019t have a GPU, you are not going to have a good time.",
            "If we look at an example relationship extraction call in the OpenNRE library, we\u2019ll notice that it only infers relationships and doesn\u2019t try to extract named entities. We have to provide a pair of entities with the h and t parameters and then the model tries to infer a relationship.",
            "model.infer({'text': 'He was the son of M\u00e1el D\u00fain mac M\u00e1ele Fithrich, and grandson of the high king \u00c1ed Uaridnach (died 612).', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}}) ('father', 0.5108704566955566)",
            "The results output a relationship type as well as the confidence level of the prediction. My not so spotless code for relationship extraction looks like this:",
            "We have to use the results of the named entity linking as an input to the relationship extraction process. We iterate over every permutation of a pair of entities and try to infer a relationship. As you can see by the code, we also have a relation_threshold parameter to omit relationships with a small confidence level. You will later see why we use permutations and not combinations of entities.",
            "So, if we run our example text through the relationship extraction pipeline, the results are the following:",
            "Relationship extraction is a challenging problem to tackle, so don\u2019t expect perfect results. I must say that this IE pipeline works as well, if not better than some of the commercial solutions out there. And obviously, other commercial solutions are way better.",
            "As we are dealing with entities and their relationships, it only makes sense to store the results in a graph database. I used Neo4j in my example.",
            "Remember, I said that we would try to infer a relationship between all permutations of pairs of entities instead of combinations. Looking at table results, it would be harder to spot why. In a graph visualization, it is easy to observe that while most of the relationships are inferred in both directions, that is not true in all cases. For example, the work location relationship between Elon Musk and the University of Pennsylvania is assumed in a single direction only. That brings us to another shortcoming of the OpenNRE model. The direction of the relationship isn\u2019t as precise as we would like it to be.",
            "To not leave you empty-handed, I will show you how you can use my IE implementation in your projects. We will run the IE pipeline through the BBC News Dataset found on Kaggle. The hardest part about the IE pipeline implementation was to set up all the dependencies. I want you to retain your mental sanity, so I built a docker image that you can use. Run the following command to get it up and running:",
            "docker run -p 5000:5000 tomasonjo/trinityie",
            "On the first run, the OpenNRE models have to be downloaded, so definitely don\u2019t use -rm option. If you want to do some changes to the project and built your own version, I have also prepared a GitHub repository.",
            "As we will be storing the results into Neo4j, you will also have to download and set it up. In the above example, I have used a simple graph schema, where nodes represent entities and relationships represent, well, relationships. Now we will refactor our graph schema a bit. We want to store entities and relationships in the graph but also save the original text. Having an audit trail is very useful in real-world scenarios as we already know that the IE pipeline is not perfect.",
            "It might be a bit counter-intuitive to refactor a relationship into an intermediate node. The problem we are facing is that we can\u2019t have a relationship pointing to another relationship. Given this issue, I have decided to refactor a relationship into an intermediate node. I could have used my imagination to produce better relationship types and node labels, but it is what it is. I only wanted for the relationship direction to retain its function.",
            "The code to import 500 articles in the BBC news dataset to Neo4j is the following. You\u2019ll have to have the trinityIE docker running for the IE pipeline to work.",
            "The code is also available in the form of a Jupyter Notebook on GitHub. Depending on your GPU capabilities, the IE pipeline might take some time. Let\u2019s now inspect the output. Obviously, I chose results that make sense. Run the following query:",
            "MATCH p=(e:Entity{name:'Enrico Bondi'})-[:RELATION]->(r)-[:RELATION]->(), (r)<-[:MENTIONS_REL]-(s) RETURN *",
            "Results",
            "We can observe that Enrico Bondi is an Italian citizen. He held a position at Italy\u2019s Chamber of Deputies. Another relationship was inferred that he also owns Parmalat. After a short Google search, it seems that this data is more or less at least in the realms of possible.",
            "You might wonder, what has this got to do with explainable AI. I\u2019ll give you a real-world example. This research paper is titled Drug Repurposing for COVID-19 via Knowledge Graph Completion. I\u2019m not a doctor, so don\u2019t expect a detailed presentation, but I can give a high-level overview. There are a lot of medical research papers available online. There are also online medical entities databases such as MeSH or Ensembl. Suppose you run a Named Entity Linking model on biomedical research papers and use one of the online medical databases as a target knowledge base. In that case, you can extract mentioned entities in articles. The more challenging part is the relationship extraction. Because this is such an important field, great minds have come together and extracted those relationships.",
            "Probably there are more projects, but I am aware of the SemMedDB project, which was also used in the mentioned article. Now that you have your knowledge graph, you can try to predict new purposes for existing drugs. In network science, this is referred to as link prediction. When you are trying to predict links as well as their relationship types, then the scientific community calls it knowledge graph completion. Imagine we have predicted some new use cases for existing drugs and show our results to a doctor or a pharmacologist. His response would probably be, that\u2019s nice, but what makes you think this new use case will work? The machine learning models are a black box, so that\u2019s not really helpful. But what you can give to the doctor is all the connections between the existing drug and the new disease it could treat. And not only direct relationships, but also those that are two or three hops away. I\u2019ll make up an example, so it might not make sense to a biomedical researcher. Suppose the existing drug inhibits a gene that is correlated to the disease. There might be many direct or indirect connections between the drug and the disease that might make sense. Hence, we have embarked on a step towards an explainable AI.",
            "I am really delighted with how this project worked out. I\u2019ve been tinkering with combining NLP and Knowledge graphs for the last year or so, and now I have poured all of my knowledge into a single post. I hope you enjoyed it!",
            "p.s. If you want to make some changes to the IE pipeline, the code is available as a Github repository. The code for reproducing this blog post is also available as a Jupyter Notebook."
        ]
    },
    "https://towardsdatascience.com/neo4j-cypher-python-7a919a372be7": {
        "author": "Shuyi Yang",
        "length": "6 min read",
        "title": "How to query Neo4j from Python",
        "tags": [
            "neo4j",
            "graph-database",
            "python",
            "cypher"
        ],
        "content": [
            "Neo4j is a graph database management system.",
            "According to its website:",
            "Neo4j is a native graph database, built from the ground up to leverage not only data but also data relationships. Neo4j connects data as it\u2019s stored, enabling queries never before imagined, at speeds never thought possible.",
            "In this article, we will provide a brief tutorial on using Neo4j from Python.",
            "You can skip this section if an instance of the graph DBMS is already installed and configured. In order to install Neo4j Desktop, follow the step-by-step guide on the official website. :) Once correctly installed, you should have a browser window like in the picture below.",
            "You could have some predefined sample projects already created. For this tutorial, we are going to create and use a brand new project. Just click on the New button and choose a name for our project!",
            "Initially, the project is empty. We can start by adding a new database.",
            "Once created the database, start it and open it. You should see a window like in the picture below where there is the information to use to connect to the database from a client. In our case, the URI is",
            "bolt://localhost:7687",
            "From the left vertical bar, you can enter the users administration men\u00f9 to create a new user.",
            "In this example, we are going to create an admin user.",
            "Username: superman Password: pizza Role: admin",
            "We can check the creation of the new user by selecting the user list from the left vertical bar.",
            "Now, we are ready from the server side! Let\u2019s move to Python!",
            "Install the package neo4j in our Python environment:",
            "pip install neo4j",
            "This is the officially supported driver so we stick to it in this tutorial. However, there are other well-designed community drivers you can consider for future projects. Once installed neo4j package, you can try to import it in Python.",
            "from neo4j import __version__ as neo4j_version print(neo4j_version)",
            "If everything is working, you should receive as output the version of neo4j package (4.0.1 in this case).",
            "Now, we are ready to query the graph database!",
            "Firstly, we need to define a connection class to connect to the graph database.",
            "from neo4j import GraphDatabase class Neo4jConnection: def __init__(self, uri, user, pwd): self.__uri = uri self.__user = user self.__pwd = pwd self.__driver = None try: self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd)) except Exception as e: print(\"Failed to create the driver:\", e) def close(self): if self.__driver is not None: self.__driver.close() def query(self, query, db=None): assert self.__driver is not None, \"Driver not initialized!\" session = None response = None try: session = self.__driver.session(database=db) if db is not None else self.__driver.session() response = list(session.run(query)) except Exception as e: print(\"Query failed:\", e) finally: if session is not None: session.close() return response",
            "The class above requires the url, the username and the password during the initialization. In the query method, the query string should be written in Neo4j\u2019s graph query language: Cypher. For more details, check the Cypher Refcard.",
            "Let\u2019s create an instance of connection with the parameters defined before.",
            "conn = Neo4jConnection(uri=\"bolt://localhost:7687\", user=\"superman\", pwd=\"pizza\")",
            "Then, we can make our first query! Let\u2019s create a database called coradb.",
            "conn.query(\"CREATE OR REPLACE DATABASE coradb\")",
            "We can feed our database with the data from the CORA dataset.",
            "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.",
            "Let\u2019s create the citation graph from CSV files. We read the CSV file containing nodes information line by line and add each node to the graph with label Paper and properties id and class (of the paper).",
            "query_string = ''' USING PERIODIC COMMIT 500 LOAD CSV WITH HEADERS FROM ' https://raw.githubusercontent.com/ngshya/datasets/master/cora/cora_content.csv' AS line FIELDTERMINATOR ',' CREATE (:Paper {id: line.paper_id, class: line.label}) ''' conn.query(query_string, db='coradb')",
            "We do the same with edges information (notice the \u201carrow\u201d syntax in defining the edge).",
            "query_string = ''' USING PERIODIC COMMIT 500 LOAD CSV WITH HEADERS FROM ' https://raw.githubusercontent.com/ngshya/datasets/master/cora/cora_cites.csv' AS line FIELDTERMINATOR ',' MATCH (citing_paper:Paper {id: line.citing_paper_id}),(cited_paper:Paper {id: line.cited_paper_id}) CREATE (citing_paper)-[:CITES]->(cited_paper) ''' conn.query(query_string, db='coradb')",
            "Finally, we are ready to make queries on our citation network! For example, we can ask which are the classes of papers in the network:",
            "query_string = ''' MATCH (p:Paper) RETURN DISTINCT p.class ORDER BY p.class ''' conn.query(query_string, db='coradb')",
            "or the list of most cited papers:",
            "query_string = ''' MATCH ()-->(p:Paper) RETURN id(p), count(*) as indegree ORDER BY indegree DESC LIMIT 10 ''' conn.query(query_string, db='coradb')",
            "You can use any syntax on Cypher Refcard, the only limit is your fantasy!",
            "In addition to Cypher queries, you can also run graph algorithms in Neo4j, for example: path finding, centralities computation, community detection, etc. To this end, we need to activate Neo4j Graph Data Science Library. We need to come back to Neo4j Browser and enter the configuration men\u00f9 (3 dots) of Graph Database.",
            "Under the tab Plugins, install Graph Data Science Library and restart. Before computing any measure, we need to create a graph.",
            "query_string = ''' CALL gds.graph.create( 'coraGraph', 'Paper', 'CITES' ) ''' conn.query(query_string, db='coradb')",
            "Now, we can compute the PageRank and the Betweenness of each node of the graph and save them in properties.",
            "query_string = ''' CALL gds.pageRank.write('coraGraph', { writeProperty: 'pagerank' }) YIELD nodePropertiesWritten, ranIterations ''' conn.query(query_string, db='coradb') query_string = ''' CALL gds.betweenness.write('coraGraph', { writeProperty: 'betweenness' }) YIELD minimumScore, maximumScore, scoreSum, nodePropertiesWritten ''' conn.query(query_string, db='coradb')",
            "The complete list of implemented algorithms and their syntax can be found here.",
            "If we query the graph right now, we can find out that each node has two more properties (pagerank and betweenness). We can transform the output in a dataframe and continue our analysis from there.",
            "from pandas import DataFrame query_string = ''' MATCH (p:Paper) RETURN DISTINCT p.id, p.class, p.pagerank, p.betweenness ''' dtf_data = DataFrame([dict(_) for _ in conn.query(query_string, db='coradb')]) dtf_data.sample(10)",
            "The code above produces the following output:",
            "If we have finished with the queries, we can close the connection.",
            "conn.close()",
            "Contacts: LinkedIn | Twitter"
        ]
    }
}
