{
  "took" : 18,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 30,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "sjj1woMBPS66Hy5_y9di",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://medium.datadriveninvestor.com/4-simple-steps-in-building-ocr-1f41c66099c1",
          "author" : "Naga Kiran",
          "length" : "4 min read",
          "title" : "4 Simple steps in building OCR",
          "tags" : [
            "machine-learning",
            "ocr",
            "python",
            "nlp",
            "image"
          ],
          "content" : [
            "OCR (optical character recognition) is the recognition of printed or written text characters by a computer. This involves photoscanning of the text character-by-character, analysis of the scanned-in image, and then translation of the character image into character codes, such as ASCII, commonly used in data processing.",
            "In OCR processing, the scanned-in image or bitmap is analyzed for light and dark areas in order to identify each alphabetic letter or numeric digit. When a character is recognized, it is converted into an ASCII code. Special circuit boards and computer chips designed expressly for OCR are used to speed up the recognition process.",
            "www.datadriveninvestor.com",
            "Steps in Optical Character Recognition :-",
            "1) Extraction of Character boundaries from Image,",
            "2) Building a Convolutional Neural Network(ConvNet) in remembering the Character images,",
            "3) Loading trained Convolutional Neural Network(ConvNet) Model,",
            "4) Consolidating ConvNet predictions of characters",
            "github.com",
            "The Algorithm is built in a way to segment each individual character in a Image as individual images :-) , followed by recognition and consolidation to text in an Image.",
            "to download the Pretrained Models . to download sample labelled character Images train data.",
            "1) Optical Scanning ✂️ from Image :",
            "Select any document or letter of having text information",
            "Extract Character boundaries: Contours can be explained simply as a curve joining all the continuous points (along the boundary). The contours are a useful tool for shape analysis and object detection and recognition. Here Contours explained in differentiating each individual character in an image with using contour dilation technique. Create a boundary to each character in an image with using OpenCV Contours method. Character recognition with the use ofOpenCV contours method. OpenCV code implementation in differentiating the words with the use of contours",
            "ret,thresh1 = cv2.threshold(im1,180,255,cv2.THRESH_BINARY_INV) kernel = np.ones((5,5),np.uint8) dilated = cv2.dilate(thresh1,kernel,iterations = 2) _,contours, hierarchy = cv2.findContours(dilated,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) cordinates = [] for cnt in contours: x,y,w,h = cv2.boundingRect(cnt) cordinates.append((x,y,w,h)) #bound the images cv2.rectangle(im,(x,y),(x+w,y+h),(0,255,0),1) cv2.namedWindow('BindingBox', cv2.WINDOW_NORMAL) cv2.imwrite('data/BindingBox4.jpg',im)",
            "Naming Convention followed(Labelling) : The extracted text characters should be labelled with the Original character name associated with it.",
            "Naming convention followed here is, last letter of file name should be the name associated with the character for pre-processing the images data.",
            "Pre-processing",
            "The raw data depending on the data acquisition type is subjected to a number of preliminary processing steps to make it usable in the descriptive stages of character analysis. The image resulting from scanning process may contain certain amount of noise Smoothing implies both filling and thinning. Filling eliminates small breaks, gaps and holes in digitized characters while thinning reduces width of line.",
            "(a) noise reduction (b) normalization of the data and (c) compression in the amount of information to be retained.",
            "2) Build a ConvNet Model ✂️(Character Recognition Model):",
            "Convolution Network of 8 layers with 2*4 layers residual feedbacks used in remembering the Patterns ✂️ of the Individual Character Images.",
            "1st Model will train on the Individual Character Images with direct Classification to predict the Images with softmax Classification of Character Categories. 2nd Model is same model with last before layer as predictor which will Calculate a Embedding of specified Flatten Neurons ( The Predicted flatten Values will have Feature Information of Receipt Images ).",
            "3) Load Trained ConvNet OCR model:",
            "Optical Character recognition last step involves preprocessing of image into specific word related contours and letter contours, followed by prediction and consolidating according to letter and word related contours in an image.",
            "once after training the model, we can save and load the pre-trained Optical character recognition model.",
            "4) Test and Consolidate Predictions of OCR :",
            "Consolidate predictions involves, assigning specific ID to each word related contour with the line associated with the word in image, Consolidating all predictions in a sorted series of specific word related contour and letters associated word."
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "szj1woMBPS66Hy5_y9di",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9",
          "author" : "Anders Christiansen",
          "length" : "6 min read",
          "title" : "Anchor Boxes — The key to quality object detection",
          "tags" : [
            "machine-learning",
            "object-detection",
            "computer-vision"
          ],
          "content" : [
            "One of the hardest concepts to grasp when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes. It is also one of the most important parameters you can tune for improved performance on your dataset. In fact, if anchor boxes are not tuned correctly, your neural network will never even know that certain small, large or irregular objects exist and will never have a chance to detect them. Luckily, there are some simple steps you can take to make sure you do not fall into this trap.",
            "When you use a neural network like YOLO or SDD to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object. The multiple predictions are output with the following format:",
            "Prediction 1: (X, Y, Height, Width), Class….Prediction ~80,000: (X, Y, Height, Width), Class",
            "Where the(X, Y, Height, Width) is called the “bounding box”, or box surrounding the objects. This box and the object class are labelled manually by human annotators.",
            "In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:",
            "We need to tell our network if each of its predictions is correct or not in order for it to be able to learn. But what do we tell the neural network it prediction should be? Should the predicted class be:",
            "Prediction 1: PearPrediction 2: Apple",
            "Or should it be:",
            "Prediction 1: ApplePrediction 2: Pear",
            "What if the network predicts:",
            "Prediction 1: ApplePrediction 2: Apple",
            "We need our network’s two predictors to be able to tell whether it is their job to predict the pear or the apple. To do this there are a several tools. Predictors can specialize in certain size objects, objects with a certain aspect ratio (tall vs. wide), or objects in different parts of the image. Most networks use all three criteria. In our example of the pear/apple image, we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image. Then we would have our answer for what the network should be predicting:",
            "Prediction 1: PearPrediction 2: Apple",
            "State of the art object detection systems currently do the following:",
            "1. Create thousands of “anchor boxes” or “prior boxes” for each predictor that represent the ideal location, shape and size of the object it specializes in predicting.",
            "2. For each anchor box, calculate which object’s bounding box has the highest overlap divided by non-overlap. This is called Intersection Over Union or IOU.",
            "3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU.",
            "4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example.",
            "5. If the highest IOU is less than 40%, then the anchor box should predict that there is no object.",
            "This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object appears in an image. Taking a look at an open source implementation of RetinaNet, a state-of-the-art object detector, we can visualize the anchor boxes. There are too many to visualize all at once, however here are just 1% of them:",
            "Using the default anchor box configuration can create predictors that are too specialized and objects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes. In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak our anchor boxes to be much smaller, such as this 1% sample:",
            "In the RetinaNet configuration, the smallest anchor box size is 32x32. This means that many objects smaller than this will go undetected. Here is an example from the WiderFace dataset (Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou) where we match bounding boxes to their respective anchor boxes, but some fall through the cracks:",
            "In this case, only four of the ground truth bounding boxes overlap with any of the anchor boxes. The neural network will never learn to predict the other faces. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the faces line up with at least one of our anchor boxes and our neural network can learn to detect them!",
            "As a general rule, you should ask yourself the following questions about your dataset before diving into training your model:",
            "What is the smallest size box I want to be able to detect? What is the largest size box I want to be able to detect? What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side.",
            "You can get a rough estimate of these by actually calculating the most extreme sizes and aspect ratios in the dataset. YOLO v3, another object detector, uses K-means to estimate the ideal bounding boxes. Another option is to learn the anchor box configuration.",
            "Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes and then decoding them as though they were predictions from your model. You should be able to recover the ground truth bounding boxes.",
            "Also, remember that if the center of the bounding box and anchor box differ, this will reduce the IOU. Even if you have small anchor boxes, you may miss some ground truth boxes if the stride between anchor boxes is wide. One way to ameliorate this is to lower the IOU threshold from 50% to 40%.",
            "A recent article by David Pacassi Torrico comparing current API implementations of face detection highlights the importance of correctly specifying anchor boxes. You can see that the algorithms do well except for small faces. Below are some pictures where an API failed to detect any faces at all, but many were detected with our new model:",
            "If you enjoy this article, you might like reading about object detection without anchor boxes.",
            "For a more in-depth explanation of anchor boxes you can refer to Andrew Ng’s Deep Learning Specialization or Jeremy Howards’s fast.ai"
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "tDj1woMBPS66Hy5_y9di",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://medium.com/@ghimire.aiesecer/data-augmentation-techniques-for-image-classification-in-tensorflow-5dad8db481de",
          "author" : "Suman Ghimire",
          "length" : "3 min read",
          "title" : "Data Augmentation Techniques for Image Classification in Tensorflow",
          "tags" : [
            "machine-learning",
            "augmentation",
            "deep-learning",
            "tensorflow",
            "image-classification"
          ],
          "content" : [
            "In computer vision, data augmentation is the technique to improve the performance of computer vision systems. This makes our classification algorithm more robust to changes such as sunlight, illumination etc, across our training dataset. Tensorflow API provides wide range of data augmentation methods to improve the classification performance in DNN. Using these augmentation steps to our dataset could increase the networks ability to generalize as more training data is generated, with variation from our original data. This technique is handy when dealing with small amount of dataset for the purpose of training DNN. Data augmentation, is also another way, we can reduce overfitting on models, where we increase the amount of training data using information only in our training data.",
            "Arguments can be provided as key-value pairs. The augmentation can be initialized as below, where you can specify your own desired values, otherwise, preprocessor.proto will just supply the default value:",
            "data_augmentation_options { random_image_scale { min_scale_ratio: 0.3 max_scale_ratio: 1.5 } }",
            "where, user-specified value will overwrite the defaults,",
            "def random_image_scale(image, masks=None, min_scale_ratio=0.5, max_scale_ratio=2.0, seed=None, preprocess_vars_cache=None): \"\"\"Scales the image size. Args: image: rank 3 float32 tensor contains 1 image -> [height, width, channels]. masks: (optional) rank 3 float32 tensor containing masks with size [height, width, num_masks]. The value is set to None if there are no masks. min_scale_ratio: minimum scaling ratio. max_scale_ratio: maximum scaling ratio. seed: random seed. preprocess_vars_cache: PreprocessorCache object that records       previously performed augmentations. Updated in-place. If this                            function is called multiple times with the same non-null cache, it will perform deterministically. Returns: image: image which is the same rank as input image. masks: If masks is not none, resized masks which are the same rank as input masks will be returned. with tf.name_scope('RandomImageScale', values=[image]): result = [] image_shape = tf.shape(image) image_height = image_shape[0] image_width = image_shape[1] generator_func = functools.partial( tf.random_uniform, [], minval=min_scale_ratio, maxval=max_scale_ratio, dtype=tf.float32, seed=seed) size_coef = _get_or_create_preprocess_rand_vars( generator_func, preprocessor_cache.PreprocessorCache.IMAGE_SCALE, preprocess_vars_cache) image_newysize = tf.to_int32( tf.multiply(tf.to_float(image_height), size_coef)) image_newxsize = tf.to_int32( tf.multiply(tf.to_float(image_width), size_coef)) image = tf.image.resize_images( image, [image_newysize, image_newxsize], align_corners=True) result.append(image) if masks: masks = tf.image.resize_nearest_neighbor( masks, [image_newysize, image_newxsize], align_corners=True) result.append(masks) return tuple(result)",
            "For randomly adjusting saturation,",
            "data_augmentation_options { random_adjust_saturation { } }",
            "where, the default values will be initialized unless specified,",
            "def random_adjust_saturation(image, min_delta=0.8, max_delta=1.25, seed=None, preprocess_vars_cache=None): \"\"\"Randomly adjusts saturation. Makes sure the output image is still between 0 and 255. Args: image: rank 3 float32 tensor contains 1 image -> [height, width,   channels] with pixel values varying between [0, 255]. min_delta: see max_delta. max_delta: how much to change the saturation. Saturation will change with a value between min_delta and max_delta. This value will be multiplied to the current saturation of the image. seed: random seed. preprocess_vars_cache: PreprocessorCache object that records previously performed augmentations. Updated in-place. If this function is called multiple times with the same non-null cache, it will perform deterministically. Returns: image: image which is the same shape as input image. \"\"\" with tf.name_scope('RandomAdjustSaturation', values=[image]): generator_func = functools.partial(tf.random_uniform, [], min_delta, max_delta, seed=seed) saturation_factor = _get_or_create_preprocess_rand_vars( generator_func, preprocessor_cache.PreprocessorCache.ADJUST_SATURATION, preprocess_vars_cache) image = tf.image.adjust_saturation(image/255, saturation_factor)*255 image=tf.clip_by_value(image,clip_value_min=0.0,clip_value_max=255.0) return image",
            "Here are the lists of options provided in Preprocessor.proto. Choose augmentation technique from the list of options that are suited for your purpose.",
            "NormalizeImage normalize_image = 1; RandomHorizontalFlip random_horizontal_flip = 2;      RandomVerticalFlip random_vertical_flip = 3 RandomPixelValueScale random_pixel_value_scale = 4; RandomImageScale random_image_scale = 5; RandomRGBtoGray random_rgb_to_gray = 6; RandomAdjustBrightness random_adjust_brightness = 7; RandomAdjustContrast random_adjust_contrast = 8; RandomAdjustHue random_adjust_hue = 9; RandomAdjustSaturation random_adjust_saturation = 10; RandomDistortColor random_distort_color = 11; RandomJitterBoxes random_jitter_boxes = 12; RandomCropImage random_crop_image = 13; RandomPadImage random_pad_image = 14; RandomCropPadImage random_crop_pad_image = 15; RandomCropToAspectRatio random_crop_to_aspect_ratio = 16; RandomBlackPatches random_black_patches = 17; RandomResizeMethod random_resize_method = 18; ScaleBoxesToPixelCoordinates scale_boxes_to_pixel_coordinates = 19; ResizeImage resize_image = 20; SubtractChannelMean subtract_channel_mean = 21; SSDRandomCrop ssd_random_crop = 22; SSDRandomCropPad ssd_random_crop_pad = 23; SSDRandomCropFixedAspectRatio ssd_random_crop_fixed_aspect_ratio = 24;",
            "Cheers!!! Happy Deep-Learning",
            "Suman Ghimire",
            "Co-Founder at Pixel Labs ( https://www.pixelnetworks.net/)",
            "Msc. Engineering & Technology ( Sustainable Agriculture)",
            "References",
            "github.com",
            "stackoverflow.com"
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "tTj1woMBPS66Hy5_y9dj",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://towardsdatascience.com/deepmind-releases-a-new-state-of-the-art-image-classification-model-nfnets-75c0b3f37312",
          "author" : "Mostafa Ibrahim",
          "length" : "6 min read",
          "title" : "Deepmind releases a new State-Of-The-Art Image Classification model — NFNets",
          "tags" : [
            "data-science",
            "machine-learning",
            "artificial-intelligence",
            "ai",
            "deep-learning"
          ],
          "content" : [
            "Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7× faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%.",
            "Source: arxiv",
            "One of the most annoying things about training a model is the time it takes to train it and the amount of memory needed to fit in the data and the models. Since image classification is one of the most common machine learning tasks, Deepmind released a new model that matches the state-of-art (SOTA) performance with significantly less size, higher training speed, and fewer optimization techniques for simplicity.",
            "In their work, they examine the current SOTA models such as EfficientNets and ResNets. In their analysis they pindown some of the optimization techniques that utilize a lot of memory without producing a significant value for performance. They prove that these networks can achieve the same performance without those optimization techniques.",
            "Although the proposed model might be the most interesting bit, I still find the analysis of previous work to be very interesting. Simply because this is where most of the learning happens, we start understanding what could have been done better and why the newly proposed method/technique is an improvement over the old one.",
            "The paper starts off with an analysis of batch normalisation. Why? because although it has shown great results and has been used heavily in tons of SOTA models, it has several disadvantages outlined by the paper [1], such as:",
            "Very expensive computational costs Introduces a lot of extra hyper-parameters that need further fine-tuning Causes a lot of implementation errors in distributed training Performs poorly on small batch sizes, which are used often in training larger models",
            "But first, before removing batch normalization, we have to understand what benefits it brought to the models. Because we want to find a smarter way to still have those benefits, but with fewer cons. Those benefits are [1]:",
            "It downscales residual branches in deep ResNets. ResNets are one of the most widely used image classification networks. They usually extend to thousands of layers, and batch normalization reduces the scale of “hidden activations” that often cause gradients to behave in a funny way ( gradient exploding problem ) Eliminates mean-shift for popular activation functions such as ReLU and GeLU. In large networks, the output of those activation functions typically shifts towards very large values on average. This causes the network to predict the same label for all samples in certain situations (such as initialization) decreasing its performance. Batch normalization solves this mean-shift problem.",
            "There are some other benefits, but I think you got the gist that its all mainly about regularisation and smoothing the training process.",
            "Although there have been previous attempts to remove batch normalization (BN) in various papers, the results didn’t match the SOTA performance or training latency and seemed to fail on large batch sizes, and this is the main selling point of this paper. They succeed in removing (BN) without affecting performance, and with improving the training latency by a large margin.",
            "To do that, they propose a gradient clipping technique called Adaptive Gradient Clipping (AGC) [1]. Essentially, gradient clipping is used to stabilize model training [1] by not allowing the gradient to go beyond a certain threshold. This allows using larger training rates and thus faster convergence without the exploding gradient problem.",
            "However, the main issue is setting the threshold hyper-parameter, which is quite a difficult and manual task. The main benefit of AGC is to remove this hyperparameter. To do this we have to examine the gradient norms and the parameter norms.",
            "Although I am quite interested in the mathematics behind every machine learning model, I understand that a lot of ML enthusiasts don’t enjoy reading a bunch of long differential equations, that’s why I will explain AGC from a theoretical/intuitive perspective rather than a mathematically rigorous one.",
            "A norm is simply a measure of the magnitude of a vector. AGC is built on the premise that:",
            "the unit-wise ratio of the norm of the gradients to the norm of the weights of a layer provides a simple measure of how much a single gradient descent step will change the original weights.",
            "Source: arxiv",
            "But why is that premise valid? Let’s back up a little. A very high gradient will make our learning unstable, and if that's the case then the ratio of the gradient of the weight matrix to the weight matrix will be very high.",
            "That weight ratio is equivalent to:",
            "learning rate x the ratio between the gradient and the weight matrix (which is our premise).",
            "So essentially, the ratio proposed by that premise is a valid indicator as to whether we should clip the gradient or not. There is also another minor tweak. They have found that through multiple experiments, it's much better to use a unit-wise ratio of gradient norms instead of a layer-wise ratio (because each layer can have more than one gradient).",
            "In addition to AGC, they also used dropout to substitute the regularisation effect that Batch normalization was offering.",
            "They also used an optimization technique called Sharpness-Aware Minimization (SAM) [1].",
            "Motivated by the connection between the geometry of the loss landscape and generalization — including a generalization bound that we prove here — we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several.",
            "Source: SAM arxiv paper",
            "The idea of loss sharpness seems quite interesting and I might be exploring it in another article for the sake of brevity here. One final point to note here though is that they make a small modification to SAM [1] to reduce its computational cost by 20–40%! and they only employ it on their 2 largest model variants. It’s always great to see additions being made to such techniques instead of just using them out of the box. I think this shows that they have analyzed it greatly before using it (and thus were able to optimize it a bit).",
            "Final thoughts and take away",
            "Who would have thought that replacing a minor optimization technique such as batch normalization would result in a 9x improvement in training latency. I think this sends a message of being a bit more skeptical about popular optimization techniques that are used everywhere. In all fairness, I have been a victim of this crime before, I used to just put every popular optimization technique into my machine learning projects without fully examining its pros and cons. I guess this is one of the main benefits of reading ML papers, the analysis of previous SOTAs!",
            "If you want to receive regular paper reviews about the latest papers in AI & Machine learning, add your email here & Subscribe!",
            "https://artisanal-motivator-8249.ck.page/5524b8f934",
            "References:",
            "[1] High-Performance Large-Scale Image Recognition Without Normalization. Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan. 2021",
            "If you are interested in reading more about other novel papers, check out my articles here:",
            "towardsdatascience.com",
            "towardsdatascience.com",
            "towardsdatascience.com",
            "towardsdatascience.com",
            "towardsdatascience.com"
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "tjj1woMBPS66Hy5_y9dj",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://towardsdatascience.com/facebook-deit-a-promising-new-technique-for-image-classification-6516f039b4bb",
          "author" : "Mostafa Ibrahim",
          "length" : "4 min read",
          "title" : "Facebook DeIt: State-of-art image classification with 0 convolutions and less than 1% of the state-of-art dataset",
          "tags" : [
            "machine-learning",
            "data-science",
            "artificial-intelligence",
            "deep-learning",
            "data"
          ],
          "content" : [
            "A few weeks ago, Facebook released a new ML model (Data-Efficient Image Transformer, DeIt) that achieves state-of-art image classification performance using only the ImageNet dataset (1.2 million images). State of the art visual transformers can only reach this performance using hundreds of millions of images [1]. And how Facebook achieved this is the most interesting bit since they didn’t use any convolutions or a large dataset.",
            "There are a lot of great machine learning papers that are being released almost daily. The reason that I chose this one to review is that it's using some interesting techniques.",
            "One of those techniques is attention and transformers which I don’t want to cover thoroughly since there are tons of other articles about them. I am however going to give a quick overview just so we can explore DeIt properly.",
            "Transformers and attention have been dominating the machine learning space for the last few years. They started in NLP and now they are moving to images.",
            "Visual transformers use Multi-head Self Attention layers. Those layers are based on the attention mechanism that utilizes queries, keys, and vectors to “pay attention” to information from different representations at different positions.",
            "A classic transformer block for images starts with a normal Feed Forward Network followed by a Multi-head Self Attention layer. One interesting bit is that the feed-forward network used an activation function called Gaussian Error Linear Unit which aims to regularize the model by randomly multiplying a few activations by 0.",
            "The visual transformer has some issues that were solved by this paper, such as:",
            "It was trained on 300 million images (JFT-300M [1]) Those 300 million images are a private dataset It couldn’t generalize well.",
            "Okay now that we have covered the basics, let’s start taking a look at what is special about this paper.",
            "Knowledge distillation refers to the idea of model compression by teaching a smaller network, step by step, exactly what to do using a bigger already trained network. The ‘soft labels’ refer to the output feature maps by the bigger network after every convolution layer. The smaller network is then trained to learn the exact behavior of the bigger network by trying to replicate it’s outputs at every level (not just the final loss).",
            "Source: Prakhar Ganesh",
            "This is quite fascinating, just like in the real world we have teachers, in ML we have bigger smaller networks mimicking larger networks to learn from them.",
            "Typical visual transformers use the concept of a trainable vector called the class token. This token attempts to replace conventional pooling layers that can be found in Convolutional Neural Networks. It boosts the model's performance and spreads out the information from image patches.",
            "Facebook adds a distillation token that interacts with this class token and other initial embeddings at the start to boost the self-attention mechanism of the model. This token is a trainable vector that is being learned during training.",
            "Its objective is to minimize the Kullback-Leibler (KL) divergence between the softmax of the teacher and the softmax of the student model, (this is called soft distillation). All you need to know about the KL divergence is that it measures the difference between 2 distributions.",
            "So essentially, this distillation token tries to minimize the difference in the information of the student network and the teacher network. This is quite an impressive and novel strategy!",
            "They have also verified [1] that the usefulness of this new token by attempting to add a class token (instead of the distillation token). The result was worse performance.",
            "Note that the teacher network here is a Convolutional Neural Network.",
            "One of the best things about this paper is that Facebook has released the full code, dataset, paper, and pretty much everything. They released 3 different models of different sizes. And as you can see from the graph, they all perform quite well even compared to one of the best and most recent networks, EfficientNet.",
            "In summary, I think these are the 3 main tricks to Facebook’s achievement:",
            "The power of visual transformers and attention Replacing word embeddings with patch embeddings through a distillation token Not relying on convolutions",
            "Final thoughts:",
            "There is no such thing as a perfect model, I am sure this model has a few flaws. However, it’s quite interesting to see what the top AI researchers are doing. I hope you got the intuition behind the distillation token trick so that you can be inventing your own tricks in your ML projects!",
            "I didn’t want to dive into mathematics (although I love math) so that the article would suit a larger audience. If you are interested in that and in checking out more of their results, I suggest taking a look at the paper.",
            "If you want to receive regular paper reviews about the latest papers in AI & Machine learning, add your email here & Subscribe!",
            "https://artisanal-motivator-8249.ck.page/5524b8f934",
            "References:",
            "[1] Training data-efficient image transformers & distillation through attention. Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou. 2021 In arxiv"
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "tzj1woMBPS66Hy5_y9dj",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://medium.com/deep-learning-journals/fast-scnn-explained-and-implemented-using-tensorflow-2-0-6bd17c17a49e",
          "author" : "Kshitiz Rimal",
          "length" : "8 min read",
          "title" : "Fast-SCNN explained and implemented using Tensorflow 2.0",
          "tags" : [
            "machine-learning",
            "tensorflow",
            "deep-learning",
            "semantic-segmentation",
            "fast-scnn"
          ],
          "content" : [
            "Fast Segmentation Convolutional Neural Network (Fast-SCNN) is an above real-time semantic segmentation model on high resolution image data suited to efficient computation on embedded devices with low memory. The authors of the original paper are: Rudra PK Poudel, Stephan Liwicki and Roberto Cipolla. The code used in this article is not the official implementation from the authors but an attempt by me to re-construct the model as described on the paper.",
            "Since the rise of autonomous vehicles, it is highly desirable that there exist a model that can process input in real-time. There already exist some state-of-the-art offline semantic segmentation models, but these models are large in size and memory requirement and requires expensive computation, Fast-SCNN can provide solution to all these problems.",
            "Some key aspects of Fast-SCNN are:",
            "Above real-time segmentation on high resolution images (1024 x 2048px) Yield Accuracy of 68% mean intersection over union Process Input at 123.5 frames per second on Cityscapes dataset No large pre-training required Combines spatial details at High resolution with deep features extracted at lower resolution",
            "Moreover, Fast-SCNN uses popular techniques found in state-of-the-art models to ensure the performance as listed above, techniques like Pyramid Pooling Module or PPM as used in PSPNet, Inverted Residual Bottleneck layers as used in MobileNet V2 and Feature Fusion Module used in model such as ContextNet, which uses both deep features extracted from the lower resolution data and spatial details from higher resolution data to ensure better and fast segmentation.",
            "Let us now begin with the exploration and the implementation of the Fast-SCNN. Fast-SCNN is constructed using 4 major building blocks. They are:",
            "Learning to Down-sample Global Feature Extractor Feature Fusion Classifier",
            "So far we know that the first few layers of Deep Convolutional neural network extract the low level features such as, edges and corners from the image. So to exploit this feature and make it re-usable for further layers, Learning to Down Sample is used. Its a coarse global feature extractor that can be re-used and shared by other modules in the network.",
            "Learning to Down-sample module uses 3 layers to extract out these global features. These are: Conv2D layer followed by 2 Depthwise Separable Convolutional Layers. During the implementation, after each Conv2D and Depthwise Separable Conv layers, a Batchnorm layer followed by Relu activation is used, as usually its a standard practice to introduce BatchNorm and Activations after such layers. Here, all 3 layers uses stride of 2 and kernel size of 3x3.",
            "Now, let us begin by first implementing this module. To begin, lets first install Tensorflow 2.0. It’s easier than ever to do this now. We can simply use Google Colab and begin our implementation. As of now, there is only alpha version of Tensorflow 2.0 available, which you can simply install using following command:",
            "!pip install tensorflow-gpu==2.0.0-alpha0",
            "Here, ‘-gpu’ implies that my Google Colab notebook uses GPU, and in case of yours, if you prefer to not use it, you can simply remove the ‘-gpu’ and then the Tensorflow installation will utilize the cpu of the system.",
            "After that, let’s import Tensorflow:",
            "import tensorflow as tf",
            "Now, let’s first create the Input layer for our Model. In Tensorflow 2.0 using TF.Keras high level api, we can do so by:",
            "This Input layer is our entry point to the model that we are going to build. Here we are utilizing Tf.Keras Functional api. The reason to use Functional api instead of Sequential is because, it provides flexibility that we require to construct this particular model.",
            "Moving on, let us now define the layers for the Learning to Down-sample module. For that, to make the process easy and re-usable, I have created a custom function that will check if the layer I want to add is a Conv2D layer or Depthwise Separable layer and later checks if I want to add relu at the end of the layer or not. Using this code block made the implementation easy to understand and re-usable through out the implementation.",
            "In TF.Keras, Convolutional layer is defined as tf.keras.layers.Conv2D and Depthwise Separable Layer as tf.keras.layers.SeparableConv2D",
            "Now, let’s add the layers for the module by simply calling our custom function with proper parameters:",
            "This module aimed to capture the global context for the segmentation. It directly takes the output from the Learning to Down-sample module. In this section different bottleneck residual blocks are introduced and a special module called Pyramid Pooling Module or PPM is also introduced to aggregate different region-based context information.",
            "Let’s begin with Bottleneck residual block.",
            "Above is the description for the bottleneck residual block from the paper. Similar to above, let us now implement it using tf.keras high level api.",
            "We begin by first describing some custom functions as per the table above. We begin with residual block which will call our custom conv_block function to add Conv2D and then adds DepthWise Conv2D layer followed by point-wise convolution layer, as described on table above. Then the final output from point-wise convolution is added with the original input to make it residual.",
            "This bottleneck residual block is added multiple times in the architecture, the number of times they are added is denoted by the ’n’ parameter in the Table 1 above. So, as per the architecture described in the paper, in order to added it ’n’ times, we introduce another custom function that will just do that.",
            "Now let’s add these bottleneck blocks to our model.",
            "Here, you will notice that the first input to these bottleneck block is from the output of the Learning to down-sample module.",
            "The final block to this Global Feature Extractor section is the Pyramid Pooling Module or PPM in short.",
            "PPM takes the feature maps from the last convolutional layer and then applies multiple sub region average pooling and upscaling functions to harvest different sub-region representations, and then they are concatenated together which carries both local and global context information from the image making the segmentation process more accurate.",
            "To implement that using TF.Keras, I have used another custom function",
            "Lets add this PPM module that will take input from the last bottleneck block",
            "The second argument here are the number of bin sizes to be provided to the PPM module, the bin sizes here used are as per described in the paper. These bin sizes are used to make AveragePooling at different sub regions as described in custom function above.",
            "In this module, two inputs are added together to better represent the segmentation. The first one is Higher level feature extracted from the Learning to Down-sample module, the output from this Learning to Down-sample module is point-wise convoluted first, before adding to the second input. Here no activation is added at the end of the point-wise convolution.",
            "The Second input is the output from the Global Feature Extractor. But before adding the second input, they first Upsampled by the factor of (4,4) and then DepthWise Convoluted and finally followed by another point-wise convolution. No activation is added to the point wise convolution output, the activations are introduced only after adding these two inputs.",
            "Here is the lower resolution operations implemented using TF.Keras",
            "Now, let us add these two inputs together for the feature fusion module.",
            "In classifier section, 2 DepthWise Separable Convolutional Layers are introduced followed by 1 Point-wise Convolutional layer. After each of these layers, BatchNorm layers followed by ReLU activations are also introduced.",
            "One thing to note here is that, in the original paper’s table (Table 1 above), there is no mention of Upscaling and Dropout layers after the point-wise convolutional layer, but in later part of the paper it is described that these layers are added after the point-wise convolutional layer. Therefore, during the implementation, I have also introduced these two layers as per written in the paper.",
            "After Upscaling as per desired from the final output, the SoftMax activation is introduced as the final layer.",
            "Now that we have added all the layers, let’s create our final model and compile it. To create the model, as already mentioned above, we are using Functional api from TF.Keras. Here the input to the model is the initial Input Layer described at the Learning to Down-sample module and the output is the final classifier output.",
            "Now, let’s compile it with optimizers and loss functions. In the original paper, the authors have used SGD optimizer with momentum value of 0.9 with batch-size of 12 during the training process. They have also used poly-learning rate for the learning rate scheduling with base value of 0.045 and power as 0.9. For the sake of simplicity, I have not used any learning rate scheduling here, but if required you can add it yourself for your particular training process. Further, it is always good idea to start with ADAM optimizer while compiling the model, but in this particular case with CityScapes dataset, authors have used only SGD. But in general case, it’s always good to start with ADAM optimizer and then move towards other different variations if required. For the loss function, authors have used Cross Entropy loss and so have used here during the implementation.",
            "In the paper, authors have used 19 categories from CityScapes Dataset for the training and evaluation. With this implementation you can tweak your model as per desired with any number of output you might require for your particular project.",
            "Here are some of the validation results from Fast-SCNN, compared with input image and ground truth.",
            "I hope you enjoyed this implementation article and if you think I have made any mistake during the implementation or explanation process, feel free to correct me or suggest me the changes.",
            "In this way, we can easily implement Fast-SCNN using Tensorflow 2.0 and its high level api TF.Keras. Below are the references I have used while implementing the model.",
            "For full code, you can visit my GitHub Repo:",
            "github.com",
            "Link to the Original paper: https://arxiv.org/abs/1902.04502 Link to PSPNet Original paper: https://arxiv.org/pdf/1612.01105.pdf Link to ContextNet original paper: https://arxiv.org/abs/1805.04554 CityScapes Dataset official website: https://www.cityscapes-dataset.com/ Official Guide to Tensorflow 2.0: https://www.tensorflow.org/alpha Full code for the implementation: https://github.com/kshitizrimal/Fast-SCNN/blob/master/tf_2_0_fast_scnn.py Official Implementation of ContextNet: https://www.toshiba.eu/eu/Cambridge-Research-Laboratory/Computer-Vision/Resources/ContextNet/?fbclid=IwAR1T-eLK_xLq1Hu7Xz161YCaKzoZBtQMyvUFTySxbEqM6NNHY7xWV7nq9rA Pyramid Pooling Module code, inspired from the PSPNet Implementation: https://github.com/dhkim0225/keras-image-segmentation/blob/master/model/pspnet.py Bottleneck Residual block code inspired from MobileNet V2 Implementation: https://github.com/xiaochus/MobileNetV2/blob/master/mobilenet_v2.py"
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "uDj1woMBPS66Hy5_y9dj",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a",
          "author" : "Ayush Thakur",
          "length" : "12 min read",
          "title" : "Interpretability in Deep Learning with W&B — CAM and GradCAM",
          "tags" : [
            "model-interpretability",
            "wandb",
            "convolutional-network",
            "deep-learning",
            "production-ml"
          ],
          "content" : [
            "Training a classification model is interesting, but have you ever wondered how your model is making its predictions? Is your model actually looking at the dog in the image before classifying it as a dog with 98% accuracy? Interesting, isn’t it. In today’s report, we will explore why deep learning models need to be interpretable, and some interesting methods to peek under the hood of a deep learning model. Deep learning interpretability is a very exciting area of research and much progress is being made in this direction already.",
            "So why should you care about interpretability? After all, the success of your business or your project is judged primarily by how good the accuracy of your model is. But in order to deploy our models in the real world, we need to consider other factors too. For instance, is racially biased? Or, what if it’s classifying humans with 97% accuracy, but while it classifies men with 99% accuracy, it only achieves 95% accuracy on women?",
            "Understanding how a model makes its predictions can also help us debug your network. [Check out this blog post on ‘Debugging Neural Networks with PyTorch and W&B Using Gradients and Visualizations’ for some other techniques that can help].",
            "At this point, we are all familiar with the concept that deep learning models make predictions based on the learned representation expressed in terms of other simpler representations. That is, deep learning allows us to build complex concepts out of simpler concepts. Here’s an amazing Distill Pub post to help you understand this concept better. We also know that these representations are learned while we train the model with our input data and the label, in case of some supervised learning task like image classification. One of the criticisms of this approach is that the learned features in a neural network are not interpretable.",
            "Today we’ll look at 2techniques that address this criticism and shed light into neural networks’ “black-box” nature of learning.",
            "Class Activation Map(CAM) Gradient CAM",
            "It has been observed that convolution units of various layers of a convolutional neural network act as an object detector even though no such prior about the location of the object is provided while training the network for a classification task. Even though convolution has this remarkable property, it is lost when we use a fully connected layer for the classification task. To avoid the use of a fully connected network some architectures like Network in Network(NiN) and GoogLeNet are fully convolutional neural networks.",
            "Global Average Pooling(GAP) is a very commonly used layer in such architectures. It is mainly used as a regularizer to prevent overfitting while training. The authors of Learning Deep Features for Discriminative Localization found out that by tweaking such an architecture, they can extend the advantages of GAP and can retain its localization ability until the last layer. Let’s try to quickly understand the procedure of generating CAM using GAP.",
            "The class activation map simply indicates the discriminative region in the image which the CNN uses to classify that image in a particular category. For this technique, the network consists of ConvNet and just before the Softmax layer(for multi-class classification), global average pooling is performed on the convolutional feature maps. The output of this layer is used as features for a fully-connected layer that produces the desired classification output. Given this simple connectivity structure, we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps.",
            "Let’s try to implement this. \uD83D\uDE04",
            "Suppose you have built your deep classifier with Conv blocks and a few fully connected layers. We will have to modify this architecture such that there aren’t any fully connected layers. We will use the GlobalAveragePooling2D layer between the output layer (softmax/sigmoid) and the last convolutional block.",
            "The CAMmodel provides a required modification to our cat and dog classifier. Here I am using pre-trained VGG16 model to simulate my already trained cat-dog classifier.",
            "def CAMmodel(): ## Simulating my pretrained dog and cat classifier. vgg = VGG16(include_top=False, weights='imagenet') vgg.trainable = False ## Flatten the layer so that it's not nested in the sequential model. vgg_flat = flatten_model(vgg) ## Insert GAP vgg_flat.append(keras.layers.GlobalAveragePooling2D()) vgg_flat.append(keras.layers.Dense(1, activation='sigmoid')) model = keras.models.Sequential(vgg_flat) return model",
            "A simple utility flatten_model returns the list of layers in my pre-trained model. This is done so that the layers are not nested when modified using Sequential model and the last convolutional layer can be accessed and used as an output. I appended GlobalAveragePooling2D and Dense in the returned array from flatten_model. Finally, the Sequential model is returned.",
            "def flatten_model(model_nested): ''' Utility to flatten pretrained model ''' layers_flat = [] for layer in model_nested.layers: try: layers_flat.extend(layer.layers) except AttributeError: layers_flat.append(layer) return layers_flat",
            "Next we call model.build() with the appropriate model input shape.",
            "keras.backend.clear_session() model = CAMmodel() model.build((None, None, None, 3)) # Note model.summary()",
            "Since a new layer was introduced, we have to retrain the model. But we don’t need to retrain the entire model. We can freeze the convolutional blocks by using vgg.trainable=False.",
            "Observations:",
            "There is a decline in the model performance in terms of both training and validation accuracy. The optimal train and validation accuracy that I achieved was 99.01% and 95.67% respectively. Thus for the implementation of CAM, we have to modify our architecture and thus a decline in model performance.",
            "In the __init__for the CAM class, we initialize cammodel. Notice there are two outputs from this cammodel:",
            "Output from the last convolutional layer ( block5_conv3 here) The model prediction (softmax/sigmoid).",
            "class CAM: def __init__(self, model, layerName): self.model = model self.layerName = layerName ## Prepare cammodel last_conv_layer = self.model.get_layer(self.layerName).output self.cammodel = keras.models.Model(inputs=self.model.input, outputs=[last_conv_layer, self.model.output]) def compute_heatmap(self, image, classIdx): ## Get the output of last conv layer and model prediction [conv_outputs, predictions] = self.cammodel.predict(image) conv_outputs = conv_outputs[0, :, :, :] conv_outputs = np.rollaxis(conv_outputs, 2) ## Get class weights between class_weights = self.model.layers[-1].get_weights()[0] ## Create the class activation map. caml = np.zeros(shape = conv_outputs.shape[1:3], dtype=np.float32) for i, w in enumerate(class_weights[:]): caml += w * conv_outputs[i, :, :] caml /= np.max(caml) caml = cv2.resize(caml, (image.shape[1], image.shape[2])) ## Prepare heat map heatmap = cv2.applyColorMap(np.uint8(255*caml), cv2.COLORMAP_JET) heatmap[np.where(caml < 0.2)] = 0 return heatmap def overlay_heatmap(self, heatmap, image): img = heatmap*0.5 + image img = img*255 img = img.astype('uint8') return (heatmap, img)",
            "The compute_heatmap method is responsible for generating the heatmap which is the discriminative region used by CNN to identify the category (class of image).",
            "cammodel.predict() on the input image will give the feature map of the last convolutional layer of shape (1,7,7,512) . We also extract the weights of the output layer of shape (512,1) . Finally, the dot product of the extracted weights from the final layer and the feature map is calculated to produce the class activation map.",
            "Now we wrap everything in a callback. The CamLogger callback integrates wandb.log() method to log the generated activation maps onto the W&B run page. The heatmap returned from the CAM is finally overlayed on the original image by calling overlay_heatmap() method.",
            "We can draw lot of conclusions from the the plots as shown below. \uD83D\uDC47 Note the examples chart contains validation images along with their prediction scores. If the prediction score is greater than 0.5, the network classifies the image as a dog, otherwise as a cat. While CAM charts have their corresponding class activation maps. Let's go through some observations:",
            "The model is classifying the images as dogs by looking at the facial region in the image. For some images it’s able to look at the entire body, except the paws. The model is classifying the images as cats by looking at the ears, paws and whiskers.",
            "For a misclassified image the model is not looking at where it should be looking. Thus by using CAM we are able to interpret the reason behind this misclassification, which is really cool.",
            "Why is that? Even though the ears, paws and whiskers are present in the image why did it look at something else. One reason I can think of is that since we haven’t fine tuned our pretrained VGG16 on our cat-dog dataset, the CNN as feature extractor is not entirely familiar with the patterns (distributions) appearing in our dataset.",
            "When multiple instances of the same class are present in the image, the model looks only at one of them. But that is okay, given that we are not concerned about object detection. Note that the confidence is low because of this.",
            "Other use cases:",
            "CAM can be used for a weakly supervised object localization task. The authors of the linked paper tested the ability of the CAM for a localization task on the ILSVRC 2014 benchmark dataset. The technique was able to achieve 37.1% top-5 error for object localization on this dataset, which is close to the 34.2% top-5 error achieved by a fully supervised CNN approach.",
            "Even though CAM was amazing it had some limitations:",
            "The model needs to be modified in order to use CAM. The modified model needs to be retrained, which is computationally expensive. Since fully connected Dense layers are removed. the model performance will surely suffer. This means the prediction score doesn’t give the actual picture of the model’s ability. The use case was bound by architectural constraints, i.e., architectures performing GAP over convolutional maps immediately before output layer.",
            "What makes a good visual explanation?:",
            "Certainly the technique should localize the class in the image. We saw this in CAM and it was worked remarkable good. Finer details should be captured, i.e., the activation map should be high resolution.",
            "Thus the authors of Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, a really amazing paper, came up with modifications to CAM and previous approaches. Their approach uses the gradients of any target prediction flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the class of the image.",
            "Thus Grad-CAM is a strict generalization over CAM. Beside overcoming the limitations of CAM it’s applicable to different deep learning tasks involving CNNs. It is applicable to:",
            "CNNs with fully-connected layers (e.g. VGG) without any modification to the network. CNNs used for structured outputs like image captioning. CNNs used in tasks with multi-modal inputs like visual Q&A or reinforcement learning, without architectural changes or re-training.",
            "Let’s implement this \uD83D\uDE04",
            "We will focus on the image classification task. Unlike CAM we don’t have to modify our model for this task and retrain it.",
            "I have used a VGG16 model pretrained on ImageNet as my base model and I'm simulating Transfer Learning with this.",
            "The layers of the baseline model are turned to non-trainable by using vgg.trainable = False. Note how I have used fully connected layers in the model.",
            "def catdogmodel(): inp = keras.layers.Input(shape=(224,224,3)) vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp, input_shape=(224,224,3)) vgg.trainable = False x = vgg.get_layer('block5_pool').output x = tf.keras.layers.GlobalAveragePooling2D()(x) x = keras.layers.Dense(64, activation='relu')(x) output = keras.layers.Dense(1, activation='sigmoid')(x) model = tf.keras.models.Model(inputs = inp, outputs=output) return model",
            "You will find the class GradCAM in the linked notebook. This is a modified implementation from Grad-CAM: Visualize class activation maps with Keras, TensorFlow, and Deep Learning, an amazing blog post, by Adrian Rosebrook of PyImageSearch.com. I would highly suggest checking out the step by step implementation of the GradCAM class in that blog post.",
            "I made two modifications to it:",
            "While doing transfer learning, that is, if your target (last) convolutional layer is non trainable, tape.gradient(loss, convOutputs) will return None . This is because tape.gradient() by default does not trace non-trainable variables/layers. Thus to use that layer for computing your gradients you need to allow GradientTape to watch it by calling tape.watch() on the target layer output (tensor). Hence the change,",
            "with tf.GradientTape() as tape: tape.watch(self.gradModel.get_layer(self.layerName).output) inputs = tf.cast(image, tf.float32) (convOutputs, predictions) = self.gradModel(inputs)",
            "The original implementation didn’t account for binary classification. The original authors also talked about softmax-ing the output. So in order to train a simple cat and dog classifier, I made a small modification. Hence the change,",
            "if len(predictions)==1: # Binary Classification loss = predictions[0] else: loss = predictions[:, classIdx]",
            "The GRADCAM class can be used after the model is trained or as a callback. Here's a small excerpt from his blog post.",
            "The third point motivated me to work on this project. I built a custom callback around this GRADCAM implementation and used wandb.log() to log the activation maps. Thus by using this callback you can use GradCAM while training.",
            "Given we’re working with a simple dataset I have only trained for few epochs and the model seems to work well.",
            "Here’s the GradCAM custom callback.",
            "class GRADCamLogger(tf.keras.callbacks.Callback): def __init__(self, validation_data, layer_name): super(GRADCamLogger, self).__init__() self.validation_data = validation_data self.layer_name = layer_name def on_epoch_end(self, logs, epoch): images = [] grad_cam = [] ## Initialize GRADCam Class cam = GradCAM(model, self.layer_name) for image in self.validation_data: image = np.expand_dims(image, 0) pred = model.predict(image) classIDx = np.argmax(pred[0]) ## Compute Heatmap heatmap = cam.compute_heatmap(image, classIDx) image = image.reshape(image.shape[1:]) image = image*255 image = image.astype(np.uint8) ## Overlay heatmap on original image heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1])) (heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5) images.append(image) grad_cam.append(output) wandb.log({\"images\": [wandb.Image(image) for image in images]}) wandb.log({\"gradcam\": [wandb.Image(cam) for cam in grad_cam]})",
            "GradCAM being a strict generalization over CAM, should be preferred over CAM. To understand the theoretical underpinnings of this technique I recommend reading Demystifying Convolutional Neural Networks using GradCam by Divyanshu Mishra or simply reading the linked paper. A couple interesting conclusions we can draw include:",
            "The model looks at the face of the dogs to classify them correctly, while I am unsure about the cat.",
            "The model is able to localize multiple instances of the class in an image, i.e. the prediction score is accounting for multiple dogs and cats in the image.",
            "Class Activation Maps and Grad-CAMs are a few approaches that introduce some explainability/interpretability into deep learning models, and are quite widely used. What’s most fascinating about these techniques is the ability to perform the object localization task, even without training the model with a location prior. GradCAM, when used for image captioning, can help us understand what region in the image is used to generate a certain word. When used for a Visual Q&A task, it can help us understand why the model came to a particular answer. Even though Grad-CAM is class-discriminative and localizes the relevant image regions, it lacks the ability to highlight fine-grained details the way pixel-space gradient visualization methods like Guided backpropagation, and Deconvolution do. Thus the authors combined Grad-CAM with Guided backpropagation.",
            "Thanks for reading this report until the end. I hope you find the callbacks introduced here helpful for your deep learning wizardry. Please feel free to reach out to me on Twitter(@ayushthakur0) for any feedback on this report. Thank you."
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "uTj1woMBPS66Hy5_y9dj",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://towardsdatascience.com/custom-object-detection-using-tensorflow-from-scratch-e61da2e10087",
          "author" : "Khush Patel",
          "length" : "8 min read",
          "title" : "Custom Object Detection using TensorFlow from Scratch",
          "tags" : [
            "tensorflow",
            "deep-learning",
            "artificial-intelligence",
            "object-detection",
            "computer-vision"
          ],
          "content" : [
            "In this tutorial, we’re going to get our hands dirty and train our own dog (corgi) detector using a pre-trained SSD MobileNet V2 model.",
            "Instead of training your own model from scratch, you can build on existing models and fine-tune them for your own purpose without requiring as much computing power.",
            "Install Tensorflow using the following command:",
            "$ pip install tensorflow",
            "If you have a GPU that you can use with Tensorflow:",
            "$ pip install tensorflow-gpu",
            "$ pip install pillow Cython lxml jupyter matplotlib",
            "Install protobuf using Homebrew (you can learn more about Homebrew here)",
            "$ brew install protobuf",
            "For protobuf installation on other OS, follow the instructions here.",
            "In this tutorial, we’re going to use resources in the Tensorflow models repository. Since it does not come with the Tensorflow installation, we need to clone it from their Github repo:",
            "First change into the Tensorflow directory:",
            "# For example: ~/anaconda/envs/<your_env_name>/lib/python3.6/site-packages/tensorflow $ cd <path_to_your_tensorflow_installation>",
            "Clone the Tensorflow models repository:",
            "$ git clone https://github.com/tensorflow/models.git",
            "From this point on, this directory will be referred to as the modelsdirectory",
            "Every time you start a new terminal window to work with the pre-trained models, it is important to compile Protobuf and change your PYTHONPATH.",
            "Run the following from your terminal:",
            "$ cd <path_to_your_tensorflow_installation>/models/research/ $ protoc object_detection/protos/*.proto --python_out=. $ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim",
            "Run a quick test to confirm that the Object Detection API is working properly:",
            "$ python object_detection/builders/model_builder_test.py",
            "If the result looks like the following, you’re ready to proceed to the next steps!",
            "............... ---------------------------------------------------------------------- Ran 15 tests in 0.123s OK",
            "To make this tutorial easier to follow along, create the following folder structure within the models directory you just cloned:",
            "models ├── annotations |   └── xmls ├── images ├── checkpoints ├── tf_record ├── research ...",
            "These folders will be used to store the required components for our model as we proceed.",
            "You can collect data in either images or video format. Here I mentioned both ways to collect data.",
            "Data preparation is the most important part of training your own model. Since we’re going to train a corgi detector, we must collect pictures of corgis! About 200 of them would be sufficient.",
            "I recommend using google-images-download to download images. It searches Google Images and then downloads images based on the inputs you provided. In the inputs, you can specify search parameters such as keywords, number of images, image format, image size, and usage rights.",
            "Since we’re downloading more than 100 images at a time, we need a chromedriver in the models directory (download here). Once you have the chromedriver ready, you could use this sample command to download images. Make sure all your images are in the jpg format:",
            "# From the models directory $ googleimagesdownload --keywords 'welsh corgi dog' \\ --limit 200 \\ --size medium \\ --chromedriver ./chromedriver \\ --format jpg",
            "After downloading, save all images to models/images/. To make subsequent processes easier, let's rename the images as numbers (e.g. 1.jpg, 2.jpg) by running the following script:",
            "import os path = 'models/images/' counter = 1 for f in os.listdir(path): suffix = f.split('.')[-1] if suffix == 'jpg' or suffix == 'png': new = '{}.{}'.format(str(counter), suffix) os.rename(path + f, path + new) counter = int(counter) + 1",
            "medium.com",
            "Once you’ve collected all the images you need, you need to label them manually. There are many packages that serve this purpose. labelImg is a popular choice.",
            "labelImg provides a user-friendly GUI. Plus, it saves label files (.xml) in the popular Pascal VOC format. If you want these images for training YOLO(You only Look Once) then use YOLO. Just set the current directory and Save directory as per our structure.",
            "Here's what a labelled image looks like in labelImg:",
            "Double check that every image has a corresponding .xml file and save them in models/annotations/xmls/.",
            "For a large number of annotations, you can use different shortcuts mentioned below:",
            "Ctrl + u - Load all of the images from a directory Ctrl + r - Change the default annotation target dir Ctrl + s - Save w - Create a rect box d - Next image a - Previous image del - Delete the selected rect box Ctrl++ - Zoom in Ctrl-- - Zoom out Ctrl + d - Copy the current label and rect box Space - Flag the current image as verified ↑→↓←Keyboard arrows to move selected rect box",
            "Classes need to be listed in the label map. Since we’re only detecting corgis, the label map should contain only one item like the following:",
            "item { id: 1 name: 'corgi' }",
            "Note that id must start from 1, because 0 is a reserved id.",
            "Save this file as label_map.pbtxt in models/annotations/",
            "trainval.txt is a list of image names without file extensions. Since we have sequential numbers for image names, the list should look like this:",
            "1 2 3 ... 198 199 200",
            "Save this file as trainval.txt in models/annotations/",
            "You can use this link to create XML files to CSV. We have all images and their bounding boxes are in XML format. Also all image has separate XML file so using this code we are creating a CSV file which contains all the XML files and their bounding box co-ordinates to single CSV file which is input for creating TFrecords.",
            "TFRecord is an important data format designed for Tensorflow. (Read more about it here). Before you can train your custom object detector, you must convert your data into the TFRecord format.",
            "Since we need to train as well as validate our model, the data set will be split into training (train.record) and validation sets (val.record). The purpose of training set is straight forward - it is the set of examples the model learns from. The validation set is a set of examples used DURING TRAINING to iteratively assess model accuracy.",
            "We’re going to use create_tf_record.py to convert our data set into train.record and val.record. Download here and save it to models/research/object_detection/dataset_tools/.",
            "Just change the label name in if row_label == ‘Label1’: as per your classifications.",
            "This script is preconfigured to do 70–30 train-val split. Execute it by running:",
            "# From the models directory $ python research/object_detection/dataset_tools/create_tf_record.py",
            "If the script is executed successfully, train.record and val.record should appear in your models/research/ directory. Move them into the models/tf_record/ directory.",
            "There are many pre-trained object detection models available in the model zoo. In order to train them using our custom data set, the models need to be restored in Tensorflow using their checkpoints (.ckpt files), which are records of previous model states.",
            "For this tutorial, we’re going to download ssd_mobilenet_v2_coco here and save its model checkpoint files (model.ckpt.meta, model.ckpt.index, model.ckpt.data-00000-of-00001) to our models/checkpoints/ directory.",
            "Each of the pretrained models has a config file that contains details about the model. To detect our custom class, the config file needs to be modified accordingly.",
            "The config files are included in the models directory you cloned in the very beginning. You can find them in:",
            "models/research/object_detection/samples/configs",
            "In our case, we’ll modify the config file for ssd_mobilenet_v2_coco. Make a copy of it first and save it in the models/ directory.",
            "Here are the items we need to change:",
            "Since we’re only trying to detect corgis, change num_classes to 1 fine_tune_checkpoint tells the model which checkpoint file to use. Set this to checkpoints/model.ckpt The model also needs to know where the TFRecord files and label maps are for both training and validation sets. Since our train.record and val.record are saved in tf_record folder, our config should reflect that:",
            "train_input_reader: { tf_record_input_reader { input_path: \"tf_record/train.record\" } label_map_path: \"annotations/label_map.pbtxt\" } eval_input_reader: { tf_record_input_reader { input_path: \"tf_record/val.record\" } label_map_path: \"annotations/label_map.pbtxt\" shuffle: false num_readers: 1 }",
            "At this point, your models directory should look like this:",
            "models ├── annotations |   ├── label_map.pbtxt |   ├── trainval.txt |   └── xmls |       ├── 1.xml |       ├── 2.xml |       ├── ... | ├── images |   ├── 1.jpg |   ├── 2.jpg |   ├── ... | ├── checkpoints |   ├── model.ckpt.data-00000-of-00001 |   ├── model.ckpt.index |   └── model.ckpt.meta | ├── tf_record |   ├── train.record |   └── val.record | ├── research |   ├── ... ...",
            "If you have successfully completed all previous steps, you’re ready to start training!",
            "Follow the steps below:",
            "# Change into the models directory $ cd tensorflow/models # Make directory for storing training progress $ mkdir train # Make directory for storing validation results $ mkdir eval # Begin training $ python research/object_detection/train.py \\ --logtostderr \\ --train_dir=train \\ --pipeline_config_path=ssd_mobilenet_v2_coco.config",
            "Training time varies depending on the computing power of your machine.",
            "Evaluation can be run in parallel with training. The eval.py script checks the train directory for progress and evaluate the model based on the most recent checkpoint.",
            "# From the models directory $ python research/object_detection/eval.py \\ --logtostderr \\ --pipeline_config_path=ssd_mobilenet_v2_coco.config \\ --checkpoint_dir=train \\ --eval_dir=eval",
            "You can visualize model training progress using Tensorboard:",
            "# From the models directory $ tensorboard --logdir=./",
            "Based on the graphs output by Tensorboard, you may decide when you want to stop training. Usually, you may stop the process when the loss function is tapering off and no longer decreasing by a significant amount. In my case, I stopped at step 3258.",
            "Once you finish training your model, you can export your model to be used for inference. If you’ve been following the folder structure, use the following command:",
            "# From the models directory $ mkdir fine_tuned_model $ python research/object_detection/export_inference_graph.py \\ --input_type image_tensor \\ --pipeline_config_path ssd_mobilenet_v2_coco.config \\ --trained_checkpoint_prefix  train/model.ckpt-<the_highest_checkpoint_number> \\ --output_directory fine_tuned_model",
            "Now that you have a model, you can use it to detect corgis in pictures and videos! For the purpose of demonstration, we’re going to detect corgis in an image. Before you proceed, pick an image you want to test the model with.",
            "The models directory came with a notebook file (.ipynb) that we can use to get inference with a few tweaks. It is located at models/research/object_detection/object_detection_tutorial.ipynb. Follow the steps below to tweak the notebook:",
            "MODEL_NAME = 'ssd_mobilenet_v2_coco_2018_03_29' PATH_TO_CKPT = 'path/to/your/frozen_inference_graph.pb' PATH_TO_LABELS = 'models/annotations/label_map.pbtxt' NUM_CLASSES = 1 Comment out cell #5 completely (just below Download Model ) Since we’re only testing on one image, comment out PATH_TO_TEST_IMAGES_DIR and TEST_IMAGE_PATHS in cell #9 (just below Detection ) In cell #11 (the last cell), remove the for-loop, unindent its content, and add path to your test image:",
            "imagepath = 'path/to/image_you_want_to_test.jpg",
            "After following through the steps, run the notebook and you should see the corgi in your test image highlighted by a bounding box!",
            "There you have your custom corgi detector!",
            "Tensorflow Object Detection Model Documentation",
            "Do visit my Website: http://www.khushpatel.com"
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "ujj1woMBPS66Hy5_y9dj",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://medium.com/mindboard/image-classification-with-variable-input-resolution-in-keras-cbfbe576126f",
          "author" : "Eric Muccino",
          "length" : "4 min read",
          "title" : "Image Classification with Variable Input Resolution in Keras",
          "tags" : [
            "convolutional-network",
            "machine-learning",
            "deep-learning",
            "neural-networks",
            "masala"
          ],
          "content" : [
            "Convolutional neural networks (CNN) are a type of neural network designed for image classification. For an introduction to CNNs, check out this post by Matthew Stewart.",
            "Many CNN architectures require that all input images have the same resolution (height and width). This happens when a convolutional layer is flattened and fully connected to a dense layer. Since the network has to initialize the dense layer weights, the convolutional layer resolution must be known, requiring the input resolution to be predetermined and constant. The fallback of this is that all images, both during training and inference, must be resized to fit the resolution dimensions, creating an extra step in image preprocessing and causing a loss of resolution on large images needing to be down-sampled. This could have negative impacts on model performance, depending on the image data used and what the model is trying predict.",
            "There is a way to avoid specifying input dimensions when setting up a CNN, allowing for variable image resolutions during training and inference. This is done by using global pooling layers after the final convolutional layer and before any dense layers within the CNN. Global pooling reduces each channel of a convolutional layer to a single value in a way that is not dependent on the resolution of the channel. This is usually done by taking either the average or the maximum of all of the values in the channel, leaving you with a single layer of neurons, one for each channel. The number of channels in a convolutional layer is defined in the model architecture and is independent of the channel resolutions.",
            "Now let’s take a look at how to train a CNN with variable image resolution using Keras. We’ll use the Imagenette dataset which can be found here. The following code was written with TensorFlow 2.1.",
            "First, we build our neural network. We will use one of the supplied CNN models in the applications module of Keras. Don’t forget to include a global pooling layer.",
            "Next, we need to build a batch generator class that will load and preprocess our data during training. While we do this, we will have to keep in mind some limitations to the extent of our image resolution variability. If an image is too large, we may run out of memory. To avoid this, we set a threshold for the maximum height and width we will allow our images to be. Anything above this threshold will be down-sampled. Also, TensorFlow requires that each input tensor has uniform shape. This means we will have to pad the images of each batch to match the height and width of the largest image in the batch. Batches containing single images do not need to be padded, so we can run inference on images without any padding if we run them one at a time.",
            "All that is left is to specify our training hyperparameters, initialize train and test generators, and train our model.",
            "In this post, we’ve explored how to use global pooling layers to create and train a CNN that is capable of handling variable image resolution. While we focused on the benefit of resolution variability, global pooling layers provide other benefits that we did not explore, such as nonlinearity and regularization. Consider using global pooling for reasons beyond input shapes.",
            "Keep in mind:While our model is capable of receiving any image resolution that fits in memory, it is generally a good idea to train on images that are roughly the same size. This is so that the model will encounter artifacts within images that are of similar scale, allowing it to generalize more easily. Similarly, for best results, we want to run inference on images that are close in size to those that were used in training. However, this will depend on the specifics of the use case and the images being used.",
            "Some improvements that could be made to the training data generator used in this tutorial: 1) Random batch image augmentations.2) Random batch padding styles.3) Parallel data processing.",
            "The Mindboard Data Science Team explores cutting-edge technologies in innovative ways to provide original solutions, including the Masala.AI product line. Masala provides media content rating services such as vRate, a browser extension that detects and blocks mature content with custom sensitivity settings. The vRate browser extension is available for download via the Chrome Web Store. Check out www.masala.ai for more info."
          ]
        }
      },
      {
        "_index" : "indexname",
        "_type" : "typename",
        "_id" : "uzj1woMBPS66Hy5_y9dj",
        "_score" : 1.0,
        "_source" : {
          "url" : "https://medium.com/analytics-vidhya/image-super-resolution-using-gans-a10254f58892",
          "author" : "gautam iruvanti",
          "length" : "4 min read",
          "title" : "Image Super Resolution Using GANs",
          "tags" : [
            "adversarial-network",
            "machine-learning",
            "data-science",
            "image-processing",
            "deep-learning"
          ],
          "content" : [
            "Image super resolution is a technique of reconstructing a high resolution image from the observed low resolution image.Most of the approaches for Image Super Resolution till now used the MSE (mean squared error )as a loss function , the problem with MSE as a loss function is that the high texture details of the image are averaged to create a smooth reconstruction .",
            "GANs solve this problem by using the perceptual loss which drives the image reconstruction towards the natural image manifold producing perceptually more realistic and convincing solutions .",
            "What are GANs ?",
            "Generative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other (thus the “adversarial”) in order to generate new, synthetic instances of data that can pass for real data.",
            "The two neural networks are the generator and the discriminator . The generator tries to produce a new data instance and the discriminator tries to distinguish whether the data belongs to the training data set or not .",
            "links to learn more about GANs :",
            "machinelearningmastery.com",
            "pathmind.com",
            "Super Resolution GANs :",
            "Super-resolution GANs apply a deep network in combination with an adversarial network to produce higher resolution images. As mentioned above, SR GANs tend to produce images which are more appealing to humans with more details compared to an architecture built without GANs.",
            "1)Introduction",
            "SR-GANs consist of two networks a generator and a discriminator.",
            "The discriminator(critic) is trained to differentiate between the real HR image and the generated image and is implemented using CNNs .",
            "The generator is used to take input LR images and generate HR output images which is sent to the critic for evaluation and is implemented using ResNet blocks.",
            "The loss function is a multi component loss function consisting of content loss and adversarial loss.",
            "2)SR GAN Architecture",
            "3)Loss Function",
            "Perceptual loss function",
            "Previous approaches to image super resolution based their approach on MSE , but we improve upon this approach by using the perceptual loss function . The perceptual loss function generates images with respect to perceptually relevant characteristics.",
            "Perceptual Loss is the weighted sum of content loss and adversarial loss",
            "Dateset used",
            "The SRGAN was trained on the DIV2K data set , the data set consists of 800 HR images .",
            "The images are down sampled and then used for training . The data set has a large variety of images .",
            "Training",
            "The network was trained on google colab on the DIV2K data set for 350 epochs .",
            "Results",
            "Link to the code",
            "github.com",
            "References and further reading",
            "arxiv.org",
            "machinelearningmastery.com",
            "pathmind.com"
          ]
        }
      }
    ]
  }
}
